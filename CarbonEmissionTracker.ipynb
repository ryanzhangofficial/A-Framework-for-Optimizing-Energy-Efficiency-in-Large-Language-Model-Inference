{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc902fd-e83c-4924-8ba9-94c90dfab95b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install codecarbon matplotlib torch transformers fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a01d95b-42b4-49b6-9392-949696256886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import fasttext\n",
    "import matplotlib.pyplot as plt\n",
    "from codecarbon import EmissionsTracker\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f766d9b5-a028-44f2-996a-4db9bd55b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wmt14_dataset = load_dataset('wmt14', 'de-en', split='validation')\n",
    "cnn_dailymail_dataset = load_dataset('cnn_dailymail', '2.0.0', split='validation')\n",
    "gsm8k_dataset = load_dataset('openai/gsm8k', 'main', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a6a30f-4789-46be-ab32-0a780244a3db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tracker = EmissionsTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f241c6f-d32c-4ff1-9482-6b1ffc8358ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "llama7b_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llama7b = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                               torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec9acb-553b-41dd-878f-4c394fe4e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
    "\n",
    "tinyllama_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tinyllama = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                 torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8da63c-f54b-42eb-83ed-cf0d3329f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama13_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                    bnb_4bit_compute_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fac7f50-7203-4921-99ac-81956dd26d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "\n",
    "llama13b_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llama13b = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                device_map='auto',\n",
    "                                                quantization_config=llama13_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac87267-8cb5-4d10-963c-511063c55aee",
   "metadata": {},
   "source": [
    "## Carbon Emissions of Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffce088-4afb-4d3d-af2a-31d1f72c23aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_classifier = fasttext.load_model(\"fasttext_classifier.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88753da-e2ca-4a1f-8cca-8ab16269770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.start()\n",
    "for idx in range(1000):\n",
    "    input_text_wmt14 = wmt14_dataset[idx]['translation']['de']\n",
    "    predicted_label, confidence_score = fasttext_classifier.predict(input_text_wmt14)\n",
    "tracker.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fedaab0-bc47-4435-a681-22cbd26890ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.start()\n",
    "for idx in range(1000):\n",
    "    input_text_cnn_dailymail = cnn_dailymail_dataset[idx]['article']\n",
    "    predicted_label, confidence_score = fasttext_classifier.predict(input_text_cnn_dailymail)\n",
    "tracker.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91788c5-a7ff-4a80-8f13-adb1b6e3e623",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.start()\n",
    "for idx in range(1000):\n",
    "    input_text_gsm8k = gsm8k_dataset[idx]['question']\n",
    "    predicted_label, confidence_score = fasttext_classifier.predict(input_text_gsm8k)\n",
    "tracker.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c81c494-b1e3-4adb-a67b-0aea7b1e949b",
   "metadata": {},
   "source": [
    "## WMT14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45749e0-b26f-48f6-acaf-81166b75fc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(model, tokenizer, dataset, current_idx):\n",
    "    input_text = wmt14_dataset[current_idx]['translation']['de']\n",
    "    input_prompt = \"Translate the sentence from German to English: \\n\\n\" + input_text + \"\\n\\n Write the translation here: \"\n",
    "\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "    output = model.generate(inputs['input_ids'])\n",
    "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    answer_prefix = \"Write the translation here: \"\n",
    "    if answer_prefix in output_text:\n",
    "        cleaned_output = output_text.split(answer_prefix)[-1].strip()\n",
    "    else:\n",
    "        cleaned_output = output_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523744a7-d5d9-4f78-8e75-3d2973b961e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tracker.start()\n",
    "\n",
    "# Llama7b\n",
    "for current_idx in range(0, 1000):\n",
    "    input_text = wmt14_dataset[current_idx]['translation']['de']\n",
    "    generate_output(llama7b, llama7b_tokenizer, input_text, current_idx)\n",
    "    \n",
    "    print(f\"Llama-7b | CURRENT IDX: {current_idx}\")\n",
    "\n",
    "emissions:float = tracker.stop()\n",
    "print(f\"Estimated emissions: {emissions:.10f} metric tons of CO2 equivalent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cee3ba1-0ce3-4ec5-be70-aaf5f8d87504",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tracker.start()\n",
    "\n",
    "# TinyLlama\n",
    "for current_idx in range(0, 1000):\n",
    "    input_text = wmt14_dataset[current_idx]['translation']['de']\n",
    "    generate_output(tinyllama, tinyllama_tokenizer, input_text, current_idx)\n",
    "    \n",
    "    print(f\"TinyLlama | CURRENT IDX: {current_idx}\")\n",
    "    \n",
    "emissions:float = tracker.stop()\n",
    "print(f\"Estimated emissions: {emissions:.10f} metric tons of CO2 equivalent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e68e47-3235-4d76-9f53-36af32b137c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tracker.start()\n",
    "\n",
    "# Llama13b\n",
    "for current_idx in range(0, 1000):\n",
    "    input_text = wmt14_dataset[current_idx]['translation']['de']\n",
    "    generate_output(llama13b, llama13b_tokenizer, input_text, current_idx)\n",
    "    \n",
    "    print(f\"Llama13b | CURRENT IDX: {current_idx}\")\n",
    "\n",
    "emissions:float = tracker.stop()\n",
    "print(f\"Estimated emissions: {emissions:.10f} metric tons of CO2 equivalent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a8bc4d-37df-48ff-9998-f8714a2729d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ec1c5-a2c5-4a5e-98ef-cbea90590c03",
   "metadata": {},
   "source": [
    "#### FastText Classifier Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25227f4-65ba-4425-8006-70f012f07e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(model, tokenizer, input_text):\n",
    "    input_prompt = \"Translate the sentence from German to English: \\n\\n\" + input_text + \"\\n\\n Write the translation here: \"\n",
    "\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "    output = model.generate(inputs['input_ids'])\n",
    "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    answer_prefix = \"Write the translation here: \"\n",
    "    if answer_prefix in output_text:\n",
    "        cleaned_output = output_text.split(answer_prefix)[-1].strip()\n",
    "    else:\n",
    "        cleaned_output = output_text.strip()\n",
    "\n",
    "    first_sentence = cleaned_output.split('.')[0] + '.' if '.' in cleaned_output else cleaned_output\n",
    "        \n",
    "    return first_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f93d0a-b141-42e3-865c-1e194e1585c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_classifier = fasttext.load_model(\"fasttext_classifier.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c88843-c253-4e7d-9f69-3f218698ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classifier_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e96397-a6cd-4d5d-8b2f-81e6fa661a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(1000):\n",
    "    input_text_wmt14 = wmt14_dataset[idx]['translation']['de']\n",
    "    predicted_label, confidence_score = fasttext_classifier.predict(input_text_wmt14)\n",
    "\n",
    "    if \"7b\" in predicted_label[0]: \n",
    "        test_classifier_dict[input_text_wmt14] = \"7b\"\n",
    "    elif \"tiny\" in predicted_label[0]: \n",
    "        test_classifier_dict[input_text_wmt14] = \"Tiny\"\n",
    "    elif \"13b\" in predicted_label[0]: \n",
    "        test_classifier_dict[input_text_wmt14] = \"13b\"\n",
    "        \n",
    "inputs_7b = [key for key, value in test_classifier_dict.items() if value == \"7b\"]\n",
    "inputs_tiny = [key for key, value in test_classifier_dict.items() if value == \"Tiny\"]\n",
    "inputs_13b = [key for key, value in test_classifier_dict.items() if value == \"13b\"]\n",
    "\n",
    "print(f\"Number of inputs for 7b: {len(inputs_7b)}\")\n",
    "print(f\"Number of inputs for Tiny: {len(inputs_tiny)}\")\n",
    "print(f\"Number of inputs for 13b: {len(inputs_13b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa63373-7db8-4122-add4-927503f5f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_7b = [key for key, value in test_classifier_dict.items() if value == \"7b\"]\n",
    "inputs_tiny = [key for key, value in test_classifier_dict.items() if value == \"Tiny\"]\n",
    "inputs_13b = [key for key, value in test_classifier_dict.items() if value == \"13b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e744089-b0ec-467e-974d-e97302ae3735",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tracker.start()\n",
    "\n",
    "for x, y in list(test_classifier_dict.items()):\n",
    "    if (y == \"7b\"):\n",
    "        output = generate_output(llama7b, llama7b_tokenizer, x)\n",
    "        print(f\"{counter}/{len(inputs_7b)}: Inferenced a {y} sample.\")\n",
    "        counter += 1\n",
    "        \n",
    "emissions:float = tracker.stop()\n",
    "print(f\"Estimated emissions: {emissions:.10f} metric tons of CO2 equivalent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63105f1f-20a8-493a-832f-9ac3950c5864",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tracker.start()\n",
    "\n",
    "for x, y in list(test_classifier_dict.items()):\n",
    "    if (y == \"Tiny\"):\n",
    "        output = generate_output(tinyllama, tinyllama_tokenizer, x)\n",
    "        print(f\"{counter}/{len(inputs_tiny)}: Inferenced a {y} sample.\")\n",
    "        counter += 1\n",
    "\n",
    "emissions:float = tracker.stop()\n",
    "print(f\"Estimated emissions: {emissions:.10f} metric tons of CO2 equivalent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823fad77-e06a-48fd-b7ff-38e4e328a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e3238c-0edb-4f8b-9045-366d23f5922e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tracker.start()\n",
    "\n",
    "for x, y in list(test_classifier_dict.items())[start:]:\n",
    "    if (y == \"13b\"):\n",
    "        output = generate_output(llama13b, llama13b_tokenizer, x)\n",
    "        print(f\"{counter}/{len(inputs_13b)}: Inferenced a {y} sample.\")\n",
    "        counter += 1\n",
    "        torch.cuda.empty_cache()\n",
    "    start += 1\n",
    "    \n",
    "emissions:float = tracker.stop()\n",
    "print(f\"Estimated emissions: {emissions:.10f} metric tons of CO2 equivalent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f093573d-3082-489c-9c48-349df45a6510",
   "metadata": {},
   "source": [
    "## CNN_Dailymail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f30fec-440c-4879-9ef1-c95f285a17c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(model, tokenizer, dataset, current_idx):\n",
    "    input_text = cnn_dailymail_dataset[current_idx]['article'] \n",
    "    input_prompt = \"Summarize the following text in under 50 words: \\n\\n\" + input_text + \"\\n\\n Write the summary here: \"\n",
    "\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "    output = model.generate(inputs['input_ids'], max_new_tokens=100)\n",
    "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    answer_prefix = \"Lets think step by step: \"\n",
    "    if answer_prefix in output_text:\n",
    "        cleaned_output = output_text.split(answer_prefix)[-1].strip()\n",
    "    else:\n",
    "        cleaned_output = output_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc58e918-00b3-45c9-b278-cf394873958e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tracker.start()\n",
    "\n",
    "# Llama7b\n",
    "for current_idx in range(0, 1000):\n",
    "    input_text = cnn_dailymail_dataset[current_idx]['article'] \n",
    "    generate_output(llama7b, llama7b_tokenizer, input_text, current_idx)\n",
    "    \n",
    "    print(f\"Llama-7b | CURRENT IDX: {current_idx}\")\n",
    "\n",
    "emissions:float = tracker.stop()\n",
    "print(f\"Estimated emissions: {emissions:.10f} metric tons of CO2 equivalent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b8362f-d76f-4d21-bbcb-4bdb145ef453",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tracker.start()\n",
    "\n",
    "# TinyLlama\n",
    "for current_idx in range(0, 1000):\n",
    "    input_text = cnn_dailymail_dataset[current_idx]['article'] \n",
    "    generate_output(tinyllama, tinyllama_tokenizer, input_text, current_idx)\n",
    "    \n",
    "    print(f\"TinyLlama | CURRENT IDX: {current_idx}\")\n",
    "\n",
    "emissions:float = tracker.stop()\n",
    "print(f\"Estimated emissions: {emissions:.10f} metric tons of CO2 equivalent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d42f2c7-e2c9-4df5-a200-c62aa1d1800e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tracker.start()\n",
    "\n",
    "# Llama13b\n",
    "for current_idx in range(0, 1000):\n",
    "    input_text = cnn_dailymail_dataset[current_idx]['article'] \n",
    "    generate_output(llama13b, llama13b_tokenizer, input_text, current_idx)\n",
    "    \n",
    "    print(f\"Llama-13b | CURRENT IDX: {current_idx}\")\n",
    "\n",
    "emissions:float = tracker.stop()\n",
    "print(f\"Estimated emissions: {emissions:.10f} metric tons of CO2 equivalent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d998f-942d-4deb-b4be-054d81614da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a44f1d6-8a9f-4cde-87bb-8f0a0de148a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(model, tokenizer, input_text):\n",
    "    input_prompt = \"Summarize the following text in under 50 words: \\n\\n\" + input_text + \"\\n\\n Write the summary here: \"\n",
    "\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "    output = model.generate(inputs['input_ids'], max_new_tokens=100)\n",
    "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    summary_prefix = \"Write the summary here: \"\n",
    "    if summary_prefix in output_text:\n",
    "        cleaned_output = output_text.split(summary_prefix)[-1].strip()\n",
    "    else:\n",
    "        cleaned_output = output_text.strip()\n",
    "        \n",
    "    return cleaned_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbdaeb8-3104-4b84-bfb0-e22e1ab40fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_classifier = fasttext.load_model(\"fasttext_classifier.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eadf84-6b3c-4772-851d-4249c7ac1a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classifier_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5d6e8b-c0f6-490c-9731-829715824467",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(1000):\n",
    "    input_text_cnn_dailymail = cnn_dailymail_dataset[idx]['article']\n",
    "    predicted_label, confidence_score = fasttext_classifier.predict(input_text_cnn_dailymail)\n",
    "\n",
    "    if \"7b\" in predicted_label[0]: \n",
    "        test_classifier_dict[input_text_cnn_dailymail] = \"7b\"\n",
    "    elif \"tiny\" in predicted_label[0]: \n",
    "        test_classifier_dict[input_text_cnn_dailymail] = \"Tiny\"\n",
    "    elif \"13b\" in predicted_label[0]: \n",
    "        test_classifier_dict[input_text_cnn_dailymail] = \"13b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a4d2da-ef61-4367-a262-1a89f0210d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_7b = [key for key, value in test_classifier_dict.items() if value == \"7b\"]\n",
    "inputs_tiny = [key for key, value in test_classifier_dict.items() if value == \"Tiny\"]\n",
    "inputs_13b = [key for key, value in test_classifier_dict.items() if value == \"13b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643ea9d8-0f86-4822-bef2-10a1a705295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of inputs for 7b: {len(inputs_7b)}\")\n",
    "print(f\"Number of inputs for Tiny: {len(inputs_tiny)}\")\n",
    "print(f\"Number of inputs for 13b: {len(inputs_13b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7065dcdc-31fa-401d-83c0-7b22c7700f1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tracker.start()\n",
    "\n",
    "for x, y in list(test_classifier_dict.items()):   \n",
    "    if (y == \"7b\"):\n",
    "        output = generate_output(llama7b, llama7b_tokenizer, x)\n",
    "        print(f\"{counter}/{len(inputs_7b)}: Inferenced a {y} sample.\")\n",
    "        counter += 1\n",
    "        \n",
    "emissions:float = tracker.stop()\n",
    "print(f\"Estimated emissions: {emissions:.10f} metric tons of CO2 equivalent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fe687d-adeb-4ffb-815b-b20d3b1fd870",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tracker.start()\n",
    "\n",
    "for x, y in list(test_classifier_dict.items()):\n",
    "    if (y == \"Tiny\"):\n",
    "        output = generate_output(tinyllama, tinyllama_tokenizer, x)\n",
    "        print(f\"{counter}/{len(inputs_tiny)}: Inferenced a {y} sample.\")\n",
    "        counter += 1\n",
    "        \n",
    "emissions:float = tracker.stop()\n",
    "print(f\"Estimated emissions: {emissions:.10f} metric tons of CO2 equivalent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb85a272-43bc-4c5f-86d8-730112752581",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95c906a-37ac-4739-ac09-af7125aef9f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tracker.start()\n",
    "\n",
    "for x, y in list(test_classifier_dict.items())[start:]:\n",
    "    if (y == \"13b\"):\n",
    "        output = generate_output(llama13b, llama13b_tokenizer, x)\n",
    "        print(f\"{counter}/{len(inputs_13b)}: Inferenced a {y} sample.\")\n",
    "        counter += 1\n",
    "        torch.cuda.empty_cache()\n",
    "    start += 1\n",
    "    \n",
    "emissions:float = tracker.stop()\n",
    "print(f\"Estimated emissions: {emissions:.10f} metric tons of CO2 equivalent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d855d8-d590-4d23-b634-739038e17064",
   "metadata": {},
   "source": [
    "## GSM8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d0b571-8740-4675-a777-a54e1636c4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(model, tokenizer, dataset, current_idx):\n",
    "    input_text = gsm8k_dataset[current_idx]['question'] \n",
    "    input_prompt = \"Answer the following math question: \\n\\n\" + input_text + \"\\n\\n Lets think step by step: \"\n",
    "\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "    output = model.generate(inputs['input_ids'])\n",
    "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    answer_prefix = \"Lets think step by step: \"\n",
    "    if answer_prefix in output_text:\n",
    "        cleaned_output = output_text.split(answer_prefix)[-1].strip()\n",
    "    else:\n",
    "        cleaned_output = output_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91458faa-b03d-4a83-8541-ab1e7d3c97fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tracker.start()\n",
    "\n",
    "# Llama7b\n",
    "for current_idx in range(0, 1000):\n",
    "    input_text = gsm8k_dataset[current_idx]['question'] \n",
    "    generate_output(llama7b, llama7b_tokenizer, input_text, current_idx)\n",
    "    \n",
    "    print(f\"Llama-7b | CURRENT IDX: {current_idx}\")\n",
    "\n",
    "emissions:float = tracker.stop()\n",
    "print(f\"Estimated emissions: {emissions:.10f} metric tons of CO2 equivalent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2315ebe9-757f-4abe-8fda-10d6f9a4b894",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tracker.start()\n",
    "\n",
    "# TinyLlama\n",
    "for current_idx in range(0, 1000):\n",
    "    input_text = gsm8k_dataset[current_idx]['question'] \n",
    "    generate_output(tinyllama, tinyllama_tokenizer, input_text, current_idx)\n",
    "    \n",
    "    print(f\"TinyLlama | CURRENT IDX: {current_idx}\")\n",
    "\n",
    "emissions:float = tracker.stop()\n",
    "print(f\"Estimated emissions: {emissions:.10f} metric tons of CO2 equivalent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db1d4da-ad2c-4730-b66a-58cdee4b01e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tracker.start()\n",
    "\n",
    "# Llama13b\n",
    "for current_idx in range(0, 1000):\n",
    "    input_text = gsm8k_dataset[current_idx]['question'] \n",
    "    output_7b = generate_output(llama13b, llama13b_tokenizer, input_text, current_idx)\n",
    "    \n",
    "    print(f\"Llama-13b | CURRENT IDX: {current_idx}\")\n",
    "\n",
    "emissions:float = tracker.stop()\n",
    "print(f\"Estimated emissions: {emissions:.10f} metric tons of CO2 equivalent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8d0996-6d07-4f3f-a6f4-ba9959f592d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ddfc7f-84dd-47d2-943a-9af6bcb90aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(model, tokenizer, input_text):\n",
    "    input_prompt = \"Answer the following math question: \\n\\n\" + input_text + \"\\n\\n Lets think step by step: \"\n",
    "\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "    output = model.generate(inputs['input_ids'])\n",
    "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    answer_prefix = \"Lets think step by step: \"\n",
    "    if answer_prefix in output_text:\n",
    "        cleaned_output = output_text.split(answer_prefix)[-1].strip()\n",
    "    else:\n",
    "        cleaned_output = output_text.strip()\n",
    "        \n",
    "    return cleaned_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acdabb1-2af7-44f6-bb75-64ffaf6675aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_classifier = fasttext.load_model(\"fasttext_classifier.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5233259-0fa9-455b-9d7b-b83eb186490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classifier_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4c3471-72c4-48b9-b972-0d9662d6da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(1000):\n",
    "    input_text_gsm8k = gsm8k_dataset[idx]['question'] \n",
    "    predicted_label, confidence_score = fasttext_classifier.predict(input_text_gsm8k)\n",
    "\n",
    "    if \"7b\" in predicted_label[0]: \n",
    "        test_classifier_dict[input_text_gsm8k] = \"7b\"\n",
    "    elif \"tiny\" in predicted_label[0]: \n",
    "        test_classifier_dict[input_text_gsm8k] = \"Tiny\"\n",
    "    elif \"13b\" in predicted_label[0]: \n",
    "        test_classifier_dict[input_text_gsm8k] = \"13b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba142939-e64f-46e8-a4e7-9ea3e80af97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_7b = [key for key, value in test_classifier_dict.items() if value == \"7b\"]\n",
    "inputs_tiny = [key for key, value in test_classifier_dict.items() if value == \"Tiny\"]\n",
    "inputs_13b = [key for key, value in test_classifier_dict.items() if value == \"13b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885e81ad-4151-4cb7-81cd-ff6175d61635",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of inputs for 7b: {len(inputs_7b)}\")\n",
    "print(f\"Number of inputs for Tiny: {len(inputs_tiny)}\")\n",
    "print(f\"Number of inputs for 13b: {len(inputs_13b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0b5785-bed4-411e-ac3c-5379f65558ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tracker.start()\n",
    "\n",
    "for x, y in list(test_classifier_dict.items()):\n",
    "    if (y == \"7b\"):\n",
    "        output = generate_output(llama7b, llama7b_tokenizer, x)\n",
    "        print(f\"{counter}/{len(inputs_7b)}: Inferenced a {y} sample.\")\n",
    "        counter += 1\n",
    "        \n",
    "emissions:float = tracker.stop()\n",
    "print(f\"Estimated emissions: {emissions:.10f} metric tons of CO2 equivalent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1a8acf-874d-45b3-acdd-054a9afb0c67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tracker.start()\n",
    "\n",
    "for x, y in list(test_classifier_dict.items()):\n",
    "    if (y == \"Tiny\"):\n",
    "        output = generate_output(tinyllama, tinyllama_tokenizer, x)\n",
    "        print(f\"{counter}/{len(inputs_tiny)}: Inferenced a {y} sample.\")\n",
    "        counter += 1\n",
    "        \n",
    "emissions:float = tracker.stop()\n",
    "print(f\"Estimated emissions: {emissions:.10f} metric tons of CO2 equivalent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d349e95-1171-41b2-957c-895a26f92807",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "start = 706"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b6fb73-8d30-46e7-993b-54f565282e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd743089-4f58-46b2-8047-a7b69aa507f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tracker.start()\n",
    "\n",
    "for x, y in list(test_classifier_dict.items())[start:]:\n",
    "    if (y == \"13b\"):\n",
    "        output = generate_output(llama13b, llama13b_tokenizer, x)\n",
    "        print(f\"{counter}/{len(inputs_13b)}: Inferenced a {y} sample.\")\n",
    "        counter += 1\n",
    "        torch.cuda.empty_cache()\n",
    "    start += 1\n",
    "    print(f\"INDEX: {start}\")\n",
    "        \n",
    "emissions:float = tracker.stop()\n",
    "print(f\"Estimated emissions: {emissions:.10f} metric tons of CO2 equivalent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f7e588-b21c-485b-a823-6e7c1ce21f63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
