{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcac6095-37d9-4508-805e-aa190de4a224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./lib/python3.9/site-packages (4.41.2)\n",
      "Requirement already satisfied: datasets in ./lib/python3.9/site-packages (2.19.2)\n",
      "Requirement already satisfied: torch in ./lib/python3.9/site-packages (2.3.1)\n",
      "Requirement already satisfied: ipywidgets in ./lib/python3.9/site-packages (8.1.3)\n",
      "Requirement already satisfied: sentence_transformers in ./lib/python3.9/site-packages (3.0.0)\n",
      "Requirement already satisfied: matplotlib in ./lib/python3.9/site-packages (3.9.0)\n",
      "Requirement already satisfied: nltk in ./lib/python3.9/site-packages (3.8.1)\n",
      "Requirement already satisfied: bitsandbytes in ./lib/python3.9/site-packages (0.43.1)\n",
      "Requirement already satisfied: accelerate in ./lib/python3.9/site-packages (0.31.0)\n",
      "Requirement already satisfied: filelock in ./lib/python3.9/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in ./lib/python3.9/site-packages (from transformers) (0.23.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./lib/python3.9/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./lib/python3.9/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in ./lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./lib/python3.9/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./lib/python3.9/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./lib/python3.9/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in ./lib/python3.9/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./lib/python3.9/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./lib/python3.9/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./lib/python3.9/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in ./lib/python3.9/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in ./lib/python3.9/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in ./lib/python3.9/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in ./lib/python3.9/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./lib/python3.9/site-packages (from torch) (4.12.1)\n",
      "Requirement already satisfied: sympy in ./lib/python3.9/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in ./lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./lib/python3.9/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./lib/python3.9/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./lib/python3.9/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./lib/python3.9/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./lib/python3.9/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./lib/python3.9/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./lib/python3.9/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in ./lib/python3.9/site-packages (from torch) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./lib/python3.9/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./lib/python3.9/site-packages (from ipywidgets) (8.18.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./lib/python3.9/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in ./lib/python3.9/site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in ./lib/python3.9/site-packages (from ipywidgets) (3.0.11)\n",
      "Requirement already satisfied: scikit-learn in ./lib/python3.9/site-packages (from sentence_transformers) (1.5.0)\n",
      "Requirement already satisfied: scipy in ./lib/python3.9/site-packages (from sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: Pillow in ./lib/python3.9/site-packages (from sentence_transformers) (10.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./lib/python3.9/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./lib/python3.9/site-packages (from matplotlib) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./lib/python3.9/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./lib/python3.9/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./lib/python3.9/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./lib/python3.9/site-packages (from matplotlib) (6.4.0)\n",
      "Requirement already satisfied: click in ./lib/python3.9/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./lib/python3.9/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: psutil in ./lib/python3.9/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./lib/python3.9/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./lib/python3.9/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./lib/python3.9/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./lib/python3.9/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.19.2)\n",
      "Requirement already satisfied: decorator in ./lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in ./lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.46)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in ./lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in ./lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in ./lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: six>=1.5 in ./lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./lib/python3.9/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./lib/python3.9/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./lib/python3.9/site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./lib/python3.9/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./lib/python3.9/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in ./lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./lib/python3.9/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./lib/python3.9/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in ./lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets torch ipywidgets sentence_transformers matplotlib nltk bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b8c32c9-dbcd-401d-b9c6-7caecdad8317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "995df5ec-914f-44e0-8f47-f4178edd9232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d622bd4-6223-43e9-a191-3f499b0354fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun 23 21:10:33 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  | 00000000:17:00.0 Off |                  N/A |\n",
      "| 30%   48C    P8              22W / 370W |   3647MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3070        On  | 00000000:65:00.0 Off |                  N/A |\n",
      "| 33%   51C    P8              22W / 220W |     13MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A       919      G   /usr/lib/xorg/Xorg                            9MiB |\n",
      "|    0   N/A  N/A      1043      G   /usr/bin/gnome-shell                          6MiB |\n",
      "|    0   N/A  N/A     81037      C   /home/ryan/myenv/bin/python3               3616MiB |\n",
      "|    1   N/A  N/A       919      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8bcbb0-25b0-4b42-8bc9-3607d7d6bb64",
   "metadata": {},
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0045c0b-2dfd-4044-a56e-591ff3017b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba39bc865a74f3a84e6e780da1d7601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a493bcb-fb76-4e61-996c-f4237d8fa5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad198ed-734f-4f0c-90d4-55dbcaf61c54",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38c74653-3188-48ad-a901-1ebd6296aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2a7502-f859-40a5-b3b9-fcad071c07b6",
   "metadata": {},
   "source": [
    "Llama7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bea5f32-1cb1-4893-8a56-06175223f555",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8188f3fdac41778da81d0939da2635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "llama7b_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llama7b = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                               torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11e79aa-7da1-4745-a3b4-9eaa5b48ae0b",
   "metadata": {},
   "source": [
    "TinyLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "073e3e3d-3950-4467-a4f5-c46aef74121b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
    "\n",
    "tinyllama_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tinyllama = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                 torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f42151-3491-4838-a518-7a86d0003126",
   "metadata": {},
   "source": [
    "Llama13b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "119fe424-aeba-4652-b1e5-911dcafe9bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama13_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                    bnb_4bit_compute_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b69b5a3-1066-4369-9ebc-037674f3e1e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf24226b40b04891b781f21fbe45fae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "\n",
    "llama13b_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llama13b = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                device_map='auto',\n",
    "                                                quantization_config=llama13_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556c99dc-6b86-4e0e-b655-d9b02a5a77d2",
   "metadata": {},
   "source": [
    "### Basic Model Loading + Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acb3914-3d72-42cd-a5e0-e396f5429951",
   "metadata": {},
   "source": [
    "#### Llama 7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11b92648-b804-4757-9813-94d533fb7f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a land far, far away, there was a magical kingdom called \"The Land of the Free.\" In this kingdom, everyone was free to do as they pleased, and no one was ever punished for their actions.\n",
      "\n",
      "One day, a young prince named \"Liberty\" decided to take a walk in the forest. As he wandered deeper into the woods, he came across a beautiful fairy named \"Justice.\"\n",
      "\n",
      "\"Who\n",
      "\n",
      "Execution time: 2.9925544261932373 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "input_text = \"Once upon a time in a land far, far away\"\n",
    "inputs = llama7b_tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "output = llama7b.generate(inputs['input_ids'], max_length=100)\n",
    "\n",
    "output_text = llama7b_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output_text)\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "print()\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ada08d8-9b4e-46b8-bee5-280d2217fd17",
   "metadata": {},
   "source": [
    "#### TinyLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aada3b66-3cc4-4d71-a014-c55b610ecfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a land far, far away, there lived a young girl named Lily. Lily was a kind and gentle girl, always looking for ways to help others. One day, while walking through the woods, Lily stumbled upon a group of animals who were being hunted by a pack of wolves. The wolves were fierce and dangerous, and they were determined to take the animals for their own. Lily knew that she had to do something to\n",
      "\n",
      "Execution time: 1.5956108570098877 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "input_text = \"Once upon a time in a land far, far away\"\n",
    "inputs = tinyllama_tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "output = tinyllama.generate(inputs['input_ids'], max_length=100)\n",
    "\n",
    "output_text = tinyllama_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output_text)\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "print()\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9da26a-875d-42e1-a3e5-b5680e16054d",
   "metadata": {},
   "source": [
    "#### Llama 13b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a310f942-ad56-421c-a3a0-7a0dc803825e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryan/myenv/lib/python3.9/site-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a land far, far away, there was a magical kingdom called \"Happily Ever Laughter.\" The kingdom was ruled by a wise and witty king named \"King Punsley\" who was loved by all his subjects for his ability to make them laugh with his clever jokes and puns.\n",
      "\n",
      "One day, a brave and clever princess named \"Princess Punsalot\" decided to go on a quest to\n",
      "\n",
      "Execution time: 5.729012727737427 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "input_text = \"Once upon a time in a land far, far away\"\n",
    "inputs = llama13b_tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "output = llama13b.generate(inputs['input_ids'], max_length=100)\n",
    "\n",
    "output_text = llama13b_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output_text)\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "print()\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cd5e40f-ccab-4a71-98e6-c3aebcd0858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a9f561-ba57-4ba1-8f5f-ac5a5d9475bf",
   "metadata": {},
   "source": [
    "### **WMT 2014 (Machine Translation)** \n",
    "is a collection of datasets used in shared tasks of the Ninth Workshop on Statistical Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14ea7983-d95a-4f39-9017-7baae54dc8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wmt14_dataset = load_dataset('wmt14', 'de-en', split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a458a1af-7109-4c22-a3c3-0e2412ab7538",
   "metadata": {},
   "source": [
    "#### Example inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "033965fd-4a81-48c5-b12c-d26b030dc6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = wmt14_dataset[0]['translation']['en']  \n",
    "input_prompt = f\"Translate to English: {input_text}\"\n",
    "\n",
    "inputs = llama7b_tokenizer(input_prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11df2231-71ad-4eed-9aab-3e3ce34b8d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate to English: Gutach: Increased safety for pedestrians and cyclists in the city\n",
      "\n",
      "Gutach: Increased safety for pedestrians and cyclists in the city\n",
      "========================================================\n"
     ]
    }
   ],
   "source": [
    "output = llama7b.generate(inputs['input_ids'], max_length=50)\n",
    "\n",
    "output_text = llama7b_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b6fbde-bcae-4a8e-a519-06305470d524",
   "metadata": {},
   "source": [
    "#### Llama 7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b4f6857-42da-4e3e-8d5d-de0684612d46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Translate to English: Gutach: Noch mehr Sicherheit für Fußgänger\n",
      "Output: Translate to English: Gutach: Noch mehr Sicherheit für Fußgänger und Radfahrer\n",
      "\n",
      "Original text:\n",
      "Gutach: Noch mehr Sicherheit für Fußgänger und Radfahrer\n",
      "\n",
      "Translation:\n",
      "Gutach: More safety for pedestrians and cyclists\n",
      "\n",
      "\n",
      "\n",
      "Input: Translate to English: Sie stehen keine 100 Meter voneinander entfernt: Am Dienstag ist in Gutach die neue B 33-Fußgängerampel am Dorfparkplatz in Betrieb genommen worden - in Sichtweite der älteren Rathausampel.\n",
      "Output: Translate to English: Sie stehen keine 100 Meter voneinander entfernt: Am Dienstag ist in Gutach die neue B 33-Fußgängerampel am Dorfparkplatz in Betrieb genommen worden.\n",
      "\n",
      "I hope this helps! Let me know if you have any questions.\n",
      "\n",
      "Input: Translate to English: Zwei Anlagen so nah beieinander: Absicht oder Schildbürgerstreich?\n",
      "Output: Translate to English: Zwei Anlagen so nah beieinander: Absicht oder Schildbürgerstreich?\n",
      "\n",
      "Context: A lawyer is discussing two nearby industrial facilities with a client, and the client is concerned about the potential impact of one facility on the other. The lawyer is trying to reassure the client that the two facilities are not in\n",
      "\n",
      "Input: Translate to English: Diese Frage hat Gutachs Bürgermeister gestern klar beantwortet.\n",
      "Output: Translate to English: Diese Frage hat Gutachs Bürgermeister gestern klar beantwortet.\n",
      "\n",
      "I hope someone can help me with this! I'm trying to translate a sentence from German to English, and I'm having a bit of trouble. Here's the sentence:\n",
      "\n",
      "\"Diese Frage hat Gutach\n",
      "\n",
      "Input: Translate to English: \"Die Rathausampel ist damals installiert worden, weil diese den Schulweg sichert\", erläuterte Eckert gestern.\n",
      "Output: Translate to English: \"Die Rathausampel ist damals installiert worden, weil diese den Schulweg sichert\", erläuterte Eckert gestern.\n",
      "\n",
      "Please translate \"Die Rathausampel ist damals installiert worden, weil diese den Schulweg sichert,\" as it is a quote from a person named Eckert.\n",
      "\n",
      "The given sentence in German can be translated to\n",
      "\n",
      "\n",
      "Execution time: 170.93490362167358 seconds\n"
     ]
    }
   ],
   "source": [
    "num_examples = 5 \n",
    "\n",
    "state_time = time.time()\n",
    "for i in range(num_examples):\n",
    "    input_text = wmt14_dataset[i]['translation']['de']\n",
    "    input_prompt = f\"Translate to English: {input_text}\"\n",
    "    \n",
    "    inputs = llama7b_tokenizer(input_prompt, return_tensors=\"pt\", truncation=True, max_length=50)\n",
    "    output = llama7b.generate(inputs['input_ids'], max_new_tokens=50)\n",
    "    output_text = llama7b_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"Input: {input_prompt}\")\n",
    "    print(f\"Output: {output_text}\")\n",
    "    print()\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "print()\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9107d737-f3ff-4004-ba17-6c40523cbcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e70660-33a1-4f3a-83db-a2cffea3b2b5",
   "metadata": {},
   "source": [
    "#### Tiny Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbf1fdc4-47b6-4b1a-872c-0ded38b4c2d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m input_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslate to English: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tinyllama_tokenizer(input_prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtinyllama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m output_text \u001b[38;5;241m=\u001b[39m tinyllama_tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/myenv/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.9/site-packages/transformers/generation/utils.py:1758\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1750\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1751\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1752\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1753\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1754\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1755\u001b[0m     )\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1758\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1770\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1772\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config) \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1773\u001b[0m     )\n",
      "File \u001b[0;32m~/myenv/lib/python3.9/site-packages/transformers/generation/utils.py:2397\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2394\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2396\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2397\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2398\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2400\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2401\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2402\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2405\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1164\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1161\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1164\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:968\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    957\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    958\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    959\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    965\u001b[0m         cache_position,\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:726\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[1;32m    725\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 726\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_attention_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    727\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    728\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:88\u001b[0m, in \u001b[0;36mLlamaRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     86\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     87\u001b[0m variance \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 88\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariance_epsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m*\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(input_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_examples = 5 \n",
    "\n",
    "state_time = time.time()\n",
    "for i in range(num_examples):\n",
    "    input_text = wmt14_dataset[i]['translation']['de']\n",
    "    input_prompt = f\"Translate to English: {input_text}\"\n",
    "    \n",
    "    inputs = tinyllama_tokenizer(input_prompt, return_tensors=\"pt\", truncation=True, max_length=50)\n",
    "    output = tinyllama.generate(inputs['input_ids'], max_new_tokens=50)\n",
    "    output_text = tinyllama_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"Input: {input_prompt}\")\n",
    "    print(f\"Output: {output_text}\")\n",
    "    print()\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "print()\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79237f29-9453-463c-98a1-78a1ece9b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887106ac-190d-4f02-a3e0-5e60b2242c8a",
   "metadata": {},
   "source": [
    "#### Llama 13b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6dd4c5a-42c8-4f3e-82c9-4058fe252b8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Translate to English: Gutach: Noch mehr Sicherheit für Fußgänger\n",
      "Output: Translate to English: Gutach: Noch mehr Sicherheit für Fußgänger und Radfahrer\n",
      "\n",
      "Please provide the translation to English.\n",
      "\n",
      "\"Gutach: More safety for pedestrians and cyclists\"\n",
      "\n",
      "Would you like me to translate the entire text or just the headline? Additionally\n",
      "\n",
      "Input: Translate to English: Sie stehen keine 100 Meter voneinander entfernt: Am Dienstag ist in Gutach die neue B 33-Fußgängerampel am Dorfparkplatz in Betrieb genommen worden - in Sichtweite der älteren Rathausampel.\n",
      "Output: Translate to English: Sie stehen keine 100 Meter voneinander entfernt: Am Dienstag ist in Gutach die neue B 33-Fußgängerampel am Dorfparkplatz in Betrieb genommen worden.\n",
      "\n",
      "Translation to English: You are not 100 meters apart: On Tuesday, the new pedestrian traffic light at the village park parking lot on the B 33 was put into operation in Gutach\n",
      "\n",
      "Input: Translate to English: Zwei Anlagen so nah beieinander: Absicht oder Schildbürgerstreich?\n",
      "Output: Translate to English: Zwei Anlagen so nah beieinander: Absicht oder Schildbürgerstreich?\n",
      "\n",
      "Please help me with the translation of the following German sentence into English:\n",
      "\n",
      "Zwei Anlagen so nah beieinander: Absicht oder Schildbürgerstreich?\n",
      "\n",
      "I've tried to translate it myself\n",
      "\n",
      "Input: Translate to English: Diese Frage hat Gutachs Bürgermeister gestern klar beantwortet.\n",
      "Output: Translate to English: Diese Frage hat Gutachs Bürgermeister gestern klar beantwortet.\n",
      "\n",
      "Please help me to translate the sentence above into English.\n",
      "\n",
      "Here's what I have tried so far:\n",
      "\n",
      "\"This question has been clearly answered by the mayor's statement yesterday.\"\n",
      "\n",
      "But I'm not sure\n",
      "\n",
      "Input: Translate to English: \"Die Rathausampel ist damals installiert worden, weil diese den Schulweg sichert\", erläuterte Eckert gestern.\n",
      "Output: Translate to English: \"Die Rathausampel ist damals installiert worden, weil diese den Schulweg sichert\", erläuterte Eckert gestern.\n",
      "\n",
      "Translation to English: \"The town hall bell was installed at that time to ensure the school route,\" Eckert explained yesterday.\n",
      "\n",
      "\n",
      "Execution time: 416.3473560810089 seconds\n"
     ]
    }
   ],
   "source": [
    "num_examples = 5 \n",
    "\n",
    "state_time = time.time()\n",
    "for i in range(num_examples):\n",
    "    input_text = wmt14_dataset[i]['translation']['de']\n",
    "    input_prompt = f\"Translate to English: {input_text}\"\n",
    "    \n",
    "    inputs = llama13b_tokenizer(input_prompt, return_tensors=\"pt\", truncation=True, max_length=50)\n",
    "    output = llama13b.generate(inputs['input_ids'], max_new_tokens=50)\n",
    "    output_text = llama13b_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"Input: {input_prompt}\")\n",
    "    print(f\"Output: {output_text}\")\n",
    "    print()\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "print()\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2d4ac2d-b268-4749-879f-a0d49be38ecd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4746b4-6ec3-44d4-a5c2-08310eeca7a3",
   "metadata": {},
   "source": [
    "#### Functionalize Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57e1268b-ff84-4f21-88db-f29db0b3bde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points in different token ranges:\n",
      "0-50: 2290\n",
      "51-100: 692\n",
      "101-150: 21\n",
      "\n",
      "Data point with the most tokens is at index: 393\n",
      "Number of tokens: 131\n",
      "Input text: Die früher supergeheime NSA, deren Spitzname einst No Such Agency (Keine solche Behörde) lautete, findet sich inzwischen im hellen Licht der Öffentlichkeit und sieht sich nach den in den letzten Monaten bekannt gewordenen Enthüllungen über ihr ausgedehntes Überwachungsprogramm im In- und Ausland scharfer Kritik ausgesetzt – ein Resultat der geheimen NSA-Daten, die vom desillusionierten ehemaligen NSA-Mitarbeiter Edward Snowden gestohlen und veröffentlicht wurden.\n"
     ]
    }
   ],
   "source": [
    "token_ranges = {\n",
    "    '0-50': 0,\n",
    "    '51-100': 0,\n",
    "    '101-150': 0\n",
    "}\n",
    "\n",
    "max_tokens = -1\n",
    "\n",
    "for idx, data in enumerate(wmt14_dataset):\n",
    "    input_text = data['translation']['de']\n",
    "    tokens = llama13b_tokenizer(input_text, return_tensors=\"pt\")\n",
    "    num_tokens = len(tokens['input_ids'][0])\n",
    "    \n",
    "    if num_tokens > max_tokens:\n",
    "        max_tokens = num_tokens\n",
    "        max_tokens_idx = idx\n",
    "    \n",
    "    if num_tokens <= 50:\n",
    "        token_ranges['0-50'] += 1\n",
    "    elif num_tokens <= 100:\n",
    "        token_ranges['51-100'] += 1\n",
    "    elif num_tokens <= 150:\n",
    "        token_ranges['101-150'] += 1\n",
    "\n",
    "print(\"Number of data points in different token ranges:\")\n",
    "for key, value in token_ranges.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(f\"\\nData point with the most tokens is at index: {max_tokens_idx}\")\n",
    "print(f\"Number of tokens: {max_tokens}\")\n",
    "print(f\"Input text: {wmt14_dataset[max_tokens_idx]['translation']['de']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d739df1-1344-45f5-9b19-f6359f1d8904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(model, tokenizer, dataset, current_idx):\n",
    "    outputs = []\n",
    "    \n",
    "    input_text = wmt14_dataset[current_idx]['translation']['de']\n",
    "    input_prompt = \"Translate the sentence from German to English: \\n\\n\" + input_text + \"\\n\\n Write the translation here: \"\n",
    "\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "    output = model.generate(inputs['input_ids'])\n",
    "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    answer_prefix = \"Write the translation here: \"\n",
    "    if answer_prefix in output_text:\n",
    "        cleaned_output = output_text.split(answer_prefix)[-1].strip()\n",
    "    else:\n",
    "        cleaned_output = output_text.strip()\n",
    "\n",
    "    first_sentence = cleaned_output.split('.')[0] + '.' if '.' in cleaned_output else cleaned_output\n",
    "    outputs.append(first_sentence)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583b2579-af14-4bef-b356-4d42e059f662",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1f50e32-583b-4f30-b13e-733b9ce2f949",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "outputs_7b = []\n",
    "outputs_tiny = []\n",
    "outputs_13b = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caea6a20-681a-4f4e-afa4-2b381dce00ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-7b | CURRENT IDX: 396 | Length: 397\n",
      "Llama-7b | CURRENT IDX: 397 | Length: 398\n",
      "Llama-7b | CURRENT IDX: 398 | Length: 399\n",
      "Llama-7b | CURRENT IDX: 399 | Length: 400\n",
      "Llama-7b | CURRENT IDX: 400 | Length: 401\n",
      "Llama-7b | CURRENT IDX: 401 | Length: 402\n",
      "Llama-7b | CURRENT IDX: 402 | Length: 403\n",
      "Llama-7b | CURRENT IDX: 403 | Length: 404\n",
      "Llama-7b | CURRENT IDX: 404 | Length: 405\n",
      "Llama-7b | CURRENT IDX: 405 | Length: 406\n",
      "Llama-7b | CURRENT IDX: 406 | Length: 407\n",
      "Llama-7b | CURRENT IDX: 407 | Length: 408\n",
      "Llama-7b | CURRENT IDX: 408 | Length: 409\n",
      "Llama-7b | CURRENT IDX: 409 | Length: 410\n",
      "Llama-7b | CURRENT IDX: 410 | Length: 411\n",
      "Llama-7b | CURRENT IDX: 411 | Length: 412\n",
      "Llama-7b | CURRENT IDX: 412 | Length: 413\n",
      "Llama-7b | CURRENT IDX: 413 | Length: 414\n",
      "Llama-7b | CURRENT IDX: 414 | Length: 415\n",
      "Llama-7b | CURRENT IDX: 415 | Length: 416\n",
      "Llama-7b | CURRENT IDX: 416 | Length: 417\n",
      "Llama-7b | CURRENT IDX: 417 | Length: 418\n",
      "Llama-7b | CURRENT IDX: 418 | Length: 419\n",
      "Llama-7b | CURRENT IDX: 419 | Length: 420\n",
      "Llama-7b | CURRENT IDX: 420 | Length: 421\n",
      "Llama-7b | CURRENT IDX: 421 | Length: 422\n",
      "Llama-7b | CURRENT IDX: 422 | Length: 423\n",
      "Llama-7b | CURRENT IDX: 423 | Length: 424\n",
      "Llama-7b | CURRENT IDX: 424 | Length: 425\n",
      "Llama-7b | CURRENT IDX: 425 | Length: 426\n",
      "Llama-7b | CURRENT IDX: 426 | Length: 427\n",
      "Llama-7b | CURRENT IDX: 427 | Length: 428\n",
      "Llama-7b | CURRENT IDX: 428 | Length: 429\n",
      "Llama-7b | CURRENT IDX: 429 | Length: 430\n",
      "Llama-7b | CURRENT IDX: 430 | Length: 431\n",
      "Llama-7b | CURRENT IDX: 431 | Length: 432\n",
      "Llama-7b | CURRENT IDX: 432 | Length: 433\n",
      "Llama-7b | CURRENT IDX: 433 | Length: 434\n",
      "Llama-7b | CURRENT IDX: 434 | Length: 435\n",
      "Llama-7b | CURRENT IDX: 435 | Length: 436\n",
      "Llama-7b | CURRENT IDX: 436 | Length: 437\n",
      "Llama-7b | CURRENT IDX: 437 | Length: 438\n",
      "Llama-7b | CURRENT IDX: 438 | Length: 439\n",
      "Llama-7b | CURRENT IDX: 439 | Length: 440\n",
      "Llama-7b | CURRENT IDX: 440 | Length: 441\n",
      "Llama-7b | CURRENT IDX: 441 | Length: 442\n",
      "Llama-7b | CURRENT IDX: 442 | Length: 443\n",
      "Llama-7b | CURRENT IDX: 443 | Length: 444\n",
      "Llama-7b | CURRENT IDX: 444 | Length: 445\n",
      "Llama-7b | CURRENT IDX: 445 | Length: 446\n",
      "Llama-7b | CURRENT IDX: 446 | Length: 447\n",
      "Llama-7b | CURRENT IDX: 447 | Length: 448\n",
      "Llama-7b | CURRENT IDX: 448 | Length: 449\n",
      "Llama-7b | CURRENT IDX: 449 | Length: 450\n",
      "Llama-7b | CURRENT IDX: 450 | Length: 451\n",
      "Llama-7b | CURRENT IDX: 451 | Length: 452\n",
      "Llama-7b | CURRENT IDX: 452 | Length: 453\n",
      "Llama-7b | CURRENT IDX: 453 | Length: 454\n",
      "Llama-7b | CURRENT IDX: 454 | Length: 455\n",
      "Llama-7b | CURRENT IDX: 455 | Length: 456\n",
      "Llama-7b | CURRENT IDX: 456 | Length: 457\n",
      "Llama-7b | CURRENT IDX: 457 | Length: 458\n",
      "Llama-7b | CURRENT IDX: 458 | Length: 459\n",
      "Llama-7b | CURRENT IDX: 459 | Length: 460\n",
      "Llama-7b | CURRENT IDX: 460 | Length: 461\n",
      "Llama-7b | CURRENT IDX: 461 | Length: 462\n",
      "Llama-7b | CURRENT IDX: 462 | Length: 463\n",
      "Llama-7b | CURRENT IDX: 463 | Length: 464\n",
      "Llama-7b | CURRENT IDX: 464 | Length: 465\n",
      "Llama-7b | CURRENT IDX: 465 | Length: 466\n",
      "Llama-7b | CURRENT IDX: 466 | Length: 467\n",
      "Llama-7b | CURRENT IDX: 467 | Length: 468\n",
      "Llama-7b | CURRENT IDX: 468 | Length: 469\n",
      "Llama-7b | CURRENT IDX: 469 | Length: 470\n",
      "Llama-7b | CURRENT IDX: 470 | Length: 471\n",
      "Llama-7b | CURRENT IDX: 471 | Length: 472\n",
      "Llama-7b | CURRENT IDX: 472 | Length: 473\n",
      "Llama-7b | CURRENT IDX: 473 | Length: 474\n",
      "Llama-7b | CURRENT IDX: 474 | Length: 475\n",
      "Llama-7b | CURRENT IDX: 475 | Length: 476\n",
      "Llama-7b | CURRENT IDX: 476 | Length: 477\n",
      "Llama-7b | CURRENT IDX: 477 | Length: 478\n",
      "Llama-7b | CURRENT IDX: 478 | Length: 479\n",
      "Llama-7b | CURRENT IDX: 479 | Length: 480\n",
      "Llama-7b | CURRENT IDX: 480 | Length: 481\n",
      "Llama-7b | CURRENT IDX: 481 | Length: 482\n",
      "Llama-7b | CURRENT IDX: 482 | Length: 483\n",
      "Llama-7b | CURRENT IDX: 483 | Length: 484\n",
      "Llama-7b | CURRENT IDX: 484 | Length: 485\n",
      "Llama-7b | CURRENT IDX: 485 | Length: 486\n",
      "Llama-7b | CURRENT IDX: 486 | Length: 487\n",
      "Llama-7b | CURRENT IDX: 487 | Length: 488\n",
      "Llama-7b | CURRENT IDX: 488 | Length: 489\n",
      "Llama-7b | CURRENT IDX: 489 | Length: 490\n",
      "Llama-7b | CURRENT IDX: 490 | Length: 491\n",
      "Llama-7b | CURRENT IDX: 491 | Length: 492\n",
      "Llama-7b | CURRENT IDX: 492 | Length: 493\n",
      "Llama-7b | CURRENT IDX: 493 | Length: 494\n",
      "Llama-7b | CURRENT IDX: 494 | Length: 495\n",
      "Llama-7b | CURRENT IDX: 495 | Length: 496\n",
      "Llama-7b | CURRENT IDX: 496 | Length: 497\n",
      "Llama-7b | CURRENT IDX: 497 | Length: 498\n",
      "Llama-7b | CURRENT IDX: 498 | Length: 499\n",
      "Llama-7b | CURRENT IDX: 499 | Length: 500\n",
      "Llama-7b | CURRENT IDX: 500 | Length: 501\n",
      "Llama-7b | CURRENT IDX: 501 | Length: 502\n",
      "Llama-7b | CURRENT IDX: 502 | Length: 503\n",
      "Llama-7b | CURRENT IDX: 503 | Length: 504\n",
      "Llama-7b | CURRENT IDX: 504 | Length: 505\n",
      "Llama-7b | CURRENT IDX: 505 | Length: 506\n",
      "Llama-7b | CURRENT IDX: 506 | Length: 507\n",
      "Llama-7b | CURRENT IDX: 507 | Length: 508\n",
      "Llama-7b | CURRENT IDX: 508 | Length: 509\n",
      "Llama-7b | CURRENT IDX: 509 | Length: 510\n",
      "Llama-7b | CURRENT IDX: 510 | Length: 511\n",
      "Llama-7b | CURRENT IDX: 511 | Length: 512\n",
      "Llama-7b | CURRENT IDX: 512 | Length: 513\n",
      "Llama-7b | CURRENT IDX: 513 | Length: 514\n",
      "Llama-7b | CURRENT IDX: 514 | Length: 515\n",
      "Llama-7b | CURRENT IDX: 515 | Length: 516\n",
      "Llama-7b | CURRENT IDX: 516 | Length: 517\n",
      "Llama-7b | CURRENT IDX: 517 | Length: 518\n",
      "Llama-7b | CURRENT IDX: 518 | Length: 519\n",
      "Llama-7b | CURRENT IDX: 519 | Length: 520\n",
      "Llama-7b | CURRENT IDX: 520 | Length: 521\n",
      "Llama-7b | CURRENT IDX: 521 | Length: 522\n",
      "Llama-7b | CURRENT IDX: 522 | Length: 523\n",
      "Llama-7b | CURRENT IDX: 523 | Length: 524\n",
      "Llama-7b | CURRENT IDX: 524 | Length: 525\n",
      "Llama-7b | CURRENT IDX: 525 | Length: 526\n",
      "Llama-7b | CURRENT IDX: 526 | Length: 527\n",
      "Llama-7b | CURRENT IDX: 527 | Length: 528\n",
      "Llama-7b | CURRENT IDX: 528 | Length: 529\n",
      "Llama-7b | CURRENT IDX: 529 | Length: 530\n",
      "Llama-7b | CURRENT IDX: 530 | Length: 531\n",
      "Llama-7b | CURRENT IDX: 531 | Length: 532\n",
      "Llama-7b | CURRENT IDX: 532 | Length: 533\n",
      "Llama-7b | CURRENT IDX: 533 | Length: 534\n",
      "Llama-7b | CURRENT IDX: 534 | Length: 535\n",
      "Llama-7b | CURRENT IDX: 535 | Length: 536\n",
      "Llama-7b | CURRENT IDX: 536 | Length: 537\n",
      "Llama-7b | CURRENT IDX: 537 | Length: 538\n",
      "Llama-7b | CURRENT IDX: 538 | Length: 539\n",
      "Llama-7b | CURRENT IDX: 539 | Length: 540\n",
      "Llama-7b | CURRENT IDX: 540 | Length: 541\n",
      "Llama-7b | CURRENT IDX: 541 | Length: 542\n",
      "Llama-7b | CURRENT IDX: 542 | Length: 543\n",
      "Llama-7b | CURRENT IDX: 543 | Length: 544\n",
      "Llama-7b | CURRENT IDX: 544 | Length: 545\n",
      "Llama-7b | CURRENT IDX: 545 | Length: 546\n",
      "Llama-7b | CURRENT IDX: 546 | Length: 547\n",
      "Llama-7b | CURRENT IDX: 547 | Length: 548\n",
      "Llama-7b | CURRENT IDX: 548 | Length: 549\n",
      "Llama-7b | CURRENT IDX: 549 | Length: 550\n",
      "Llama-7b | CURRENT IDX: 550 | Length: 551\n",
      "Llama-7b | CURRENT IDX: 551 | Length: 552\n",
      "Llama-7b | CURRENT IDX: 552 | Length: 553\n",
      "Llama-7b | CURRENT IDX: 553 | Length: 554\n",
      "Llama-7b | CURRENT IDX: 554 | Length: 555\n",
      "Llama-7b | CURRENT IDX: 555 | Length: 556\n",
      "Llama-7b | CURRENT IDX: 556 | Length: 557\n",
      "Llama-7b | CURRENT IDX: 557 | Length: 558\n",
      "Llama-7b | CURRENT IDX: 558 | Length: 559\n",
      "Llama-7b | CURRENT IDX: 559 | Length: 560\n",
      "Llama-7b | CURRENT IDX: 560 | Length: 561\n",
      "Llama-7b | CURRENT IDX: 561 | Length: 562\n",
      "Llama-7b | CURRENT IDX: 562 | Length: 563\n",
      "Llama-7b | CURRENT IDX: 563 | Length: 564\n",
      "Llama-7b | CURRENT IDX: 564 | Length: 565\n",
      "Llama-7b | CURRENT IDX: 565 | Length: 566\n",
      "Llama-7b | CURRENT IDX: 566 | Length: 567\n",
      "Llama-7b | CURRENT IDX: 567 | Length: 568\n",
      "Llama-7b | CURRENT IDX: 568 | Length: 569\n",
      "Llama-7b | CURRENT IDX: 569 | Length: 570\n",
      "Llama-7b | CURRENT IDX: 570 | Length: 571\n",
      "Llama-7b | CURRENT IDX: 571 | Length: 572\n",
      "Llama-7b | CURRENT IDX: 572 | Length: 573\n",
      "Llama-7b | CURRENT IDX: 573 | Length: 574\n",
      "Llama-7b | CURRENT IDX: 574 | Length: 575\n",
      "Llama-7b | CURRENT IDX: 575 | Length: 576\n",
      "Llama-7b | CURRENT IDX: 576 | Length: 577\n",
      "Llama-7b | CURRENT IDX: 577 | Length: 578\n",
      "Llama-7b | CURRENT IDX: 578 | Length: 579\n",
      "Llama-7b | CURRENT IDX: 579 | Length: 580\n",
      "Llama-7b | CURRENT IDX: 580 | Length: 581\n",
      "Llama-7b | CURRENT IDX: 581 | Length: 582\n",
      "Llama-7b | CURRENT IDX: 582 | Length: 583\n",
      "Llama-7b | CURRENT IDX: 583 | Length: 584\n",
      "Llama-7b | CURRENT IDX: 584 | Length: 585\n",
      "Llama-7b | CURRENT IDX: 585 | Length: 586\n",
      "Llama-7b | CURRENT IDX: 586 | Length: 587\n",
      "Llama-7b | CURRENT IDX: 587 | Length: 588\n",
      "Llama-7b | CURRENT IDX: 588 | Length: 589\n",
      "Llama-7b | CURRENT IDX: 589 | Length: 590\n",
      "Llama-7b | CURRENT IDX: 590 | Length: 591\n",
      "Llama-7b | CURRENT IDX: 591 | Length: 592\n",
      "Llama-7b | CURRENT IDX: 592 | Length: 593\n",
      "Llama-7b | CURRENT IDX: 593 | Length: 594\n",
      "Llama-7b | CURRENT IDX: 594 | Length: 595\n",
      "Llama-7b | CURRENT IDX: 595 | Length: 596\n",
      "Llama-7b | CURRENT IDX: 596 | Length: 597\n",
      "Llama-7b | CURRENT IDX: 597 | Length: 598\n",
      "Llama-7b | CURRENT IDX: 598 | Length: 599\n",
      "Llama-7b | CURRENT IDX: 599 | Length: 600\n",
      "Llama-7b | CURRENT IDX: 600 | Length: 601\n",
      "Llama-7b | CURRENT IDX: 601 | Length: 602\n",
      "Llama-7b | CURRENT IDX: 602 | Length: 603\n",
      "Llama-7b | CURRENT IDX: 603 | Length: 604\n",
      "Llama-7b | CURRENT IDX: 604 | Length: 605\n",
      "Llama-7b | CURRENT IDX: 605 | Length: 606\n",
      "Llama-7b | CURRENT IDX: 606 | Length: 607\n",
      "Llama-7b | CURRENT IDX: 607 | Length: 608\n",
      "Llama-7b | CURRENT IDX: 608 | Length: 609\n",
      "Llama-7b | CURRENT IDX: 609 | Length: 610\n",
      "Llama-7b | CURRENT IDX: 610 | Length: 611\n",
      "Llama-7b | CURRENT IDX: 611 | Length: 612\n",
      "Llama-7b | CURRENT IDX: 612 | Length: 613\n",
      "Llama-7b | CURRENT IDX: 613 | Length: 614\n",
      "Llama-7b | CURRENT IDX: 614 | Length: 615\n",
      "Llama-7b | CURRENT IDX: 615 | Length: 616\n",
      "Llama-7b | CURRENT IDX: 616 | Length: 617\n",
      "Llama-7b | CURRENT IDX: 617 | Length: 618\n",
      "Llama-7b | CURRENT IDX: 618 | Length: 619\n",
      "Llama-7b | CURRENT IDX: 619 | Length: 620\n",
      "Llama-7b | CURRENT IDX: 620 | Length: 621\n",
      "Llama-7b | CURRENT IDX: 621 | Length: 622\n",
      "Llama-7b | CURRENT IDX: 622 | Length: 623\n",
      "Llama-7b | CURRENT IDX: 623 | Length: 624\n",
      "Llama-7b | CURRENT IDX: 624 | Length: 625\n",
      "Llama-7b | CURRENT IDX: 625 | Length: 626\n",
      "Llama-7b | CURRENT IDX: 626 | Length: 627\n",
      "Llama-7b | CURRENT IDX: 627 | Length: 628\n",
      "Llama-7b | CURRENT IDX: 628 | Length: 629\n",
      "Llama-7b | CURRENT IDX: 629 | Length: 630\n",
      "Llama-7b | CURRENT IDX: 630 | Length: 631\n",
      "Llama-7b | CURRENT IDX: 631 | Length: 632\n",
      "Llama-7b | CURRENT IDX: 632 | Length: 633\n",
      "Llama-7b | CURRENT IDX: 633 | Length: 634\n",
      "Llama-7b | CURRENT IDX: 634 | Length: 635\n",
      "Llama-7b | CURRENT IDX: 635 | Length: 636\n",
      "Llama-7b | CURRENT IDX: 636 | Length: 637\n",
      "Llama-7b | CURRENT IDX: 637 | Length: 638\n",
      "Llama-7b | CURRENT IDX: 638 | Length: 639\n",
      "Llama-7b | CURRENT IDX: 639 | Length: 640\n",
      "Llama-7b | CURRENT IDX: 640 | Length: 641\n",
      "Llama-7b | CURRENT IDX: 641 | Length: 642\n",
      "Llama-7b | CURRENT IDX: 642 | Length: 643\n",
      "Llama-7b | CURRENT IDX: 643 | Length: 644\n",
      "Llama-7b | CURRENT IDX: 644 | Length: 645\n",
      "Llama-7b | CURRENT IDX: 645 | Length: 646\n",
      "Llama-7b | CURRENT IDX: 646 | Length: 647\n",
      "Llama-7b | CURRENT IDX: 647 | Length: 648\n",
      "Llama-7b | CURRENT IDX: 648 | Length: 649\n",
      "Llama-7b | CURRENT IDX: 649 | Length: 650\n",
      "Llama-7b | CURRENT IDX: 650 | Length: 651\n",
      "Llama-7b | CURRENT IDX: 651 | Length: 652\n",
      "Llama-7b | CURRENT IDX: 652 | Length: 653\n",
      "Llama-7b | CURRENT IDX: 653 | Length: 654\n",
      "Llama-7b | CURRENT IDX: 654 | Length: 655\n",
      "Llama-7b | CURRENT IDX: 655 | Length: 656\n",
      "Llama-7b | CURRENT IDX: 656 | Length: 657\n",
      "Llama-7b | CURRENT IDX: 657 | Length: 658\n",
      "Llama-7b | CURRENT IDX: 658 | Length: 659\n",
      "Llama-7b | CURRENT IDX: 659 | Length: 660\n",
      "Llama-7b | CURRENT IDX: 660 | Length: 661\n",
      "Llama-7b | CURRENT IDX: 661 | Length: 662\n",
      "Llama-7b | CURRENT IDX: 662 | Length: 663\n",
      "Llama-7b | CURRENT IDX: 663 | Length: 664\n",
      "Llama-7b | CURRENT IDX: 664 | Length: 665\n",
      "Llama-7b | CURRENT IDX: 665 | Length: 666\n",
      "Llama-7b | CURRENT IDX: 666 | Length: 667\n",
      "Llama-7b | CURRENT IDX: 667 | Length: 668\n",
      "Llama-7b | CURRENT IDX: 668 | Length: 669\n",
      "Llama-7b | CURRENT IDX: 669 | Length: 670\n",
      "Llama-7b | CURRENT IDX: 670 | Length: 671\n",
      "Llama-7b | CURRENT IDX: 671 | Length: 672\n",
      "Llama-7b | CURRENT IDX: 672 | Length: 673\n",
      "Llama-7b | CURRENT IDX: 673 | Length: 674\n",
      "Llama-7b | CURRENT IDX: 674 | Length: 675\n",
      "Llama-7b | CURRENT IDX: 675 | Length: 676\n",
      "Llama-7b | CURRENT IDX: 676 | Length: 677\n",
      "Llama-7b | CURRENT IDX: 677 | Length: 678\n",
      "Llama-7b | CURRENT IDX: 678 | Length: 679\n",
      "Llama-7b | CURRENT IDX: 679 | Length: 680\n",
      "Llama-7b | CURRENT IDX: 680 | Length: 681\n",
      "Llama-7b | CURRENT IDX: 681 | Length: 682\n",
      "Llama-7b | CURRENT IDX: 682 | Length: 683\n",
      "Llama-7b | CURRENT IDX: 683 | Length: 684\n",
      "Llama-7b | CURRENT IDX: 684 | Length: 685\n",
      "Llama-7b | CURRENT IDX: 685 | Length: 686\n",
      "Llama-7b | CURRENT IDX: 686 | Length: 687\n",
      "Llama-7b | CURRENT IDX: 687 | Length: 688\n",
      "Llama-7b | CURRENT IDX: 688 | Length: 689\n",
      "Llama-7b | CURRENT IDX: 689 | Length: 690\n",
      "Llama-7b | CURRENT IDX: 690 | Length: 691\n",
      "Llama-7b | CURRENT IDX: 691 | Length: 692\n",
      "Llama-7b | CURRENT IDX: 692 | Length: 693\n",
      "Llama-7b | CURRENT IDX: 693 | Length: 694\n",
      "Llama-7b | CURRENT IDX: 694 | Length: 695\n",
      "Llama-7b | CURRENT IDX: 695 | Length: 696\n",
      "Llama-7b | CURRENT IDX: 696 | Length: 697\n",
      "Llama-7b | CURRENT IDX: 697 | Length: 698\n",
      "Llama-7b | CURRENT IDX: 698 | Length: 699\n",
      "Llama-7b | CURRENT IDX: 699 | Length: 700\n",
      "Llama-7b | CURRENT IDX: 700 | Length: 701\n",
      "Llama-7b | CURRENT IDX: 701 | Length: 702\n",
      "Llama-7b | CURRENT IDX: 702 | Length: 703\n",
      "Llama-7b | CURRENT IDX: 703 | Length: 704\n",
      "Llama-7b | CURRENT IDX: 704 | Length: 705\n",
      "Llama-7b | CURRENT IDX: 705 | Length: 706\n",
      "Llama-7b | CURRENT IDX: 706 | Length: 707\n",
      "Llama-7b | CURRENT IDX: 707 | Length: 708\n",
      "Llama-7b | CURRENT IDX: 708 | Length: 709\n",
      "Llama-7b | CURRENT IDX: 709 | Length: 710\n",
      "Llama-7b | CURRENT IDX: 710 | Length: 711\n",
      "Llama-7b | CURRENT IDX: 711 | Length: 712\n",
      "Llama-7b | CURRENT IDX: 712 | Length: 713\n",
      "Llama-7b | CURRENT IDX: 713 | Length: 714\n",
      "Llama-7b | CURRENT IDX: 714 | Length: 715\n",
      "Llama-7b | CURRENT IDX: 715 | Length: 716\n",
      "Llama-7b | CURRENT IDX: 716 | Length: 717\n",
      "Llama-7b | CURRENT IDX: 717 | Length: 718\n",
      "Llama-7b | CURRENT IDX: 718 | Length: 719\n",
      "Llama-7b | CURRENT IDX: 719 | Length: 720\n",
      "Llama-7b | CURRENT IDX: 720 | Length: 721\n",
      "Llama-7b | CURRENT IDX: 721 | Length: 722\n",
      "Llama-7b | CURRENT IDX: 722 | Length: 723\n",
      "Llama-7b | CURRENT IDX: 723 | Length: 724\n",
      "Llama-7b | CURRENT IDX: 724 | Length: 725\n",
      "Llama-7b | CURRENT IDX: 725 | Length: 726\n",
      "Llama-7b | CURRENT IDX: 726 | Length: 727\n",
      "Llama-7b | CURRENT IDX: 727 | Length: 728\n",
      "Llama-7b | CURRENT IDX: 728 | Length: 729\n",
      "Llama-7b | CURRENT IDX: 729 | Length: 730\n",
      "Llama-7b | CURRENT IDX: 730 | Length: 731\n",
      "Llama-7b | CURRENT IDX: 731 | Length: 732\n",
      "Llama-7b | CURRENT IDX: 732 | Length: 733\n",
      "Llama-7b | CURRENT IDX: 733 | Length: 734\n",
      "Llama-7b | CURRENT IDX: 734 | Length: 735\n",
      "Llama-7b | CURRENT IDX: 735 | Length: 736\n",
      "Llama-7b | CURRENT IDX: 736 | Length: 737\n",
      "Llama-7b | CURRENT IDX: 737 | Length: 738\n",
      "Llama-7b | CURRENT IDX: 738 | Length: 739\n",
      "Llama-7b | CURRENT IDX: 739 | Length: 740\n",
      "Llama-7b | CURRENT IDX: 740 | Length: 741\n",
      "Llama-7b | CURRENT IDX: 741 | Length: 742\n",
      "Llama-7b | CURRENT IDX: 742 | Length: 743\n",
      "Llama-7b | CURRENT IDX: 743 | Length: 744\n",
      "Llama-7b | CURRENT IDX: 744 | Length: 745\n",
      "Llama-7b | CURRENT IDX: 745 | Length: 746\n",
      "Llama-7b | CURRENT IDX: 746 | Length: 747\n",
      "Llama-7b | CURRENT IDX: 747 | Length: 748\n",
      "Llama-7b | CURRENT IDX: 748 | Length: 749\n",
      "Llama-7b | CURRENT IDX: 749 | Length: 750\n",
      "Llama-7b | CURRENT IDX: 750 | Length: 751\n",
      "Llama-7b | CURRENT IDX: 751 | Length: 752\n",
      "Llama-7b | CURRENT IDX: 752 | Length: 753\n",
      "Llama-7b | CURRENT IDX: 753 | Length: 754\n",
      "Llama-7b | CURRENT IDX: 754 | Length: 755\n",
      "Llama-7b | CURRENT IDX: 755 | Length: 756\n",
      "Llama-7b | CURRENT IDX: 756 | Length: 757\n",
      "Llama-7b | CURRENT IDX: 757 | Length: 758\n",
      "Llama-7b | CURRENT IDX: 758 | Length: 759\n",
      "Llama-7b | CURRENT IDX: 759 | Length: 760\n",
      "Llama-7b | CURRENT IDX: 760 | Length: 761\n",
      "Llama-7b | CURRENT IDX: 761 | Length: 762\n",
      "Llama-7b | CURRENT IDX: 762 | Length: 763\n",
      "Llama-7b | CURRENT IDX: 763 | Length: 764\n",
      "Llama-7b | CURRENT IDX: 764 | Length: 765\n",
      "Llama-7b | CURRENT IDX: 765 | Length: 766\n",
      "Llama-7b | CURRENT IDX: 766 | Length: 767\n",
      "Llama-7b | CURRENT IDX: 767 | Length: 768\n",
      "Llama-7b | CURRENT IDX: 768 | Length: 769\n",
      "Llama-7b | CURRENT IDX: 769 | Length: 770\n",
      "Llama-7b | CURRENT IDX: 770 | Length: 771\n",
      "Llama-7b | CURRENT IDX: 771 | Length: 772\n",
      "Llama-7b | CURRENT IDX: 772 | Length: 773\n",
      "Llama-7b | CURRENT IDX: 773 | Length: 774\n",
      "Llama-7b | CURRENT IDX: 774 | Length: 775\n",
      "Llama-7b | CURRENT IDX: 775 | Length: 776\n",
      "Llama-7b | CURRENT IDX: 776 | Length: 777\n",
      "Llama-7b | CURRENT IDX: 777 | Length: 778\n",
      "Llama-7b | CURRENT IDX: 778 | Length: 779\n",
      "Llama-7b | CURRENT IDX: 779 | Length: 780\n",
      "Llama-7b | CURRENT IDX: 780 | Length: 781\n",
      "Llama-7b | CURRENT IDX: 781 | Length: 782\n",
      "Llama-7b | CURRENT IDX: 782 | Length: 783\n",
      "Llama-7b | CURRENT IDX: 783 | Length: 784\n",
      "Llama-7b | CURRENT IDX: 784 | Length: 785\n",
      "Llama-7b | CURRENT IDX: 785 | Length: 786\n",
      "Llama-7b | CURRENT IDX: 786 | Length: 787\n",
      "Llama-7b | CURRENT IDX: 787 | Length: 788\n",
      "Llama-7b | CURRENT IDX: 788 | Length: 789\n",
      "Llama-7b | CURRENT IDX: 789 | Length: 790\n",
      "Llama-7b | CURRENT IDX: 790 | Length: 791\n",
      "Llama-7b | CURRENT IDX: 791 | Length: 792\n",
      "Llama-7b | CURRENT IDX: 792 | Length: 793\n",
      "Llama-7b | CURRENT IDX: 793 | Length: 794\n",
      "Llama-7b | CURRENT IDX: 794 | Length: 795\n",
      "Llama-7b | CURRENT IDX: 795 | Length: 796\n",
      "Llama-7b | CURRENT IDX: 796 | Length: 797\n",
      "Llama-7b | CURRENT IDX: 797 | Length: 798\n",
      "Llama-7b | CURRENT IDX: 798 | Length: 799\n",
      "Llama-7b | CURRENT IDX: 799 | Length: 800\n",
      "Llama-7b | CURRENT IDX: 800 | Length: 801\n",
      "Llama-7b | CURRENT IDX: 801 | Length: 802\n",
      "Llama-7b | CURRENT IDX: 802 | Length: 803\n",
      "Llama-7b | CURRENT IDX: 803 | Length: 804\n",
      "Llama-7b | CURRENT IDX: 804 | Length: 805\n",
      "Llama-7b | CURRENT IDX: 805 | Length: 806\n",
      "Llama-7b | CURRENT IDX: 806 | Length: 807\n",
      "Llama-7b | CURRENT IDX: 807 | Length: 808\n",
      "Llama-7b | CURRENT IDX: 808 | Length: 809\n",
      "Llama-7b | CURRENT IDX: 809 | Length: 810\n",
      "Llama-7b | CURRENT IDX: 810 | Length: 811\n",
      "Llama-7b | CURRENT IDX: 811 | Length: 812\n",
      "Llama-7b | CURRENT IDX: 812 | Length: 813\n",
      "Llama-7b | CURRENT IDX: 813 | Length: 814\n",
      "Llama-7b | CURRENT IDX: 814 | Length: 815\n",
      "Llama-7b | CURRENT IDX: 815 | Length: 816\n",
      "Llama-7b | CURRENT IDX: 816 | Length: 817\n",
      "Llama-7b | CURRENT IDX: 817 | Length: 818\n",
      "Llama-7b | CURRENT IDX: 818 | Length: 819\n",
      "Llama-7b | CURRENT IDX: 819 | Length: 820\n",
      "Llama-7b | CURRENT IDX: 820 | Length: 821\n",
      "Llama-7b | CURRENT IDX: 821 | Length: 822\n",
      "Llama-7b | CURRENT IDX: 822 | Length: 823\n",
      "Llama-7b | CURRENT IDX: 823 | Length: 824\n",
      "Llama-7b | CURRENT IDX: 824 | Length: 825\n",
      "Llama-7b | CURRENT IDX: 825 | Length: 826\n",
      "Llama-7b | CURRENT IDX: 826 | Length: 827\n",
      "Llama-7b | CURRENT IDX: 827 | Length: 828\n",
      "Llama-7b | CURRENT IDX: 828 | Length: 829\n",
      "Llama-7b | CURRENT IDX: 829 | Length: 830\n",
      "Llama-7b | CURRENT IDX: 830 | Length: 831\n",
      "Llama-7b | CURRENT IDX: 831 | Length: 832\n",
      "Llama-7b | CURRENT IDX: 832 | Length: 833\n",
      "Llama-7b | CURRENT IDX: 833 | Length: 834\n",
      "Llama-7b | CURRENT IDX: 834 | Length: 835\n",
      "Llama-7b | CURRENT IDX: 835 | Length: 836\n",
      "Llama-7b | CURRENT IDX: 836 | Length: 837\n",
      "Llama-7b | CURRENT IDX: 837 | Length: 838\n",
      "Llama-7b | CURRENT IDX: 838 | Length: 839\n",
      "Llama-7b | CURRENT IDX: 839 | Length: 840\n",
      "Llama-7b | CURRENT IDX: 840 | Length: 841\n",
      "Llama-7b | CURRENT IDX: 841 | Length: 842\n",
      "Llama-7b | CURRENT IDX: 842 | Length: 843\n",
      "Llama-7b | CURRENT IDX: 843 | Length: 844\n",
      "Llama-7b | CURRENT IDX: 844 | Length: 845\n",
      "Llama-7b | CURRENT IDX: 845 | Length: 846\n",
      "Llama-7b | CURRENT IDX: 846 | Length: 847\n",
      "Llama-7b | CURRENT IDX: 847 | Length: 848\n",
      "Llama-7b | CURRENT IDX: 848 | Length: 849\n",
      "Llama-7b | CURRENT IDX: 849 | Length: 850\n",
      "Llama-7b | CURRENT IDX: 850 | Length: 851\n",
      "Llama-7b | CURRENT IDX: 851 | Length: 852\n",
      "Llama-7b | CURRENT IDX: 852 | Length: 853\n",
      "Llama-7b | CURRENT IDX: 853 | Length: 854\n",
      "Llama-7b | CURRENT IDX: 854 | Length: 855\n",
      "Llama-7b | CURRENT IDX: 855 | Length: 856\n",
      "Llama-7b | CURRENT IDX: 856 | Length: 857\n",
      "Llama-7b | CURRENT IDX: 857 | Length: 858\n",
      "Llama-7b | CURRENT IDX: 858 | Length: 859\n",
      "Llama-7b | CURRENT IDX: 859 | Length: 860\n",
      "Llama-7b | CURRENT IDX: 860 | Length: 861\n",
      "Llama-7b | CURRENT IDX: 861 | Length: 862\n",
      "Llama-7b | CURRENT IDX: 862 | Length: 863\n",
      "Llama-7b | CURRENT IDX: 863 | Length: 864\n",
      "Llama-7b | CURRENT IDX: 864 | Length: 865\n",
      "Llama-7b | CURRENT IDX: 865 | Length: 866\n",
      "Llama-7b | CURRENT IDX: 866 | Length: 867\n",
      "Llama-7b | CURRENT IDX: 867 | Length: 868\n",
      "Llama-7b | CURRENT IDX: 868 | Length: 869\n",
      "Llama-7b | CURRENT IDX: 869 | Length: 870\n",
      "Llama-7b | CURRENT IDX: 870 | Length: 871\n",
      "Llama-7b | CURRENT IDX: 871 | Length: 872\n",
      "Llama-7b | CURRENT IDX: 872 | Length: 873\n",
      "Llama-7b | CURRENT IDX: 873 | Length: 874\n",
      "Llama-7b | CURRENT IDX: 874 | Length: 875\n",
      "Llama-7b | CURRENT IDX: 875 | Length: 876\n",
      "Llama-7b | CURRENT IDX: 876 | Length: 877\n",
      "Llama-7b | CURRENT IDX: 877 | Length: 878\n",
      "Llama-7b | CURRENT IDX: 878 | Length: 879\n",
      "Llama-7b | CURRENT IDX: 879 | Length: 880\n",
      "Llama-7b | CURRENT IDX: 880 | Length: 881\n",
      "Llama-7b | CURRENT IDX: 881 | Length: 882\n",
      "Llama-7b | CURRENT IDX: 882 | Length: 883\n",
      "Llama-7b | CURRENT IDX: 883 | Length: 884\n",
      "Llama-7b | CURRENT IDX: 884 | Length: 885\n",
      "Llama-7b | CURRENT IDX: 885 | Length: 886\n",
      "Llama-7b | CURRENT IDX: 886 | Length: 887\n",
      "Llama-7b | CURRENT IDX: 887 | Length: 888\n",
      "Llama-7b | CURRENT IDX: 888 | Length: 889\n",
      "Llama-7b | CURRENT IDX: 889 | Length: 890\n",
      "Llama-7b | CURRENT IDX: 890 | Length: 891\n",
      "Llama-7b | CURRENT IDX: 891 | Length: 892\n",
      "Llama-7b | CURRENT IDX: 892 | Length: 893\n",
      "Llama-7b | CURRENT IDX: 893 | Length: 894\n",
      "Llama-7b | CURRENT IDX: 894 | Length: 895\n",
      "Llama-7b | CURRENT IDX: 895 | Length: 896\n",
      "Llama-7b | CURRENT IDX: 896 | Length: 897\n",
      "Llama-7b | CURRENT IDX: 897 | Length: 898\n",
      "Llama-7b | CURRENT IDX: 898 | Length: 899\n",
      "Llama-7b | CURRENT IDX: 899 | Length: 900\n",
      "Llama-7b | CURRENT IDX: 900 | Length: 901\n",
      "Llama-7b | CURRENT IDX: 901 | Length: 902\n",
      "Llama-7b | CURRENT IDX: 902 | Length: 903\n",
      "Llama-7b | CURRENT IDX: 903 | Length: 904\n",
      "Llama-7b | CURRENT IDX: 904 | Length: 905\n",
      "Llama-7b | CURRENT IDX: 905 | Length: 906\n",
      "Llama-7b | CURRENT IDX: 906 | Length: 907\n",
      "Llama-7b | CURRENT IDX: 907 | Length: 908\n",
      "Llama-7b | CURRENT IDX: 908 | Length: 909\n",
      "Llama-7b | CURRENT IDX: 909 | Length: 910\n",
      "Llama-7b | CURRENT IDX: 910 | Length: 911\n",
      "Llama-7b | CURRENT IDX: 911 | Length: 912\n",
      "Llama-7b | CURRENT IDX: 912 | Length: 913\n",
      "Llama-7b | CURRENT IDX: 913 | Length: 914\n",
      "Llama-7b | CURRENT IDX: 914 | Length: 915\n",
      "Llama-7b | CURRENT IDX: 915 | Length: 916\n",
      "Llama-7b | CURRENT IDX: 916 | Length: 917\n",
      "Llama-7b | CURRENT IDX: 917 | Length: 918\n",
      "Llama-7b | CURRENT IDX: 918 | Length: 919\n",
      "Llama-7b | CURRENT IDX: 919 | Length: 920\n",
      "Llama-7b | CURRENT IDX: 920 | Length: 921\n",
      "Llama-7b | CURRENT IDX: 921 | Length: 922\n",
      "Llama-7b | CURRENT IDX: 922 | Length: 923\n",
      "Llama-7b | CURRENT IDX: 923 | Length: 924\n",
      "Llama-7b | CURRENT IDX: 924 | Length: 925\n",
      "Llama-7b | CURRENT IDX: 925 | Length: 926\n",
      "Llama-7b | CURRENT IDX: 926 | Length: 927\n",
      "Llama-7b | CURRENT IDX: 927 | Length: 928\n",
      "Llama-7b | CURRENT IDX: 928 | Length: 929\n",
      "Llama-7b | CURRENT IDX: 929 | Length: 930\n",
      "Llama-7b | CURRENT IDX: 930 | Length: 931\n",
      "Llama-7b | CURRENT IDX: 931 | Length: 932\n",
      "Llama-7b | CURRENT IDX: 932 | Length: 933\n",
      "Llama-7b | CURRENT IDX: 933 | Length: 934\n",
      "Llama-7b | CURRENT IDX: 934 | Length: 935\n",
      "Llama-7b | CURRENT IDX: 935 | Length: 936\n",
      "Llama-7b | CURRENT IDX: 936 | Length: 937\n",
      "Llama-7b | CURRENT IDX: 937 | Length: 938\n",
      "Llama-7b | CURRENT IDX: 938 | Length: 939\n",
      "Llama-7b | CURRENT IDX: 939 | Length: 940\n",
      "Llama-7b | CURRENT IDX: 940 | Length: 941\n",
      "Llama-7b | CURRENT IDX: 941 | Length: 942\n",
      "Llama-7b | CURRENT IDX: 942 | Length: 943\n",
      "Llama-7b | CURRENT IDX: 943 | Length: 944\n",
      "Llama-7b | CURRENT IDX: 944 | Length: 945\n",
      "Llama-7b | CURRENT IDX: 945 | Length: 946\n",
      "Llama-7b | CURRENT IDX: 946 | Length: 947\n",
      "Llama-7b | CURRENT IDX: 947 | Length: 948\n",
      "Llama-7b | CURRENT IDX: 948 | Length: 949\n",
      "Llama-7b | CURRENT IDX: 949 | Length: 950\n",
      "Llama-7b | CURRENT IDX: 950 | Length: 951\n",
      "Llama-7b | CURRENT IDX: 951 | Length: 952\n",
      "Llama-7b | CURRENT IDX: 952 | Length: 953\n",
      "Llama-7b | CURRENT IDX: 953 | Length: 954\n",
      "Llama-7b | CURRENT IDX: 954 | Length: 955\n",
      "Llama-7b | CURRENT IDX: 955 | Length: 956\n",
      "Llama-7b | CURRENT IDX: 956 | Length: 957\n",
      "Llama-7b | CURRENT IDX: 957 | Length: 958\n",
      "Llama-7b | CURRENT IDX: 958 | Length: 959\n",
      "Llama-7b | CURRENT IDX: 959 | Length: 960\n",
      "Llama-7b | CURRENT IDX: 960 | Length: 961\n",
      "Llama-7b | CURRENT IDX: 961 | Length: 962\n",
      "Llama-7b | CURRENT IDX: 962 | Length: 963\n",
      "Llama-7b | CURRENT IDX: 963 | Length: 964\n",
      "Llama-7b | CURRENT IDX: 964 | Length: 965\n",
      "Llama-7b | CURRENT IDX: 965 | Length: 966\n",
      "Llama-7b | CURRENT IDX: 966 | Length: 967\n",
      "Llama-7b | CURRENT IDX: 967 | Length: 968\n",
      "Llama-7b | CURRENT IDX: 968 | Length: 969\n",
      "Llama-7b | CURRENT IDX: 969 | Length: 970\n",
      "Llama-7b | CURRENT IDX: 970 | Length: 971\n",
      "Llama-7b | CURRENT IDX: 971 | Length: 972\n",
      "Llama-7b | CURRENT IDX: 972 | Length: 973\n",
      "Llama-7b | CURRENT IDX: 973 | Length: 974\n",
      "Llama-7b | CURRENT IDX: 974 | Length: 975\n",
      "Llama-7b | CURRENT IDX: 975 | Length: 976\n",
      "Llama-7b | CURRENT IDX: 976 | Length: 977\n",
      "Llama-7b | CURRENT IDX: 977 | Length: 978\n",
      "Llama-7b | CURRENT IDX: 978 | Length: 979\n",
      "Llama-7b | CURRENT IDX: 979 | Length: 980\n",
      "Llama-7b | CURRENT IDX: 980 | Length: 981\n",
      "Llama-7b | CURRENT IDX: 981 | Length: 982\n",
      "Llama-7b | CURRENT IDX: 982 | Length: 983\n",
      "Llama-7b | CURRENT IDX: 983 | Length: 984\n",
      "Llama-7b | CURRENT IDX: 984 | Length: 985\n",
      "Llama-7b | CURRENT IDX: 985 | Length: 986\n",
      "Llama-7b | CURRENT IDX: 986 | Length: 987\n",
      "Llama-7b | CURRENT IDX: 987 | Length: 988\n",
      "Llama-7b | CURRENT IDX: 988 | Length: 989\n",
      "Llama-7b | CURRENT IDX: 989 | Length: 990\n",
      "Llama-7b | CURRENT IDX: 990 | Length: 991\n",
      "Llama-7b | CURRENT IDX: 991 | Length: 992\n",
      "Llama-7b | CURRENT IDX: 992 | Length: 993\n",
      "Llama-7b | CURRENT IDX: 993 | Length: 994\n",
      "Llama-7b | CURRENT IDX: 994 | Length: 995\n",
      "Llama-7b | CURRENT IDX: 995 | Length: 996\n",
      "Llama-7b | CURRENT IDX: 996 | Length: 997\n",
      "Llama-7b | CURRENT IDX: 997 | Length: 998\n",
      "Llama-7b | CURRENT IDX: 998 | Length: 999\n",
      "Llama-7b | CURRENT IDX: 999 | Length: 1000\n"
     ]
    }
   ],
   "source": [
    "for current_idx in range(396, 1000):\n",
    "    input_text = wmt14_dataset[current_idx]['translation']['de']\n",
    "    output_7b = generate_output(llama7b, llama7b_tokenizer, input_text, current_idx)\n",
    "\n",
    "    outputs_7b.append(output_7b)\n",
    "    \n",
    "    print(f\"Llama-7b | CURRENT IDX: {current_idx} | Length: {len(outputs_7b)}\")\n",
    "    # with open('input_output_pairs_wmt14_7b', 'wb') as f:\n",
    "    #     pickle.dump(outputs_7b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33977506-402a-4b86-a6b7-5750f58dcc89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "[['After years of negotiations, we have now reached an agreement with the International Atomic Energy Agency to clear up the differences of the past few years, wrote Foreign Minister Mohammed Jawad Sarif on his Facebook page.'], ['It may still be a long way off, but the atomic energy negotiator is satisfied with the negotiation process and even more optimistic that both sides will come to a solution in the end.']]\n"
     ]
    }
   ],
   "source": [
    "with open('input_output_pairs_wmt14_7b', 'rb') as f:\n",
    "    outputs_7b = pickle.load(f)\n",
    "\n",
    "print(len(outputs_7b))\n",
    "print(outputs_7b[998:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7caf86b3-83c9-4df6-bed6-45fd16543919",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyLlama | CURRENT IDX: 0 | Length: 1\n",
      "TinyLlama | CURRENT IDX: 1 | Length: 2\n",
      "TinyLlama | CURRENT IDX: 2 | Length: 3\n",
      "TinyLlama | CURRENT IDX: 3 | Length: 4\n",
      "TinyLlama | CURRENT IDX: 4 | Length: 5\n",
      "TinyLlama | CURRENT IDX: 5 | Length: 6\n",
      "TinyLlama | CURRENT IDX: 6 | Length: 7\n",
      "TinyLlama | CURRENT IDX: 7 | Length: 8\n",
      "TinyLlama | CURRENT IDX: 8 | Length: 9\n",
      "TinyLlama | CURRENT IDX: 9 | Length: 10\n",
      "TinyLlama | CURRENT IDX: 10 | Length: 11\n",
      "TinyLlama | CURRENT IDX: 11 | Length: 12\n",
      "TinyLlama | CURRENT IDX: 12 | Length: 13\n",
      "TinyLlama | CURRENT IDX: 13 | Length: 14\n",
      "TinyLlama | CURRENT IDX: 14 | Length: 15\n",
      "TinyLlama | CURRENT IDX: 15 | Length: 16\n",
      "TinyLlama | CURRENT IDX: 16 | Length: 17\n",
      "TinyLlama | CURRENT IDX: 17 | Length: 18\n",
      "TinyLlama | CURRENT IDX: 18 | Length: 19\n",
      "TinyLlama | CURRENT IDX: 19 | Length: 20\n",
      "TinyLlama | CURRENT IDX: 20 | Length: 21\n",
      "TinyLlama | CURRENT IDX: 21 | Length: 22\n",
      "TinyLlama | CURRENT IDX: 22 | Length: 23\n",
      "TinyLlama | CURRENT IDX: 23 | Length: 24\n",
      "TinyLlama | CURRENT IDX: 24 | Length: 25\n",
      "TinyLlama | CURRENT IDX: 25 | Length: 26\n",
      "TinyLlama | CURRENT IDX: 26 | Length: 27\n",
      "TinyLlama | CURRENT IDX: 27 | Length: 28\n",
      "TinyLlama | CURRENT IDX: 28 | Length: 29\n",
      "TinyLlama | CURRENT IDX: 29 | Length: 30\n",
      "TinyLlama | CURRENT IDX: 30 | Length: 31\n",
      "TinyLlama | CURRENT IDX: 31 | Length: 32\n",
      "TinyLlama | CURRENT IDX: 32 | Length: 33\n",
      "TinyLlama | CURRENT IDX: 33 | Length: 34\n",
      "TinyLlama | CURRENT IDX: 34 | Length: 35\n",
      "TinyLlama | CURRENT IDX: 35 | Length: 36\n",
      "TinyLlama | CURRENT IDX: 36 | Length: 37\n",
      "TinyLlama | CURRENT IDX: 37 | Length: 38\n",
      "TinyLlama | CURRENT IDX: 38 | Length: 39\n",
      "TinyLlama | CURRENT IDX: 39 | Length: 40\n",
      "TinyLlama | CURRENT IDX: 40 | Length: 41\n",
      "TinyLlama | CURRENT IDX: 41 | Length: 42\n",
      "TinyLlama | CURRENT IDX: 42 | Length: 43\n",
      "TinyLlama | CURRENT IDX: 43 | Length: 44\n",
      "TinyLlama | CURRENT IDX: 44 | Length: 45\n",
      "TinyLlama | CURRENT IDX: 45 | Length: 46\n",
      "TinyLlama | CURRENT IDX: 46 | Length: 47\n",
      "TinyLlama | CURRENT IDX: 47 | Length: 48\n",
      "TinyLlama | CURRENT IDX: 48 | Length: 49\n",
      "TinyLlama | CURRENT IDX: 49 | Length: 50\n",
      "TinyLlama | CURRENT IDX: 50 | Length: 51\n",
      "TinyLlama | CURRENT IDX: 51 | Length: 52\n",
      "TinyLlama | CURRENT IDX: 52 | Length: 53\n",
      "TinyLlama | CURRENT IDX: 53 | Length: 54\n",
      "TinyLlama | CURRENT IDX: 54 | Length: 55\n",
      "TinyLlama | CURRENT IDX: 55 | Length: 56\n",
      "TinyLlama | CURRENT IDX: 56 | Length: 57\n",
      "TinyLlama | CURRENT IDX: 57 | Length: 58\n",
      "TinyLlama | CURRENT IDX: 58 | Length: 59\n",
      "TinyLlama | CURRENT IDX: 59 | Length: 60\n",
      "TinyLlama | CURRENT IDX: 60 | Length: 61\n",
      "TinyLlama | CURRENT IDX: 61 | Length: 62\n",
      "TinyLlama | CURRENT IDX: 62 | Length: 63\n",
      "TinyLlama | CURRENT IDX: 63 | Length: 64\n",
      "TinyLlama | CURRENT IDX: 64 | Length: 65\n",
      "TinyLlama | CURRENT IDX: 65 | Length: 66\n",
      "TinyLlama | CURRENT IDX: 66 | Length: 67\n",
      "TinyLlama | CURRENT IDX: 67 | Length: 68\n",
      "TinyLlama | CURRENT IDX: 68 | Length: 69\n",
      "TinyLlama | CURRENT IDX: 69 | Length: 70\n",
      "TinyLlama | CURRENT IDX: 70 | Length: 71\n",
      "TinyLlama | CURRENT IDX: 71 | Length: 72\n",
      "TinyLlama | CURRENT IDX: 72 | Length: 73\n",
      "TinyLlama | CURRENT IDX: 73 | Length: 74\n",
      "TinyLlama | CURRENT IDX: 74 | Length: 75\n",
      "TinyLlama | CURRENT IDX: 75 | Length: 76\n",
      "TinyLlama | CURRENT IDX: 76 | Length: 77\n",
      "TinyLlama | CURRENT IDX: 77 | Length: 78\n",
      "TinyLlama | CURRENT IDX: 78 | Length: 79\n",
      "TinyLlama | CURRENT IDX: 79 | Length: 80\n",
      "TinyLlama | CURRENT IDX: 80 | Length: 81\n",
      "TinyLlama | CURRENT IDX: 81 | Length: 82\n",
      "TinyLlama | CURRENT IDX: 82 | Length: 83\n",
      "TinyLlama | CURRENT IDX: 83 | Length: 84\n",
      "TinyLlama | CURRENT IDX: 84 | Length: 85\n",
      "TinyLlama | CURRENT IDX: 85 | Length: 86\n",
      "TinyLlama | CURRENT IDX: 86 | Length: 87\n",
      "TinyLlama | CURRENT IDX: 87 | Length: 88\n",
      "TinyLlama | CURRENT IDX: 88 | Length: 89\n",
      "TinyLlama | CURRENT IDX: 89 | Length: 90\n",
      "TinyLlama | CURRENT IDX: 90 | Length: 91\n",
      "TinyLlama | CURRENT IDX: 91 | Length: 92\n",
      "TinyLlama | CURRENT IDX: 92 | Length: 93\n",
      "TinyLlama | CURRENT IDX: 93 | Length: 94\n",
      "TinyLlama | CURRENT IDX: 94 | Length: 95\n",
      "TinyLlama | CURRENT IDX: 95 | Length: 96\n",
      "TinyLlama | CURRENT IDX: 96 | Length: 97\n",
      "TinyLlama | CURRENT IDX: 97 | Length: 98\n",
      "TinyLlama | CURRENT IDX: 98 | Length: 99\n",
      "TinyLlama | CURRENT IDX: 99 | Length: 100\n",
      "TinyLlama | CURRENT IDX: 100 | Length: 101\n",
      "TinyLlama | CURRENT IDX: 101 | Length: 102\n",
      "TinyLlama | CURRENT IDX: 102 | Length: 103\n",
      "TinyLlama | CURRENT IDX: 103 | Length: 104\n",
      "TinyLlama | CURRENT IDX: 104 | Length: 105\n",
      "TinyLlama | CURRENT IDX: 105 | Length: 106\n",
      "TinyLlama | CURRENT IDX: 106 | Length: 107\n",
      "TinyLlama | CURRENT IDX: 107 | Length: 108\n",
      "TinyLlama | CURRENT IDX: 108 | Length: 109\n",
      "TinyLlama | CURRENT IDX: 109 | Length: 110\n",
      "TinyLlama | CURRENT IDX: 110 | Length: 111\n",
      "TinyLlama | CURRENT IDX: 111 | Length: 112\n",
      "TinyLlama | CURRENT IDX: 112 | Length: 113\n",
      "TinyLlama | CURRENT IDX: 113 | Length: 114\n",
      "TinyLlama | CURRENT IDX: 114 | Length: 115\n",
      "TinyLlama | CURRENT IDX: 115 | Length: 116\n",
      "TinyLlama | CURRENT IDX: 116 | Length: 117\n",
      "TinyLlama | CURRENT IDX: 117 | Length: 118\n",
      "TinyLlama | CURRENT IDX: 118 | Length: 119\n",
      "TinyLlama | CURRENT IDX: 119 | Length: 120\n",
      "TinyLlama | CURRENT IDX: 120 | Length: 121\n",
      "TinyLlama | CURRENT IDX: 121 | Length: 122\n",
      "TinyLlama | CURRENT IDX: 122 | Length: 123\n",
      "TinyLlama | CURRENT IDX: 123 | Length: 124\n",
      "TinyLlama | CURRENT IDX: 124 | Length: 125\n",
      "TinyLlama | CURRENT IDX: 125 | Length: 126\n",
      "TinyLlama | CURRENT IDX: 126 | Length: 127\n",
      "TinyLlama | CURRENT IDX: 127 | Length: 128\n",
      "TinyLlama | CURRENT IDX: 128 | Length: 129\n",
      "TinyLlama | CURRENT IDX: 129 | Length: 130\n",
      "TinyLlama | CURRENT IDX: 130 | Length: 131\n",
      "TinyLlama | CURRENT IDX: 131 | Length: 132\n",
      "TinyLlama | CURRENT IDX: 132 | Length: 133\n",
      "TinyLlama | CURRENT IDX: 133 | Length: 134\n",
      "TinyLlama | CURRENT IDX: 134 | Length: 135\n",
      "TinyLlama | CURRENT IDX: 135 | Length: 136\n",
      "TinyLlama | CURRENT IDX: 136 | Length: 137\n",
      "TinyLlama | CURRENT IDX: 137 | Length: 138\n",
      "TinyLlama | CURRENT IDX: 138 | Length: 139\n",
      "TinyLlama | CURRENT IDX: 139 | Length: 140\n",
      "TinyLlama | CURRENT IDX: 140 | Length: 141\n",
      "TinyLlama | CURRENT IDX: 141 | Length: 142\n",
      "TinyLlama | CURRENT IDX: 142 | Length: 143\n",
      "TinyLlama | CURRENT IDX: 143 | Length: 144\n",
      "TinyLlama | CURRENT IDX: 144 | Length: 145\n",
      "TinyLlama | CURRENT IDX: 145 | Length: 146\n",
      "TinyLlama | CURRENT IDX: 146 | Length: 147\n",
      "TinyLlama | CURRENT IDX: 147 | Length: 148\n",
      "TinyLlama | CURRENT IDX: 148 | Length: 149\n",
      "TinyLlama | CURRENT IDX: 149 | Length: 150\n",
      "TinyLlama | CURRENT IDX: 150 | Length: 151\n",
      "TinyLlama | CURRENT IDX: 151 | Length: 152\n",
      "TinyLlama | CURRENT IDX: 152 | Length: 153\n",
      "TinyLlama | CURRENT IDX: 153 | Length: 154\n",
      "TinyLlama | CURRENT IDX: 154 | Length: 155\n",
      "TinyLlama | CURRENT IDX: 155 | Length: 156\n",
      "TinyLlama | CURRENT IDX: 156 | Length: 157\n",
      "TinyLlama | CURRENT IDX: 157 | Length: 158\n",
      "TinyLlama | CURRENT IDX: 158 | Length: 159\n",
      "TinyLlama | CURRENT IDX: 159 | Length: 160\n",
      "TinyLlama | CURRENT IDX: 160 | Length: 161\n",
      "TinyLlama | CURRENT IDX: 161 | Length: 162\n",
      "TinyLlama | CURRENT IDX: 162 | Length: 163\n",
      "TinyLlama | CURRENT IDX: 163 | Length: 164\n",
      "TinyLlama | CURRENT IDX: 164 | Length: 165\n",
      "TinyLlama | CURRENT IDX: 165 | Length: 166\n",
      "TinyLlama | CURRENT IDX: 166 | Length: 167\n",
      "TinyLlama | CURRENT IDX: 167 | Length: 168\n",
      "TinyLlama | CURRENT IDX: 168 | Length: 169\n",
      "TinyLlama | CURRENT IDX: 169 | Length: 170\n",
      "TinyLlama | CURRENT IDX: 170 | Length: 171\n",
      "TinyLlama | CURRENT IDX: 171 | Length: 172\n",
      "TinyLlama | CURRENT IDX: 172 | Length: 173\n",
      "TinyLlama | CURRENT IDX: 173 | Length: 174\n",
      "TinyLlama | CURRENT IDX: 174 | Length: 175\n",
      "TinyLlama | CURRENT IDX: 175 | Length: 176\n",
      "TinyLlama | CURRENT IDX: 176 | Length: 177\n",
      "TinyLlama | CURRENT IDX: 177 | Length: 178\n",
      "TinyLlama | CURRENT IDX: 178 | Length: 179\n",
      "TinyLlama | CURRENT IDX: 179 | Length: 180\n",
      "TinyLlama | CURRENT IDX: 180 | Length: 181\n",
      "TinyLlama | CURRENT IDX: 181 | Length: 182\n",
      "TinyLlama | CURRENT IDX: 182 | Length: 183\n",
      "TinyLlama | CURRENT IDX: 183 | Length: 184\n",
      "TinyLlama | CURRENT IDX: 184 | Length: 185\n",
      "TinyLlama | CURRENT IDX: 185 | Length: 186\n",
      "TinyLlama | CURRENT IDX: 186 | Length: 187\n",
      "TinyLlama | CURRENT IDX: 187 | Length: 188\n",
      "TinyLlama | CURRENT IDX: 188 | Length: 189\n",
      "TinyLlama | CURRENT IDX: 189 | Length: 190\n",
      "TinyLlama | CURRENT IDX: 190 | Length: 191\n",
      "TinyLlama | CURRENT IDX: 191 | Length: 192\n",
      "TinyLlama | CURRENT IDX: 192 | Length: 193\n",
      "TinyLlama | CURRENT IDX: 193 | Length: 194\n",
      "TinyLlama | CURRENT IDX: 194 | Length: 195\n",
      "TinyLlama | CURRENT IDX: 195 | Length: 196\n",
      "TinyLlama | CURRENT IDX: 196 | Length: 197\n",
      "TinyLlama | CURRENT IDX: 197 | Length: 198\n",
      "TinyLlama | CURRENT IDX: 198 | Length: 199\n",
      "TinyLlama | CURRENT IDX: 199 | Length: 200\n",
      "TinyLlama | CURRENT IDX: 200 | Length: 201\n",
      "TinyLlama | CURRENT IDX: 201 | Length: 202\n",
      "TinyLlama | CURRENT IDX: 202 | Length: 203\n",
      "TinyLlama | CURRENT IDX: 203 | Length: 204\n",
      "TinyLlama | CURRENT IDX: 204 | Length: 205\n",
      "TinyLlama | CURRENT IDX: 205 | Length: 206\n",
      "TinyLlama | CURRENT IDX: 206 | Length: 207\n",
      "TinyLlama | CURRENT IDX: 207 | Length: 208\n",
      "TinyLlama | CURRENT IDX: 208 | Length: 209\n",
      "TinyLlama | CURRENT IDX: 209 | Length: 210\n",
      "TinyLlama | CURRENT IDX: 210 | Length: 211\n",
      "TinyLlama | CURRENT IDX: 211 | Length: 212\n",
      "TinyLlama | CURRENT IDX: 212 | Length: 213\n",
      "TinyLlama | CURRENT IDX: 213 | Length: 214\n",
      "TinyLlama | CURRENT IDX: 214 | Length: 215\n",
      "TinyLlama | CURRENT IDX: 215 | Length: 216\n",
      "TinyLlama | CURRENT IDX: 216 | Length: 217\n",
      "TinyLlama | CURRENT IDX: 217 | Length: 218\n",
      "TinyLlama | CURRENT IDX: 218 | Length: 219\n",
      "TinyLlama | CURRENT IDX: 219 | Length: 220\n",
      "TinyLlama | CURRENT IDX: 220 | Length: 221\n",
      "TinyLlama | CURRENT IDX: 221 | Length: 222\n",
      "TinyLlama | CURRENT IDX: 222 | Length: 223\n",
      "TinyLlama | CURRENT IDX: 223 | Length: 224\n",
      "TinyLlama | CURRENT IDX: 224 | Length: 225\n",
      "TinyLlama | CURRENT IDX: 225 | Length: 226\n",
      "TinyLlama | CURRENT IDX: 226 | Length: 227\n",
      "TinyLlama | CURRENT IDX: 227 | Length: 228\n",
      "TinyLlama | CURRENT IDX: 228 | Length: 229\n",
      "TinyLlama | CURRENT IDX: 229 | Length: 230\n",
      "TinyLlama | CURRENT IDX: 230 | Length: 231\n",
      "TinyLlama | CURRENT IDX: 231 | Length: 232\n",
      "TinyLlama | CURRENT IDX: 232 | Length: 233\n",
      "TinyLlama | CURRENT IDX: 233 | Length: 234\n",
      "TinyLlama | CURRENT IDX: 234 | Length: 235\n",
      "TinyLlama | CURRENT IDX: 235 | Length: 236\n",
      "TinyLlama | CURRENT IDX: 236 | Length: 237\n",
      "TinyLlama | CURRENT IDX: 237 | Length: 238\n",
      "TinyLlama | CURRENT IDX: 238 | Length: 239\n",
      "TinyLlama | CURRENT IDX: 239 | Length: 240\n",
      "TinyLlama | CURRENT IDX: 240 | Length: 241\n",
      "TinyLlama | CURRENT IDX: 241 | Length: 242\n",
      "TinyLlama | CURRENT IDX: 242 | Length: 243\n",
      "TinyLlama | CURRENT IDX: 243 | Length: 244\n",
      "TinyLlama | CURRENT IDX: 244 | Length: 245\n",
      "TinyLlama | CURRENT IDX: 245 | Length: 246\n",
      "TinyLlama | CURRENT IDX: 246 | Length: 247\n",
      "TinyLlama | CURRENT IDX: 247 | Length: 248\n",
      "TinyLlama | CURRENT IDX: 248 | Length: 249\n",
      "TinyLlama | CURRENT IDX: 249 | Length: 250\n",
      "TinyLlama | CURRENT IDX: 250 | Length: 251\n",
      "TinyLlama | CURRENT IDX: 251 | Length: 252\n",
      "TinyLlama | CURRENT IDX: 252 | Length: 253\n",
      "TinyLlama | CURRENT IDX: 253 | Length: 254\n",
      "TinyLlama | CURRENT IDX: 254 | Length: 255\n",
      "TinyLlama | CURRENT IDX: 255 | Length: 256\n",
      "TinyLlama | CURRENT IDX: 256 | Length: 257\n",
      "TinyLlama | CURRENT IDX: 257 | Length: 258\n",
      "TinyLlama | CURRENT IDX: 258 | Length: 259\n",
      "TinyLlama | CURRENT IDX: 259 | Length: 260\n",
      "TinyLlama | CURRENT IDX: 260 | Length: 261\n",
      "TinyLlama | CURRENT IDX: 261 | Length: 262\n",
      "TinyLlama | CURRENT IDX: 262 | Length: 263\n",
      "TinyLlama | CURRENT IDX: 263 | Length: 264\n",
      "TinyLlama | CURRENT IDX: 264 | Length: 265\n",
      "TinyLlama | CURRENT IDX: 265 | Length: 266\n",
      "TinyLlama | CURRENT IDX: 266 | Length: 267\n",
      "TinyLlama | CURRENT IDX: 267 | Length: 268\n",
      "TinyLlama | CURRENT IDX: 268 | Length: 269\n",
      "TinyLlama | CURRENT IDX: 269 | Length: 270\n",
      "TinyLlama | CURRENT IDX: 270 | Length: 271\n",
      "TinyLlama | CURRENT IDX: 271 | Length: 272\n",
      "TinyLlama | CURRENT IDX: 272 | Length: 273\n",
      "TinyLlama | CURRENT IDX: 273 | Length: 274\n",
      "TinyLlama | CURRENT IDX: 274 | Length: 275\n",
      "TinyLlama | CURRENT IDX: 275 | Length: 276\n",
      "TinyLlama | CURRENT IDX: 276 | Length: 277\n",
      "TinyLlama | CURRENT IDX: 277 | Length: 278\n",
      "TinyLlama | CURRENT IDX: 278 | Length: 279\n",
      "TinyLlama | CURRENT IDX: 279 | Length: 280\n",
      "TinyLlama | CURRENT IDX: 280 | Length: 281\n",
      "TinyLlama | CURRENT IDX: 281 | Length: 282\n",
      "TinyLlama | CURRENT IDX: 282 | Length: 283\n",
      "TinyLlama | CURRENT IDX: 283 | Length: 284\n",
      "TinyLlama | CURRENT IDX: 284 | Length: 285\n",
      "TinyLlama | CURRENT IDX: 285 | Length: 286\n",
      "TinyLlama | CURRENT IDX: 286 | Length: 287\n",
      "TinyLlama | CURRENT IDX: 287 | Length: 288\n",
      "TinyLlama | CURRENT IDX: 288 | Length: 289\n",
      "TinyLlama | CURRENT IDX: 289 | Length: 290\n",
      "TinyLlama | CURRENT IDX: 290 | Length: 291\n",
      "TinyLlama | CURRENT IDX: 291 | Length: 292\n",
      "TinyLlama | CURRENT IDX: 292 | Length: 293\n",
      "TinyLlama | CURRENT IDX: 293 | Length: 294\n",
      "TinyLlama | CURRENT IDX: 294 | Length: 295\n",
      "TinyLlama | CURRENT IDX: 295 | Length: 296\n",
      "TinyLlama | CURRENT IDX: 296 | Length: 297\n",
      "TinyLlama | CURRENT IDX: 297 | Length: 298\n",
      "TinyLlama | CURRENT IDX: 298 | Length: 299\n",
      "TinyLlama | CURRENT IDX: 299 | Length: 300\n",
      "TinyLlama | CURRENT IDX: 300 | Length: 301\n",
      "TinyLlama | CURRENT IDX: 301 | Length: 302\n",
      "TinyLlama | CURRENT IDX: 302 | Length: 303\n",
      "TinyLlama | CURRENT IDX: 303 | Length: 304\n",
      "TinyLlama | CURRENT IDX: 304 | Length: 305\n",
      "TinyLlama | CURRENT IDX: 305 | Length: 306\n",
      "TinyLlama | CURRENT IDX: 306 | Length: 307\n",
      "TinyLlama | CURRENT IDX: 307 | Length: 308\n",
      "TinyLlama | CURRENT IDX: 308 | Length: 309\n",
      "TinyLlama | CURRENT IDX: 309 | Length: 310\n",
      "TinyLlama | CURRENT IDX: 310 | Length: 311\n",
      "TinyLlama | CURRENT IDX: 311 | Length: 312\n",
      "TinyLlama | CURRENT IDX: 312 | Length: 313\n",
      "TinyLlama | CURRENT IDX: 313 | Length: 314\n",
      "TinyLlama | CURRENT IDX: 314 | Length: 315\n",
      "TinyLlama | CURRENT IDX: 315 | Length: 316\n",
      "TinyLlama | CURRENT IDX: 316 | Length: 317\n",
      "TinyLlama | CURRENT IDX: 317 | Length: 318\n",
      "TinyLlama | CURRENT IDX: 318 | Length: 319\n",
      "TinyLlama | CURRENT IDX: 319 | Length: 320\n",
      "TinyLlama | CURRENT IDX: 320 | Length: 321\n",
      "TinyLlama | CURRENT IDX: 321 | Length: 322\n",
      "TinyLlama | CURRENT IDX: 322 | Length: 323\n",
      "TinyLlama | CURRENT IDX: 323 | Length: 324\n",
      "TinyLlama | CURRENT IDX: 324 | Length: 325\n",
      "TinyLlama | CURRENT IDX: 325 | Length: 326\n",
      "TinyLlama | CURRENT IDX: 326 | Length: 327\n",
      "TinyLlama | CURRENT IDX: 327 | Length: 328\n",
      "TinyLlama | CURRENT IDX: 328 | Length: 329\n",
      "TinyLlama | CURRENT IDX: 329 | Length: 330\n",
      "TinyLlama | CURRENT IDX: 330 | Length: 331\n",
      "TinyLlama | CURRENT IDX: 331 | Length: 332\n",
      "TinyLlama | CURRENT IDX: 332 | Length: 333\n",
      "TinyLlama | CURRENT IDX: 333 | Length: 334\n",
      "TinyLlama | CURRENT IDX: 334 | Length: 335\n",
      "TinyLlama | CURRENT IDX: 335 | Length: 336\n",
      "TinyLlama | CURRENT IDX: 336 | Length: 337\n",
      "TinyLlama | CURRENT IDX: 337 | Length: 338\n",
      "TinyLlama | CURRENT IDX: 338 | Length: 339\n",
      "TinyLlama | CURRENT IDX: 339 | Length: 340\n",
      "TinyLlama | CURRENT IDX: 340 | Length: 341\n",
      "TinyLlama | CURRENT IDX: 341 | Length: 342\n",
      "TinyLlama | CURRENT IDX: 342 | Length: 343\n",
      "TinyLlama | CURRENT IDX: 343 | Length: 344\n",
      "TinyLlama | CURRENT IDX: 344 | Length: 345\n",
      "TinyLlama | CURRENT IDX: 345 | Length: 346\n",
      "TinyLlama | CURRENT IDX: 346 | Length: 347\n",
      "TinyLlama | CURRENT IDX: 347 | Length: 348\n",
      "TinyLlama | CURRENT IDX: 348 | Length: 349\n",
      "TinyLlama | CURRENT IDX: 349 | Length: 350\n",
      "TinyLlama | CURRENT IDX: 350 | Length: 351\n",
      "TinyLlama | CURRENT IDX: 351 | Length: 352\n",
      "TinyLlama | CURRENT IDX: 352 | Length: 353\n",
      "TinyLlama | CURRENT IDX: 353 | Length: 354\n",
      "TinyLlama | CURRENT IDX: 354 | Length: 355\n",
      "TinyLlama | CURRENT IDX: 355 | Length: 356\n",
      "TinyLlama | CURRENT IDX: 356 | Length: 357\n",
      "TinyLlama | CURRENT IDX: 357 | Length: 358\n",
      "TinyLlama | CURRENT IDX: 358 | Length: 359\n",
      "TinyLlama | CURRENT IDX: 359 | Length: 360\n",
      "TinyLlama | CURRENT IDX: 360 | Length: 361\n",
      "TinyLlama | CURRENT IDX: 361 | Length: 362\n",
      "TinyLlama | CURRENT IDX: 362 | Length: 363\n",
      "TinyLlama | CURRENT IDX: 363 | Length: 364\n",
      "TinyLlama | CURRENT IDX: 364 | Length: 365\n",
      "TinyLlama | CURRENT IDX: 365 | Length: 366\n",
      "TinyLlama | CURRENT IDX: 366 | Length: 367\n",
      "TinyLlama | CURRENT IDX: 367 | Length: 368\n",
      "TinyLlama | CURRENT IDX: 368 | Length: 369\n",
      "TinyLlama | CURRENT IDX: 369 | Length: 370\n",
      "TinyLlama | CURRENT IDX: 370 | Length: 371\n",
      "TinyLlama | CURRENT IDX: 371 | Length: 372\n",
      "TinyLlama | CURRENT IDX: 372 | Length: 373\n",
      "TinyLlama | CURRENT IDX: 373 | Length: 374\n",
      "TinyLlama | CURRENT IDX: 374 | Length: 375\n",
      "TinyLlama | CURRENT IDX: 375 | Length: 376\n",
      "TinyLlama | CURRENT IDX: 376 | Length: 377\n",
      "TinyLlama | CURRENT IDX: 377 | Length: 378\n",
      "TinyLlama | CURRENT IDX: 378 | Length: 379\n",
      "TinyLlama | CURRENT IDX: 379 | Length: 380\n",
      "TinyLlama | CURRENT IDX: 380 | Length: 381\n",
      "TinyLlama | CURRENT IDX: 381 | Length: 382\n",
      "TinyLlama | CURRENT IDX: 382 | Length: 383\n",
      "TinyLlama | CURRENT IDX: 383 | Length: 384\n",
      "TinyLlama | CURRENT IDX: 384 | Length: 385\n",
      "TinyLlama | CURRENT IDX: 385 | Length: 386\n",
      "TinyLlama | CURRENT IDX: 386 | Length: 387\n",
      "TinyLlama | CURRENT IDX: 387 | Length: 388\n",
      "TinyLlama | CURRENT IDX: 388 | Length: 389\n",
      "TinyLlama | CURRENT IDX: 389 | Length: 390\n",
      "TinyLlama | CURRENT IDX: 390 | Length: 391\n",
      "TinyLlama | CURRENT IDX: 391 | Length: 392\n",
      "TinyLlama | CURRENT IDX: 392 | Length: 393\n",
      "TinyLlama | CURRENT IDX: 393 | Length: 394\n",
      "TinyLlama | CURRENT IDX: 394 | Length: 395\n",
      "TinyLlama | CURRENT IDX: 395 | Length: 396\n",
      "TinyLlama | CURRENT IDX: 396 | Length: 397\n",
      "TinyLlama | CURRENT IDX: 397 | Length: 398\n",
      "TinyLlama | CURRENT IDX: 398 | Length: 399\n",
      "TinyLlama | CURRENT IDX: 399 | Length: 400\n",
      "TinyLlama | CURRENT IDX: 400 | Length: 401\n",
      "TinyLlama | CURRENT IDX: 401 | Length: 402\n",
      "TinyLlama | CURRENT IDX: 402 | Length: 403\n",
      "TinyLlama | CURRENT IDX: 403 | Length: 404\n",
      "TinyLlama | CURRENT IDX: 404 | Length: 405\n",
      "TinyLlama | CURRENT IDX: 405 | Length: 406\n",
      "TinyLlama | CURRENT IDX: 406 | Length: 407\n",
      "TinyLlama | CURRENT IDX: 407 | Length: 408\n",
      "TinyLlama | CURRENT IDX: 408 | Length: 409\n",
      "TinyLlama | CURRENT IDX: 409 | Length: 410\n",
      "TinyLlama | CURRENT IDX: 410 | Length: 411\n",
      "TinyLlama | CURRENT IDX: 411 | Length: 412\n",
      "TinyLlama | CURRENT IDX: 412 | Length: 413\n",
      "TinyLlama | CURRENT IDX: 413 | Length: 414\n",
      "TinyLlama | CURRENT IDX: 414 | Length: 415\n",
      "TinyLlama | CURRENT IDX: 415 | Length: 416\n",
      "TinyLlama | CURRENT IDX: 416 | Length: 417\n",
      "TinyLlama | CURRENT IDX: 417 | Length: 418\n",
      "TinyLlama | CURRENT IDX: 418 | Length: 419\n",
      "TinyLlama | CURRENT IDX: 419 | Length: 420\n",
      "TinyLlama | CURRENT IDX: 420 | Length: 421\n",
      "TinyLlama | CURRENT IDX: 421 | Length: 422\n",
      "TinyLlama | CURRENT IDX: 422 | Length: 423\n",
      "TinyLlama | CURRENT IDX: 423 | Length: 424\n",
      "TinyLlama | CURRENT IDX: 424 | Length: 425\n",
      "TinyLlama | CURRENT IDX: 425 | Length: 426\n",
      "TinyLlama | CURRENT IDX: 426 | Length: 427\n",
      "TinyLlama | CURRENT IDX: 427 | Length: 428\n",
      "TinyLlama | CURRENT IDX: 428 | Length: 429\n",
      "TinyLlama | CURRENT IDX: 429 | Length: 430\n",
      "TinyLlama | CURRENT IDX: 430 | Length: 431\n",
      "TinyLlama | CURRENT IDX: 431 | Length: 432\n",
      "TinyLlama | CURRENT IDX: 432 | Length: 433\n",
      "TinyLlama | CURRENT IDX: 433 | Length: 434\n",
      "TinyLlama | CURRENT IDX: 434 | Length: 435\n",
      "TinyLlama | CURRENT IDX: 435 | Length: 436\n",
      "TinyLlama | CURRENT IDX: 436 | Length: 437\n",
      "TinyLlama | CURRENT IDX: 437 | Length: 438\n",
      "TinyLlama | CURRENT IDX: 438 | Length: 439\n",
      "TinyLlama | CURRENT IDX: 439 | Length: 440\n",
      "TinyLlama | CURRENT IDX: 440 | Length: 441\n",
      "TinyLlama | CURRENT IDX: 441 | Length: 442\n",
      "TinyLlama | CURRENT IDX: 442 | Length: 443\n",
      "TinyLlama | CURRENT IDX: 443 | Length: 444\n",
      "TinyLlama | CURRENT IDX: 444 | Length: 445\n",
      "TinyLlama | CURRENT IDX: 445 | Length: 446\n",
      "TinyLlama | CURRENT IDX: 446 | Length: 447\n",
      "TinyLlama | CURRENT IDX: 447 | Length: 448\n",
      "TinyLlama | CURRENT IDX: 448 | Length: 449\n",
      "TinyLlama | CURRENT IDX: 449 | Length: 450\n",
      "TinyLlama | CURRENT IDX: 450 | Length: 451\n",
      "TinyLlama | CURRENT IDX: 451 | Length: 452\n",
      "TinyLlama | CURRENT IDX: 452 | Length: 453\n",
      "TinyLlama | CURRENT IDX: 453 | Length: 454\n",
      "TinyLlama | CURRENT IDX: 454 | Length: 455\n",
      "TinyLlama | CURRENT IDX: 455 | Length: 456\n",
      "TinyLlama | CURRENT IDX: 456 | Length: 457\n",
      "TinyLlama | CURRENT IDX: 457 | Length: 458\n",
      "TinyLlama | CURRENT IDX: 458 | Length: 459\n",
      "TinyLlama | CURRENT IDX: 459 | Length: 460\n",
      "TinyLlama | CURRENT IDX: 460 | Length: 461\n",
      "TinyLlama | CURRENT IDX: 461 | Length: 462\n",
      "TinyLlama | CURRENT IDX: 462 | Length: 463\n",
      "TinyLlama | CURRENT IDX: 463 | Length: 464\n",
      "TinyLlama | CURRENT IDX: 464 | Length: 465\n",
      "TinyLlama | CURRENT IDX: 465 | Length: 466\n",
      "TinyLlama | CURRENT IDX: 466 | Length: 467\n",
      "TinyLlama | CURRENT IDX: 467 | Length: 468\n",
      "TinyLlama | CURRENT IDX: 468 | Length: 469\n",
      "TinyLlama | CURRENT IDX: 469 | Length: 470\n",
      "TinyLlama | CURRENT IDX: 470 | Length: 471\n",
      "TinyLlama | CURRENT IDX: 471 | Length: 472\n",
      "TinyLlama | CURRENT IDX: 472 | Length: 473\n",
      "TinyLlama | CURRENT IDX: 473 | Length: 474\n",
      "TinyLlama | CURRENT IDX: 474 | Length: 475\n",
      "TinyLlama | CURRENT IDX: 475 | Length: 476\n",
      "TinyLlama | CURRENT IDX: 476 | Length: 477\n",
      "TinyLlama | CURRENT IDX: 477 | Length: 478\n",
      "TinyLlama | CURRENT IDX: 478 | Length: 479\n",
      "TinyLlama | CURRENT IDX: 479 | Length: 480\n",
      "TinyLlama | CURRENT IDX: 480 | Length: 481\n",
      "TinyLlama | CURRENT IDX: 481 | Length: 482\n",
      "TinyLlama | CURRENT IDX: 482 | Length: 483\n",
      "TinyLlama | CURRENT IDX: 483 | Length: 484\n",
      "TinyLlama | CURRENT IDX: 484 | Length: 485\n",
      "TinyLlama | CURRENT IDX: 485 | Length: 486\n",
      "TinyLlama | CURRENT IDX: 486 | Length: 487\n",
      "TinyLlama | CURRENT IDX: 487 | Length: 488\n",
      "TinyLlama | CURRENT IDX: 488 | Length: 489\n",
      "TinyLlama | CURRENT IDX: 489 | Length: 490\n",
      "TinyLlama | CURRENT IDX: 490 | Length: 491\n",
      "TinyLlama | CURRENT IDX: 491 | Length: 492\n",
      "TinyLlama | CURRENT IDX: 492 | Length: 493\n",
      "TinyLlama | CURRENT IDX: 493 | Length: 494\n",
      "TinyLlama | CURRENT IDX: 494 | Length: 495\n",
      "TinyLlama | CURRENT IDX: 495 | Length: 496\n",
      "TinyLlama | CURRENT IDX: 496 | Length: 497\n",
      "TinyLlama | CURRENT IDX: 497 | Length: 498\n",
      "TinyLlama | CURRENT IDX: 498 | Length: 499\n",
      "TinyLlama | CURRENT IDX: 499 | Length: 500\n",
      "TinyLlama | CURRENT IDX: 500 | Length: 501\n",
      "TinyLlama | CURRENT IDX: 501 | Length: 502\n",
      "TinyLlama | CURRENT IDX: 502 | Length: 503\n",
      "TinyLlama | CURRENT IDX: 503 | Length: 504\n",
      "TinyLlama | CURRENT IDX: 504 | Length: 505\n",
      "TinyLlama | CURRENT IDX: 505 | Length: 506\n",
      "TinyLlama | CURRENT IDX: 506 | Length: 507\n",
      "TinyLlama | CURRENT IDX: 507 | Length: 508\n",
      "TinyLlama | CURRENT IDX: 508 | Length: 509\n",
      "TinyLlama | CURRENT IDX: 509 | Length: 510\n",
      "TinyLlama | CURRENT IDX: 510 | Length: 511\n",
      "TinyLlama | CURRENT IDX: 511 | Length: 512\n",
      "TinyLlama | CURRENT IDX: 512 | Length: 513\n",
      "TinyLlama | CURRENT IDX: 513 | Length: 514\n",
      "TinyLlama | CURRENT IDX: 514 | Length: 515\n",
      "TinyLlama | CURRENT IDX: 515 | Length: 516\n",
      "TinyLlama | CURRENT IDX: 516 | Length: 517\n",
      "TinyLlama | CURRENT IDX: 517 | Length: 518\n",
      "TinyLlama | CURRENT IDX: 518 | Length: 519\n",
      "TinyLlama | CURRENT IDX: 519 | Length: 520\n",
      "TinyLlama | CURRENT IDX: 520 | Length: 521\n",
      "TinyLlama | CURRENT IDX: 521 | Length: 522\n",
      "TinyLlama | CURRENT IDX: 522 | Length: 523\n",
      "TinyLlama | CURRENT IDX: 523 | Length: 524\n",
      "TinyLlama | CURRENT IDX: 524 | Length: 525\n",
      "TinyLlama | CURRENT IDX: 525 | Length: 526\n",
      "TinyLlama | CURRENT IDX: 526 | Length: 527\n",
      "TinyLlama | CURRENT IDX: 527 | Length: 528\n",
      "TinyLlama | CURRENT IDX: 528 | Length: 529\n",
      "TinyLlama | CURRENT IDX: 529 | Length: 530\n",
      "TinyLlama | CURRENT IDX: 530 | Length: 531\n",
      "TinyLlama | CURRENT IDX: 531 | Length: 532\n",
      "TinyLlama | CURRENT IDX: 532 | Length: 533\n",
      "TinyLlama | CURRENT IDX: 533 | Length: 534\n",
      "TinyLlama | CURRENT IDX: 534 | Length: 535\n",
      "TinyLlama | CURRENT IDX: 535 | Length: 536\n",
      "TinyLlama | CURRENT IDX: 536 | Length: 537\n",
      "TinyLlama | CURRENT IDX: 537 | Length: 538\n",
      "TinyLlama | CURRENT IDX: 538 | Length: 539\n",
      "TinyLlama | CURRENT IDX: 539 | Length: 540\n",
      "TinyLlama | CURRENT IDX: 540 | Length: 541\n",
      "TinyLlama | CURRENT IDX: 541 | Length: 542\n",
      "TinyLlama | CURRENT IDX: 542 | Length: 543\n",
      "TinyLlama | CURRENT IDX: 543 | Length: 544\n",
      "TinyLlama | CURRENT IDX: 544 | Length: 545\n",
      "TinyLlama | CURRENT IDX: 545 | Length: 546\n",
      "TinyLlama | CURRENT IDX: 546 | Length: 547\n",
      "TinyLlama | CURRENT IDX: 547 | Length: 548\n",
      "TinyLlama | CURRENT IDX: 548 | Length: 549\n",
      "TinyLlama | CURRENT IDX: 549 | Length: 550\n",
      "TinyLlama | CURRENT IDX: 550 | Length: 551\n",
      "TinyLlama | CURRENT IDX: 551 | Length: 552\n",
      "TinyLlama | CURRENT IDX: 552 | Length: 553\n",
      "TinyLlama | CURRENT IDX: 553 | Length: 554\n",
      "TinyLlama | CURRENT IDX: 554 | Length: 555\n",
      "TinyLlama | CURRENT IDX: 555 | Length: 556\n",
      "TinyLlama | CURRENT IDX: 556 | Length: 557\n",
      "TinyLlama | CURRENT IDX: 557 | Length: 558\n",
      "TinyLlama | CURRENT IDX: 558 | Length: 559\n",
      "TinyLlama | CURRENT IDX: 559 | Length: 560\n",
      "TinyLlama | CURRENT IDX: 560 | Length: 561\n",
      "TinyLlama | CURRENT IDX: 561 | Length: 562\n",
      "TinyLlama | CURRENT IDX: 562 | Length: 563\n",
      "TinyLlama | CURRENT IDX: 563 | Length: 564\n",
      "TinyLlama | CURRENT IDX: 564 | Length: 565\n",
      "TinyLlama | CURRENT IDX: 565 | Length: 566\n",
      "TinyLlama | CURRENT IDX: 566 | Length: 567\n",
      "TinyLlama | CURRENT IDX: 567 | Length: 568\n",
      "TinyLlama | CURRENT IDX: 568 | Length: 569\n",
      "TinyLlama | CURRENT IDX: 569 | Length: 570\n",
      "TinyLlama | CURRENT IDX: 570 | Length: 571\n",
      "TinyLlama | CURRENT IDX: 571 | Length: 572\n",
      "TinyLlama | CURRENT IDX: 572 | Length: 573\n",
      "TinyLlama | CURRENT IDX: 573 | Length: 574\n",
      "TinyLlama | CURRENT IDX: 574 | Length: 575\n",
      "TinyLlama | CURRENT IDX: 575 | Length: 576\n",
      "TinyLlama | CURRENT IDX: 576 | Length: 577\n",
      "TinyLlama | CURRENT IDX: 577 | Length: 578\n",
      "TinyLlama | CURRENT IDX: 578 | Length: 579\n",
      "TinyLlama | CURRENT IDX: 579 | Length: 580\n",
      "TinyLlama | CURRENT IDX: 580 | Length: 581\n",
      "TinyLlama | CURRENT IDX: 581 | Length: 582\n",
      "TinyLlama | CURRENT IDX: 582 | Length: 583\n",
      "TinyLlama | CURRENT IDX: 583 | Length: 584\n",
      "TinyLlama | CURRENT IDX: 584 | Length: 585\n",
      "TinyLlama | CURRENT IDX: 585 | Length: 586\n",
      "TinyLlama | CURRENT IDX: 586 | Length: 587\n",
      "TinyLlama | CURRENT IDX: 587 | Length: 588\n",
      "TinyLlama | CURRENT IDX: 588 | Length: 589\n",
      "TinyLlama | CURRENT IDX: 589 | Length: 590\n",
      "TinyLlama | CURRENT IDX: 590 | Length: 591\n",
      "TinyLlama | CURRENT IDX: 591 | Length: 592\n",
      "TinyLlama | CURRENT IDX: 592 | Length: 593\n",
      "TinyLlama | CURRENT IDX: 593 | Length: 594\n",
      "TinyLlama | CURRENT IDX: 594 | Length: 595\n",
      "TinyLlama | CURRENT IDX: 595 | Length: 596\n",
      "TinyLlama | CURRENT IDX: 596 | Length: 597\n",
      "TinyLlama | CURRENT IDX: 597 | Length: 598\n",
      "TinyLlama | CURRENT IDX: 598 | Length: 599\n",
      "TinyLlama | CURRENT IDX: 599 | Length: 600\n",
      "TinyLlama | CURRENT IDX: 600 | Length: 601\n",
      "TinyLlama | CURRENT IDX: 601 | Length: 602\n",
      "TinyLlama | CURRENT IDX: 602 | Length: 603\n",
      "TinyLlama | CURRENT IDX: 603 | Length: 604\n",
      "TinyLlama | CURRENT IDX: 604 | Length: 605\n",
      "TinyLlama | CURRENT IDX: 605 | Length: 606\n",
      "TinyLlama | CURRENT IDX: 606 | Length: 607\n",
      "TinyLlama | CURRENT IDX: 607 | Length: 608\n",
      "TinyLlama | CURRENT IDX: 608 | Length: 609\n",
      "TinyLlama | CURRENT IDX: 609 | Length: 610\n",
      "TinyLlama | CURRENT IDX: 610 | Length: 611\n",
      "TinyLlama | CURRENT IDX: 611 | Length: 612\n",
      "TinyLlama | CURRENT IDX: 612 | Length: 613\n",
      "TinyLlama | CURRENT IDX: 613 | Length: 614\n",
      "TinyLlama | CURRENT IDX: 614 | Length: 615\n",
      "TinyLlama | CURRENT IDX: 615 | Length: 616\n",
      "TinyLlama | CURRENT IDX: 616 | Length: 617\n",
      "TinyLlama | CURRENT IDX: 617 | Length: 618\n",
      "TinyLlama | CURRENT IDX: 618 | Length: 619\n",
      "TinyLlama | CURRENT IDX: 619 | Length: 620\n",
      "TinyLlama | CURRENT IDX: 620 | Length: 621\n",
      "TinyLlama | CURRENT IDX: 621 | Length: 622\n",
      "TinyLlama | CURRENT IDX: 622 | Length: 623\n",
      "TinyLlama | CURRENT IDX: 623 | Length: 624\n",
      "TinyLlama | CURRENT IDX: 624 | Length: 625\n",
      "TinyLlama | CURRENT IDX: 625 | Length: 626\n",
      "TinyLlama | CURRENT IDX: 626 | Length: 627\n",
      "TinyLlama | CURRENT IDX: 627 | Length: 628\n",
      "TinyLlama | CURRENT IDX: 628 | Length: 629\n",
      "TinyLlama | CURRENT IDX: 629 | Length: 630\n",
      "TinyLlama | CURRENT IDX: 630 | Length: 631\n",
      "TinyLlama | CURRENT IDX: 631 | Length: 632\n",
      "TinyLlama | CURRENT IDX: 632 | Length: 633\n",
      "TinyLlama | CURRENT IDX: 633 | Length: 634\n",
      "TinyLlama | CURRENT IDX: 634 | Length: 635\n",
      "TinyLlama | CURRENT IDX: 635 | Length: 636\n",
      "TinyLlama | CURRENT IDX: 636 | Length: 637\n",
      "TinyLlama | CURRENT IDX: 637 | Length: 638\n",
      "TinyLlama | CURRENT IDX: 638 | Length: 639\n",
      "TinyLlama | CURRENT IDX: 639 | Length: 640\n",
      "TinyLlama | CURRENT IDX: 640 | Length: 641\n",
      "TinyLlama | CURRENT IDX: 641 | Length: 642\n",
      "TinyLlama | CURRENT IDX: 642 | Length: 643\n",
      "TinyLlama | CURRENT IDX: 643 | Length: 644\n",
      "TinyLlama | CURRENT IDX: 644 | Length: 645\n",
      "TinyLlama | CURRENT IDX: 645 | Length: 646\n",
      "TinyLlama | CURRENT IDX: 646 | Length: 647\n",
      "TinyLlama | CURRENT IDX: 647 | Length: 648\n",
      "TinyLlama | CURRENT IDX: 648 | Length: 649\n",
      "TinyLlama | CURRENT IDX: 649 | Length: 650\n",
      "TinyLlama | CURRENT IDX: 650 | Length: 651\n",
      "TinyLlama | CURRENT IDX: 651 | Length: 652\n",
      "TinyLlama | CURRENT IDX: 652 | Length: 653\n",
      "TinyLlama | CURRENT IDX: 653 | Length: 654\n",
      "TinyLlama | CURRENT IDX: 654 | Length: 655\n",
      "TinyLlama | CURRENT IDX: 655 | Length: 656\n",
      "TinyLlama | CURRENT IDX: 656 | Length: 657\n",
      "TinyLlama | CURRENT IDX: 657 | Length: 658\n",
      "TinyLlama | CURRENT IDX: 658 | Length: 659\n",
      "TinyLlama | CURRENT IDX: 659 | Length: 660\n",
      "TinyLlama | CURRENT IDX: 660 | Length: 661\n",
      "TinyLlama | CURRENT IDX: 661 | Length: 662\n",
      "TinyLlama | CURRENT IDX: 662 | Length: 663\n",
      "TinyLlama | CURRENT IDX: 663 | Length: 664\n",
      "TinyLlama | CURRENT IDX: 664 | Length: 665\n",
      "TinyLlama | CURRENT IDX: 665 | Length: 666\n",
      "TinyLlama | CURRENT IDX: 666 | Length: 667\n",
      "TinyLlama | CURRENT IDX: 667 | Length: 668\n",
      "TinyLlama | CURRENT IDX: 668 | Length: 669\n",
      "TinyLlama | CURRENT IDX: 669 | Length: 670\n",
      "TinyLlama | CURRENT IDX: 670 | Length: 671\n",
      "TinyLlama | CURRENT IDX: 671 | Length: 672\n",
      "TinyLlama | CURRENT IDX: 672 | Length: 673\n",
      "TinyLlama | CURRENT IDX: 673 | Length: 674\n",
      "TinyLlama | CURRENT IDX: 674 | Length: 675\n",
      "TinyLlama | CURRENT IDX: 675 | Length: 676\n",
      "TinyLlama | CURRENT IDX: 676 | Length: 677\n",
      "TinyLlama | CURRENT IDX: 677 | Length: 678\n",
      "TinyLlama | CURRENT IDX: 678 | Length: 679\n",
      "TinyLlama | CURRENT IDX: 679 | Length: 680\n",
      "TinyLlama | CURRENT IDX: 680 | Length: 681\n",
      "TinyLlama | CURRENT IDX: 681 | Length: 682\n",
      "TinyLlama | CURRENT IDX: 682 | Length: 683\n",
      "TinyLlama | CURRENT IDX: 683 | Length: 684\n",
      "TinyLlama | CURRENT IDX: 684 | Length: 685\n",
      "TinyLlama | CURRENT IDX: 685 | Length: 686\n",
      "TinyLlama | CURRENT IDX: 686 | Length: 687\n",
      "TinyLlama | CURRENT IDX: 687 | Length: 688\n",
      "TinyLlama | CURRENT IDX: 688 | Length: 689\n",
      "TinyLlama | CURRENT IDX: 689 | Length: 690\n",
      "TinyLlama | CURRENT IDX: 690 | Length: 691\n",
      "TinyLlama | CURRENT IDX: 691 | Length: 692\n",
      "TinyLlama | CURRENT IDX: 692 | Length: 693\n",
      "TinyLlama | CURRENT IDX: 693 | Length: 694\n",
      "TinyLlama | CURRENT IDX: 694 | Length: 695\n",
      "TinyLlama | CURRENT IDX: 695 | Length: 696\n",
      "TinyLlama | CURRENT IDX: 696 | Length: 697\n",
      "TinyLlama | CURRENT IDX: 697 | Length: 698\n",
      "TinyLlama | CURRENT IDX: 698 | Length: 699\n",
      "TinyLlama | CURRENT IDX: 699 | Length: 700\n",
      "TinyLlama | CURRENT IDX: 700 | Length: 701\n",
      "TinyLlama | CURRENT IDX: 701 | Length: 702\n",
      "TinyLlama | CURRENT IDX: 702 | Length: 703\n",
      "TinyLlama | CURRENT IDX: 703 | Length: 704\n",
      "TinyLlama | CURRENT IDX: 704 | Length: 705\n",
      "TinyLlama | CURRENT IDX: 705 | Length: 706\n",
      "TinyLlama | CURRENT IDX: 706 | Length: 707\n",
      "TinyLlama | CURRENT IDX: 707 | Length: 708\n",
      "TinyLlama | CURRENT IDX: 708 | Length: 709\n",
      "TinyLlama | CURRENT IDX: 709 | Length: 710\n",
      "TinyLlama | CURRENT IDX: 710 | Length: 711\n",
      "TinyLlama | CURRENT IDX: 711 | Length: 712\n",
      "TinyLlama | CURRENT IDX: 712 | Length: 713\n",
      "TinyLlama | CURRENT IDX: 713 | Length: 714\n",
      "TinyLlama | CURRENT IDX: 714 | Length: 715\n",
      "TinyLlama | CURRENT IDX: 715 | Length: 716\n",
      "TinyLlama | CURRENT IDX: 716 | Length: 717\n",
      "TinyLlama | CURRENT IDX: 717 | Length: 718\n",
      "TinyLlama | CURRENT IDX: 718 | Length: 719\n",
      "TinyLlama | CURRENT IDX: 719 | Length: 720\n",
      "TinyLlama | CURRENT IDX: 720 | Length: 721\n",
      "TinyLlama | CURRENT IDX: 721 | Length: 722\n",
      "TinyLlama | CURRENT IDX: 722 | Length: 723\n",
      "TinyLlama | CURRENT IDX: 723 | Length: 724\n",
      "TinyLlama | CURRENT IDX: 724 | Length: 725\n",
      "TinyLlama | CURRENT IDX: 725 | Length: 726\n",
      "TinyLlama | CURRENT IDX: 726 | Length: 727\n",
      "TinyLlama | CURRENT IDX: 727 | Length: 728\n",
      "TinyLlama | CURRENT IDX: 728 | Length: 729\n",
      "TinyLlama | CURRENT IDX: 729 | Length: 730\n",
      "TinyLlama | CURRENT IDX: 730 | Length: 731\n",
      "TinyLlama | CURRENT IDX: 731 | Length: 732\n",
      "TinyLlama | CURRENT IDX: 732 | Length: 733\n",
      "TinyLlama | CURRENT IDX: 733 | Length: 734\n",
      "TinyLlama | CURRENT IDX: 734 | Length: 735\n",
      "TinyLlama | CURRENT IDX: 735 | Length: 736\n",
      "TinyLlama | CURRENT IDX: 736 | Length: 737\n",
      "TinyLlama | CURRENT IDX: 737 | Length: 738\n",
      "TinyLlama | CURRENT IDX: 738 | Length: 739\n",
      "TinyLlama | CURRENT IDX: 739 | Length: 740\n",
      "TinyLlama | CURRENT IDX: 740 | Length: 741\n",
      "TinyLlama | CURRENT IDX: 741 | Length: 742\n",
      "TinyLlama | CURRENT IDX: 742 | Length: 743\n",
      "TinyLlama | CURRENT IDX: 743 | Length: 744\n",
      "TinyLlama | CURRENT IDX: 744 | Length: 745\n",
      "TinyLlama | CURRENT IDX: 745 | Length: 746\n",
      "TinyLlama | CURRENT IDX: 746 | Length: 747\n",
      "TinyLlama | CURRENT IDX: 747 | Length: 748\n",
      "TinyLlama | CURRENT IDX: 748 | Length: 749\n",
      "TinyLlama | CURRENT IDX: 749 | Length: 750\n",
      "TinyLlama | CURRENT IDX: 750 | Length: 751\n",
      "TinyLlama | CURRENT IDX: 751 | Length: 752\n",
      "TinyLlama | CURRENT IDX: 752 | Length: 753\n",
      "TinyLlama | CURRENT IDX: 753 | Length: 754\n",
      "TinyLlama | CURRENT IDX: 754 | Length: 755\n",
      "TinyLlama | CURRENT IDX: 755 | Length: 756\n",
      "TinyLlama | CURRENT IDX: 756 | Length: 757\n",
      "TinyLlama | CURRENT IDX: 757 | Length: 758\n",
      "TinyLlama | CURRENT IDX: 758 | Length: 759\n",
      "TinyLlama | CURRENT IDX: 759 | Length: 760\n",
      "TinyLlama | CURRENT IDX: 760 | Length: 761\n",
      "TinyLlama | CURRENT IDX: 761 | Length: 762\n",
      "TinyLlama | CURRENT IDX: 762 | Length: 763\n",
      "TinyLlama | CURRENT IDX: 763 | Length: 764\n",
      "TinyLlama | CURRENT IDX: 764 | Length: 765\n",
      "TinyLlama | CURRENT IDX: 765 | Length: 766\n",
      "TinyLlama | CURRENT IDX: 766 | Length: 767\n",
      "TinyLlama | CURRENT IDX: 767 | Length: 768\n",
      "TinyLlama | CURRENT IDX: 768 | Length: 769\n",
      "TinyLlama | CURRENT IDX: 769 | Length: 770\n",
      "TinyLlama | CURRENT IDX: 770 | Length: 771\n",
      "TinyLlama | CURRENT IDX: 771 | Length: 772\n",
      "TinyLlama | CURRENT IDX: 772 | Length: 773\n",
      "TinyLlama | CURRENT IDX: 773 | Length: 774\n",
      "TinyLlama | CURRENT IDX: 774 | Length: 775\n",
      "TinyLlama | CURRENT IDX: 775 | Length: 776\n",
      "TinyLlama | CURRENT IDX: 776 | Length: 777\n",
      "TinyLlama | CURRENT IDX: 777 | Length: 778\n",
      "TinyLlama | CURRENT IDX: 778 | Length: 779\n",
      "TinyLlama | CURRENT IDX: 779 | Length: 780\n",
      "TinyLlama | CURRENT IDX: 780 | Length: 781\n",
      "TinyLlama | CURRENT IDX: 781 | Length: 782\n",
      "TinyLlama | CURRENT IDX: 782 | Length: 783\n",
      "TinyLlama | CURRENT IDX: 783 | Length: 784\n",
      "TinyLlama | CURRENT IDX: 784 | Length: 785\n",
      "TinyLlama | CURRENT IDX: 785 | Length: 786\n",
      "TinyLlama | CURRENT IDX: 786 | Length: 787\n",
      "TinyLlama | CURRENT IDX: 787 | Length: 788\n",
      "TinyLlama | CURRENT IDX: 788 | Length: 789\n",
      "TinyLlama | CURRENT IDX: 789 | Length: 790\n",
      "TinyLlama | CURRENT IDX: 790 | Length: 791\n",
      "TinyLlama | CURRENT IDX: 791 | Length: 792\n",
      "TinyLlama | CURRENT IDX: 792 | Length: 793\n",
      "TinyLlama | CURRENT IDX: 793 | Length: 794\n",
      "TinyLlama | CURRENT IDX: 794 | Length: 795\n",
      "TinyLlama | CURRENT IDX: 795 | Length: 796\n",
      "TinyLlama | CURRENT IDX: 796 | Length: 797\n",
      "TinyLlama | CURRENT IDX: 797 | Length: 798\n",
      "TinyLlama | CURRENT IDX: 798 | Length: 799\n",
      "TinyLlama | CURRENT IDX: 799 | Length: 800\n",
      "TinyLlama | CURRENT IDX: 800 | Length: 801\n",
      "TinyLlama | CURRENT IDX: 801 | Length: 802\n",
      "TinyLlama | CURRENT IDX: 802 | Length: 803\n",
      "TinyLlama | CURRENT IDX: 803 | Length: 804\n",
      "TinyLlama | CURRENT IDX: 804 | Length: 805\n",
      "TinyLlama | CURRENT IDX: 805 | Length: 806\n",
      "TinyLlama | CURRENT IDX: 806 | Length: 807\n",
      "TinyLlama | CURRENT IDX: 807 | Length: 808\n",
      "TinyLlama | CURRENT IDX: 808 | Length: 809\n",
      "TinyLlama | CURRENT IDX: 809 | Length: 810\n",
      "TinyLlama | CURRENT IDX: 810 | Length: 811\n",
      "TinyLlama | CURRENT IDX: 811 | Length: 812\n",
      "TinyLlama | CURRENT IDX: 812 | Length: 813\n",
      "TinyLlama | CURRENT IDX: 813 | Length: 814\n",
      "TinyLlama | CURRENT IDX: 814 | Length: 815\n",
      "TinyLlama | CURRENT IDX: 815 | Length: 816\n",
      "TinyLlama | CURRENT IDX: 816 | Length: 817\n",
      "TinyLlama | CURRENT IDX: 817 | Length: 818\n",
      "TinyLlama | CURRENT IDX: 818 | Length: 819\n",
      "TinyLlama | CURRENT IDX: 819 | Length: 820\n",
      "TinyLlama | CURRENT IDX: 820 | Length: 821\n",
      "TinyLlama | CURRENT IDX: 821 | Length: 822\n",
      "TinyLlama | CURRENT IDX: 822 | Length: 823\n",
      "TinyLlama | CURRENT IDX: 823 | Length: 824\n",
      "TinyLlama | CURRENT IDX: 824 | Length: 825\n",
      "TinyLlama | CURRENT IDX: 825 | Length: 826\n",
      "TinyLlama | CURRENT IDX: 826 | Length: 827\n",
      "TinyLlama | CURRENT IDX: 827 | Length: 828\n",
      "TinyLlama | CURRENT IDX: 828 | Length: 829\n",
      "TinyLlama | CURRENT IDX: 829 | Length: 830\n",
      "TinyLlama | CURRENT IDX: 830 | Length: 831\n",
      "TinyLlama | CURRENT IDX: 831 | Length: 832\n",
      "TinyLlama | CURRENT IDX: 832 | Length: 833\n",
      "TinyLlama | CURRENT IDX: 833 | Length: 834\n",
      "TinyLlama | CURRENT IDX: 834 | Length: 835\n",
      "TinyLlama | CURRENT IDX: 835 | Length: 836\n",
      "TinyLlama | CURRENT IDX: 836 | Length: 837\n",
      "TinyLlama | CURRENT IDX: 837 | Length: 838\n",
      "TinyLlama | CURRENT IDX: 838 | Length: 839\n",
      "TinyLlama | CURRENT IDX: 839 | Length: 840\n",
      "TinyLlama | CURRENT IDX: 840 | Length: 841\n",
      "TinyLlama | CURRENT IDX: 841 | Length: 842\n",
      "TinyLlama | CURRENT IDX: 842 | Length: 843\n",
      "TinyLlama | CURRENT IDX: 843 | Length: 844\n",
      "TinyLlama | CURRENT IDX: 844 | Length: 845\n",
      "TinyLlama | CURRENT IDX: 845 | Length: 846\n",
      "TinyLlama | CURRENT IDX: 846 | Length: 847\n",
      "TinyLlama | CURRENT IDX: 847 | Length: 848\n",
      "TinyLlama | CURRENT IDX: 848 | Length: 849\n",
      "TinyLlama | CURRENT IDX: 849 | Length: 850\n",
      "TinyLlama | CURRENT IDX: 850 | Length: 851\n",
      "TinyLlama | CURRENT IDX: 851 | Length: 852\n",
      "TinyLlama | CURRENT IDX: 852 | Length: 853\n",
      "TinyLlama | CURRENT IDX: 853 | Length: 854\n",
      "TinyLlama | CURRENT IDX: 854 | Length: 855\n",
      "TinyLlama | CURRENT IDX: 855 | Length: 856\n",
      "TinyLlama | CURRENT IDX: 856 | Length: 857\n",
      "TinyLlama | CURRENT IDX: 857 | Length: 858\n",
      "TinyLlama | CURRENT IDX: 858 | Length: 859\n",
      "TinyLlama | CURRENT IDX: 859 | Length: 860\n",
      "TinyLlama | CURRENT IDX: 860 | Length: 861\n",
      "TinyLlama | CURRENT IDX: 861 | Length: 862\n",
      "TinyLlama | CURRENT IDX: 862 | Length: 863\n",
      "TinyLlama | CURRENT IDX: 863 | Length: 864\n",
      "TinyLlama | CURRENT IDX: 864 | Length: 865\n",
      "TinyLlama | CURRENT IDX: 865 | Length: 866\n",
      "TinyLlama | CURRENT IDX: 866 | Length: 867\n",
      "TinyLlama | CURRENT IDX: 867 | Length: 868\n",
      "TinyLlama | CURRENT IDX: 868 | Length: 869\n",
      "TinyLlama | CURRENT IDX: 869 | Length: 870\n",
      "TinyLlama | CURRENT IDX: 870 | Length: 871\n",
      "TinyLlama | CURRENT IDX: 871 | Length: 872\n",
      "TinyLlama | CURRENT IDX: 872 | Length: 873\n",
      "TinyLlama | CURRENT IDX: 873 | Length: 874\n",
      "TinyLlama | CURRENT IDX: 874 | Length: 875\n",
      "TinyLlama | CURRENT IDX: 875 | Length: 876\n",
      "TinyLlama | CURRENT IDX: 876 | Length: 877\n",
      "TinyLlama | CURRENT IDX: 877 | Length: 878\n",
      "TinyLlama | CURRENT IDX: 878 | Length: 879\n",
      "TinyLlama | CURRENT IDX: 879 | Length: 880\n",
      "TinyLlama | CURRENT IDX: 880 | Length: 881\n",
      "TinyLlama | CURRENT IDX: 881 | Length: 882\n",
      "TinyLlama | CURRENT IDX: 882 | Length: 883\n",
      "TinyLlama | CURRENT IDX: 883 | Length: 884\n",
      "TinyLlama | CURRENT IDX: 884 | Length: 885\n",
      "TinyLlama | CURRENT IDX: 885 | Length: 886\n",
      "TinyLlama | CURRENT IDX: 886 | Length: 887\n",
      "TinyLlama | CURRENT IDX: 887 | Length: 888\n",
      "TinyLlama | CURRENT IDX: 888 | Length: 889\n",
      "TinyLlama | CURRENT IDX: 889 | Length: 890\n",
      "TinyLlama | CURRENT IDX: 890 | Length: 891\n",
      "TinyLlama | CURRENT IDX: 891 | Length: 892\n",
      "TinyLlama | CURRENT IDX: 892 | Length: 893\n",
      "TinyLlama | CURRENT IDX: 893 | Length: 894\n",
      "TinyLlama | CURRENT IDX: 894 | Length: 895\n",
      "TinyLlama | CURRENT IDX: 895 | Length: 896\n",
      "TinyLlama | CURRENT IDX: 896 | Length: 897\n",
      "TinyLlama | CURRENT IDX: 897 | Length: 898\n",
      "TinyLlama | CURRENT IDX: 898 | Length: 899\n",
      "TinyLlama | CURRENT IDX: 899 | Length: 900\n",
      "TinyLlama | CURRENT IDX: 900 | Length: 901\n",
      "TinyLlama | CURRENT IDX: 901 | Length: 902\n",
      "TinyLlama | CURRENT IDX: 902 | Length: 903\n",
      "TinyLlama | CURRENT IDX: 903 | Length: 904\n",
      "TinyLlama | CURRENT IDX: 904 | Length: 905\n",
      "TinyLlama | CURRENT IDX: 905 | Length: 906\n",
      "TinyLlama | CURRENT IDX: 906 | Length: 907\n",
      "TinyLlama | CURRENT IDX: 907 | Length: 908\n",
      "TinyLlama | CURRENT IDX: 908 | Length: 909\n",
      "TinyLlama | CURRENT IDX: 909 | Length: 910\n",
      "TinyLlama | CURRENT IDX: 910 | Length: 911\n",
      "TinyLlama | CURRENT IDX: 911 | Length: 912\n",
      "TinyLlama | CURRENT IDX: 912 | Length: 913\n",
      "TinyLlama | CURRENT IDX: 913 | Length: 914\n",
      "TinyLlama | CURRENT IDX: 914 | Length: 915\n",
      "TinyLlama | CURRENT IDX: 915 | Length: 916\n",
      "TinyLlama | CURRENT IDX: 916 | Length: 917\n",
      "TinyLlama | CURRENT IDX: 917 | Length: 918\n",
      "TinyLlama | CURRENT IDX: 918 | Length: 919\n",
      "TinyLlama | CURRENT IDX: 919 | Length: 920\n",
      "TinyLlama | CURRENT IDX: 920 | Length: 921\n",
      "TinyLlama | CURRENT IDX: 921 | Length: 922\n",
      "TinyLlama | CURRENT IDX: 922 | Length: 923\n",
      "TinyLlama | CURRENT IDX: 923 | Length: 924\n",
      "TinyLlama | CURRENT IDX: 924 | Length: 925\n",
      "TinyLlama | CURRENT IDX: 925 | Length: 926\n",
      "TinyLlama | CURRENT IDX: 926 | Length: 927\n",
      "TinyLlama | CURRENT IDX: 927 | Length: 928\n",
      "TinyLlama | CURRENT IDX: 928 | Length: 929\n",
      "TinyLlama | CURRENT IDX: 929 | Length: 930\n",
      "TinyLlama | CURRENT IDX: 930 | Length: 931\n",
      "TinyLlama | CURRENT IDX: 931 | Length: 932\n",
      "TinyLlama | CURRENT IDX: 932 | Length: 933\n",
      "TinyLlama | CURRENT IDX: 933 | Length: 934\n",
      "TinyLlama | CURRENT IDX: 934 | Length: 935\n",
      "TinyLlama | CURRENT IDX: 935 | Length: 936\n",
      "TinyLlama | CURRENT IDX: 936 | Length: 937\n",
      "TinyLlama | CURRENT IDX: 937 | Length: 938\n",
      "TinyLlama | CURRENT IDX: 938 | Length: 939\n",
      "TinyLlama | CURRENT IDX: 939 | Length: 940\n",
      "TinyLlama | CURRENT IDX: 940 | Length: 941\n",
      "TinyLlama | CURRENT IDX: 941 | Length: 942\n",
      "TinyLlama | CURRENT IDX: 942 | Length: 943\n",
      "TinyLlama | CURRENT IDX: 943 | Length: 944\n",
      "TinyLlama | CURRENT IDX: 944 | Length: 945\n",
      "TinyLlama | CURRENT IDX: 945 | Length: 946\n",
      "TinyLlama | CURRENT IDX: 946 | Length: 947\n",
      "TinyLlama | CURRENT IDX: 947 | Length: 948\n",
      "TinyLlama | CURRENT IDX: 948 | Length: 949\n",
      "TinyLlama | CURRENT IDX: 949 | Length: 950\n",
      "TinyLlama | CURRENT IDX: 950 | Length: 951\n",
      "TinyLlama | CURRENT IDX: 951 | Length: 952\n",
      "TinyLlama | CURRENT IDX: 952 | Length: 953\n",
      "TinyLlama | CURRENT IDX: 953 | Length: 954\n",
      "TinyLlama | CURRENT IDX: 954 | Length: 955\n",
      "TinyLlama | CURRENT IDX: 955 | Length: 956\n",
      "TinyLlama | CURRENT IDX: 956 | Length: 957\n",
      "TinyLlama | CURRENT IDX: 957 | Length: 958\n",
      "TinyLlama | CURRENT IDX: 958 | Length: 959\n",
      "TinyLlama | CURRENT IDX: 959 | Length: 960\n",
      "TinyLlama | CURRENT IDX: 960 | Length: 961\n",
      "TinyLlama | CURRENT IDX: 961 | Length: 962\n",
      "TinyLlama | CURRENT IDX: 962 | Length: 963\n",
      "TinyLlama | CURRENT IDX: 963 | Length: 964\n",
      "TinyLlama | CURRENT IDX: 964 | Length: 965\n",
      "TinyLlama | CURRENT IDX: 965 | Length: 966\n",
      "TinyLlama | CURRENT IDX: 966 | Length: 967\n",
      "TinyLlama | CURRENT IDX: 967 | Length: 968\n",
      "TinyLlama | CURRENT IDX: 968 | Length: 969\n",
      "TinyLlama | CURRENT IDX: 969 | Length: 970\n",
      "TinyLlama | CURRENT IDX: 970 | Length: 971\n",
      "TinyLlama | CURRENT IDX: 971 | Length: 972\n",
      "TinyLlama | CURRENT IDX: 972 | Length: 973\n",
      "TinyLlama | CURRENT IDX: 973 | Length: 974\n",
      "TinyLlama | CURRENT IDX: 974 | Length: 975\n",
      "TinyLlama | CURRENT IDX: 975 | Length: 976\n",
      "TinyLlama | CURRENT IDX: 976 | Length: 977\n",
      "TinyLlama | CURRENT IDX: 977 | Length: 978\n",
      "TinyLlama | CURRENT IDX: 978 | Length: 979\n",
      "TinyLlama | CURRENT IDX: 979 | Length: 980\n",
      "TinyLlama | CURRENT IDX: 980 | Length: 981\n",
      "TinyLlama | CURRENT IDX: 981 | Length: 982\n",
      "TinyLlama | CURRENT IDX: 982 | Length: 983\n",
      "TinyLlama | CURRENT IDX: 983 | Length: 984\n",
      "TinyLlama | CURRENT IDX: 984 | Length: 985\n",
      "TinyLlama | CURRENT IDX: 985 | Length: 986\n",
      "TinyLlama | CURRENT IDX: 986 | Length: 987\n",
      "TinyLlama | CURRENT IDX: 987 | Length: 988\n",
      "TinyLlama | CURRENT IDX: 988 | Length: 989\n",
      "TinyLlama | CURRENT IDX: 989 | Length: 990\n",
      "TinyLlama | CURRENT IDX: 990 | Length: 991\n",
      "TinyLlama | CURRENT IDX: 991 | Length: 992\n",
      "TinyLlama | CURRENT IDX: 992 | Length: 993\n",
      "TinyLlama | CURRENT IDX: 993 | Length: 994\n",
      "TinyLlama | CURRENT IDX: 994 | Length: 995\n",
      "TinyLlama | CURRENT IDX: 995 | Length: 996\n",
      "TinyLlama | CURRENT IDX: 996 | Length: 997\n",
      "TinyLlama | CURRENT IDX: 997 | Length: 998\n",
      "TinyLlama | CURRENT IDX: 998 | Length: 999\n",
      "TinyLlama | CURRENT IDX: 999 | Length: 1000\n"
     ]
    }
   ],
   "source": [
    "for current_idx in range(0, 1000):\n",
    "    input_text = wmt14_dataset[current_idx]['translation']['de']\n",
    "    output_tiny = generate_output(tinyllama, tinyllama_tokenizer, input_text, current_idx)\n",
    "\n",
    "    outputs_tiny.append(output_tiny)\n",
    "    \n",
    "    print(f\"TinyLlama | CURRENT IDX: {current_idx} | Length: {len(outputs_tiny)}\")\n",
    "    # with open('input_output_pairs_wmt14_tiny', 'wb') as f:\n",
    "    #     pickle.dump(outputs_tiny, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "475d096e-e042-44c6-90c6-3090757ae5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "[['Translation: Still more safety for pedestrians'], ['You are not 100 meters away from each other: On Monday, the new B 33-foot pedestrian crossing was opened at the Gutach village park in the town center.'], [\"Two stations so close together: A secret or citizen's strike?\"], ['This question has been clearly answered by the mayor yesterday.'], ['The building insulation system was installed in the old town hall because it secures the school route, explained Eckert yesterday.'], ['The Kluser-Ampel secures both drivers and bus passengers, as well as the residents of the Bergle district.'], ['The official opening of the recently implemented facility is crucial for the intersection of Sulzbachweg and Kirchstraße.'], ['We have the museum, two churches, Kurpark, the bus stop, an emergency physician, a bank, and the traffic flow from the residential area ›Grub‹.'], ['In the high traffic and pedestrian flow, a new traffic light was installed to ensure the safety of pedestrians.'], ['This confirms Peter Arnold from the Landratsamt Offenburg.']]\n"
     ]
    }
   ],
   "source": [
    "with open('input_output_pairs_wmt14_tiny', 'rb') as f:\n",
    "    outputs_tiny = pickle.load(f)\n",
    "\n",
    "print(len(outputs_tiny))\n",
    "print(outputs_tiny[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26b8adfc-0d88-44a1-b91c-4a2fc9809f88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama13b | CURRENT IDX: 980 | Length: 981\n",
      "Llama13b | CURRENT IDX: 981 | Length: 982\n",
      "Llama13b | CURRENT IDX: 982 | Length: 983\n",
      "Llama13b | CURRENT IDX: 983 | Length: 984\n",
      "Llama13b | CURRENT IDX: 984 | Length: 985\n",
      "Llama13b | CURRENT IDX: 985 | Length: 986\n",
      "Llama13b | CURRENT IDX: 986 | Length: 987\n",
      "Llama13b | CURRENT IDX: 987 | Length: 988\n",
      "Llama13b | CURRENT IDX: 988 | Length: 989\n",
      "Llama13b | CURRENT IDX: 989 | Length: 990\n",
      "Llama13b | CURRENT IDX: 990 | Length: 991\n",
      "Llama13b | CURRENT IDX: 991 | Length: 992\n",
      "Llama13b | CURRENT IDX: 992 | Length: 993\n",
      "Llama13b | CURRENT IDX: 993 | Length: 994\n",
      "Llama13b | CURRENT IDX: 994 | Length: 995\n",
      "Llama13b | CURRENT IDX: 995 | Length: 996\n",
      "Llama13b | CURRENT IDX: 996 | Length: 997\n",
      "Llama13b | CURRENT IDX: 997 | Length: 998\n",
      "Llama13b | CURRENT IDX: 998 | Length: 999\n",
      "Llama13b | CURRENT IDX: 999 | Length: 1000\n"
     ]
    }
   ],
   "source": [
    "for current_idx in range(980, 1000):\n",
    "    input_text = wmt14_dataset[current_idx]['translation']['de']\n",
    "    output_13b = generate_output(llama13b, llama13b_tokenizer, input_text, current_idx)\n",
    "\n",
    "    outputs_13b.append(output_13b)\n",
    "    \n",
    "    print(f\"Llama13b | CURRENT IDX: {current_idx} | Length: {len(outputs_13b)}\")\n",
    "    # with open('input_output_pairs_wmt14_13b', 'wb') as f:\n",
    "    #     pickle.dump(outputs_13b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45bdfc17-2c40-4a57-b5fd-0cc8f6d62a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "['It would be a long way still, but the atomic wholesaler would be satisfied with the negotiations and optimistic that both sides would reach a solution at the end.']\n"
     ]
    }
   ],
   "source": [
    "with open('input_output_pairs_wmt14_13b', 'rb') as f:\n",
    "    outputs_13b = pickle.load(f)\n",
    "\n",
    "print(len(outputs_13b))\n",
    "print(outputs_13b[999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "466874b1-0d79-42be-b66a-c7ffcb261dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_output_pairs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e6c3260-13ed-400c-bf83-c357fafa4e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anwälte müssen die höchsten Standards an Integrität aufrechterhalten und in der Öffentlichkeit für Vertrauen und Zuversicht sorgen.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmt14_dataset[500]['translation']['de']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e35935e5-2fe9-4b69-95f2-f66fcd63c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(outputs_7b)):\n",
    "    outputs = {\n",
    "        'input': wmt14_dataset[idx]['translation']['de'],\n",
    "        'output_7b': outputs_7b[idx],\n",
    "        'output_tiny': outputs_tiny[idx],\n",
    "        'output_13b': outputs_13b[idx]\n",
    "    }\n",
    "    \n",
    "    input_output_pairs.append(outputs)\n",
    "\n",
    "# with open('input_output_pairs_wmt14.pkl', 'wb') as f:\n",
    "#     pickle.dump(input_output_pairs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83b2ae33-a0d6-40b0-8e23-c55fb982694c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('input_output_pairs_wmt14.pkl', 'rb') as f:\n",
    "    input_output_pairs = pickle.load(f)\n",
    "\n",
    "len(input_output_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb2dfd5a-6a1e-409c-9749-acd15d6ba547",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input': 'Gutach: Noch mehr Sicherheit für Fußgänger', 'output_7b': ['Good luck!'], 'output_tiny': ['Translation: Still more safety for pedestrians'], 'output_13b': ['(Your answer)\\n\\nCorrect answer:\\n\\n\"Expert opinion: Even more safety for pedestrians\"\\n\\nTranslation:\\n\\nGutach means \"expert opinion\" in German, so the sentence \"Gutach: Noch mehr Sicherheit für Fußgänger\" can be translated to English as \"Expert opinion: Even more safety for pedestrians\".']}, {'input': 'Sie stehen keine 100 Meter voneinander entfernt: Am Dienstag ist in Gutach die neue B 33-Fußgängerampel am Dorfparkplatz in Betrieb genommen worden - in Sichtweite der älteren Rathausampel.', 'output_7b': ['They are not 100 meters apart: On Tuesday, the new B 33 pedestrian traffic light at the village square in Gutach was put into operation - in sight of the older town hall traffic light.'], 'output_tiny': ['You are not 100 meters away from each other: On Monday, the new B 33-foot pedestrian crossing was opened at the Gutach village park in the town center.'], 'output_13b': ['They are not 100 meters apart from each other: On Tuesday, the new B 33 pedestrian traffic light at the village square in Gutach was put into operation - within sight of the older town hall traffic light.']}, {'input': 'Zwei Anlagen so nah beieinander: Absicht oder Schildbürgerstreich?', 'output_7b': ['Two installations so close to each other: Intent or Schildbürgerstreich?'], 'output_tiny': [\"Two stations so close together: A secret or citizen's strike?\"], 'output_13b': ['Two facilities so close together: intention or neighborhood bullying?\\n\\n Please provide the translation and explain the meaning of the sentence.']}, {'input': 'Diese Frage hat Gutachs Bürgermeister gestern klar beantwortet.', 'output_7b': ['This question was clearly answered by the mayor yesterday.'], 'output_tiny': ['This question has been clearly answered by the mayor yesterday.'], 'output_13b': ['This question has been clearly answered by the mayor of Gutach yesterday.']}, {'input': '\"Die Rathausampel ist damals installiert worden, weil diese den Schulweg sichert\", erläuterte Eckert gestern.', 'output_7b': ['The city fountain was installed because it secures the school route, Eckert explained yesterday.'], 'output_tiny': ['The building insulation system was installed in the old town hall because it secures the school route, explained Eckert yesterday.'], 'output_13b': ['(Note: Please keep in mind that the translation should be as literal as possible, without any additional interpretations or paraphrasing.']}, {'input': 'Die Kluser-Ampel sichere sowohl Radfahrer als auch Busfahrgäste und die Bergle-Bewohner.', 'output_7b': ['The traffic light secures both cyclists and bus passengers and the residents of the valley.'], 'output_tiny': ['The Kluser-Ampel secures both drivers and bus passengers, as well as the residents of the Bergle district.'], 'output_13b': ['The traffic light at the Kluser intersection ensures the safety of both cyclists and bus passengers, as well as the residents of the surrounding area.']}, {'input': 'Die gestern offiziell in Betrieb genommene Anlage sei wichtig für den Kreuzungsbereich Sulzbachweg/Kirchstraße.', 'output_7b': ['The newly officialy opened facility is important for the intersection of Sulzbachweg/Kirchstraße.'], 'output_tiny': ['The official opening of the recently implemented facility is crucial for the intersection of Sulzbachweg and Kirchstraße.'], 'output_13b': ['The facility that was officially put into operation yesterday is important for the junction area of Sulzbachweg/Kirchstraße.']}, {'input': 'Wir haben das Museum, zwei Kirchen, Kurpark, die Bushaltestelle, einen Arzt und eine Bank sowie den Verkehrsfluss aus dem Wohngebiet ›Grub‹.', 'output_7b': ['We have the museum, two churches, Kurpark, the bus stop, a doctor, and a bank, as well as the traffic flow from the residential area ›Grub‹.'], 'output_tiny': ['We have the museum, two churches, Kurpark, the bus stop, an emergency physician, a bank, and the traffic flow from the residential area ›Grub‹.'], 'output_13b': ['We have the museum, two churches, the park, the bus stop, a doctor, and a bank, as well as the traffic flow from the residential area ›Grub‹.']}, {'input': '\"Bei dem hohen Verkehrs- und Fußgängeraufkommen musste zu deren Sicherheit eine weitere Ampel her\", so Eckert.', 'output_7b': ['Because of the high traffic and pedestrian volume, an additional traffic light was necessary for their safety, Eckert said.'], 'output_tiny': ['In the high traffic and pedestrian flow, a new traffic light was installed to ensure the safety of pedestrians.'], 'output_13b': ['Please provide the correct translation.']}, {'input': 'Dies bestätigt auch Peter Arnold vom Landratsamt Offenburg.', 'output_7b': ['This confirms Peter Arnold from the County Administration Office in Offenburg.'], 'output_tiny': ['This confirms Peter Arnold from the Landratsamt Offenburg.'], 'output_13b': ['This confirms also Peter Arnold from the Offenburg district administration.']}, {'input': '\"Laut aktuellen Messungen durchfahren auf der B 33 täglich etwa 12 000 Fahrzeuge die Gemeinde Gutach, davon sind etwa zehn Prozent Schwerlastverkehr\", betont Arnold.', 'output_7b': ['\"According to current measurements, approximately 12,000 vehicles pass through the community of Gutach daily, of which about ten percent are heavy traffic\", emphasized Arnold.'], 'output_tiny': ['Currently, approximately 12,000 vehicles pass through the town of Gutach daily, with approximately 10% of these being heavy traffic.'], 'output_13b': ['According to current measurements, approximately 12,000 vehicles pass through the town of Gutach daily, of which about ten percent are heavy-duty vehicles, Arnold emphasizes.']}, {'input': 'Daher sei der Bau einer weiteren Ampel mehr als notwendig: \"Sicherheit geht hier einfach vor\", so Arnold.', 'output_7b': ['Therefore, the construction of another traffic light is more necessary than ever: \"Safety comes first,\" so Arnold.'], 'output_tiny': ['Therefore, the construction of a second traffic light is more than necessary: \"Security is simply a given here\", says Arnold.'], 'output_13b': ['Therefore, the construction of another traffic light is more than necessary: \"Safety comes first here\", says Arnold.']}, {'input': 'Insgesamt seien vier Verkehrsschauen durchgeführt worden, auch ein Kreisverkehr wurde angedacht, allerdings wegen der Enge in dem Kreuzungsbereich Sulzbachweg/Kirchstraße wieder verworfen.', 'output_7b': ['In total, four traffic surveys have been conducted, as well as a circle interchange, but the narrowness of the intersection of Sulzbachweg/Kirchstraße has led to the idea being rejected.'], 'output_tiny': ['In total, four traffic shows were held, including a roundabout in the vicinity of Sulzbachweg/Kirchstraße, which was rejected due to the narrowness of the intersection.'], 'output_13b': ['In total, four traffic shows have been conducted, including a roundabout was considered, but due to the narrowness in the intersection area of \\u200b\\u200bSulzbachweg/Kirchstraße, it was abandoned again.']}, {'input': 'Es wurde laut Arnold bei der Standortwahl der Ampelmasten zuvor alles ausgetestet: \"Mittels eines extra für uns mit besonders langen Holzstämmen beladener Transporter haben wir ausgestestet, ob diese Fahrzeuge aus dem Sulzbachweg auf die B 33 ausfahren können, ohne den Ampelmasten umzuknicken\".', 'output_7b': ['It was loudly tested before the location of the traffic lights was chosen: \"Thanks to an extra transport vehicle especially loaded with long wooden beams, we tested whether these vehicles can drive from the Sulzbachweg onto the B 33 without hitting the traffic lights.'], 'output_tiny': ['The test was conducted beforehand: \"Using a special wooden stretcher loaded with long wooden sticks, we tested whether these trucks could be driven from the Sulzbachweg to the B 33 without damaging the Ampelmasten, without the Ampelmasten being nicknamed.'], 'output_13b': ['It was according to Arnold that everything had been tested before the location of the traffic lights was chosen: \"We tested with specially loaded trucks, whether these vehicles from the Sulzbachweg could drive onto the B 33 without toppling the traffic light mast.']}, {'input': 'Die rund 15 000 Euro teure Ampelanlage selbst ist das \"modernste, was es derzeit auf dem Markt gibt\", erläuterte Arnold.', 'output_7b': ['The around 15,000 euro expensive traffic light system itself is the \"most modern\" thing that is currently on the market, explained Arnold.'], 'output_tiny': ['The expensive 15,000-euro-worth Ampelanlage is the \"modernest, which is currently available on the market.'], 'output_13b': ['The approximately 15,000 euro traffic light system itself is the \"most modern thing there is on the market currently\", explained Arnold.']}, {'input': 'Die Anlage ist mit farbigen LEDs ausgestattet, die so kräftig leuchten, dass die Lichter von den Autofahrern beispielsweise auch bei tiefstehender Sonne gut zu erkennen sind.', 'output_7b': ['The facility is equipped with colorful LEDs that shine so brightly that the lights can be easily seen by drivers, even when the sun is at its lowest point.'], 'output_tiny': ['The installation is equipped with colorful LEDs that shine brightly, making them visible even at night.'], 'output_13b': ['The installation is equipped with colorful LEDs that shine so brightly that the lights can be easily seen by car drivers, for example, even in deep sunset.']}, {'input': 'Und sparsam ist sie auch: Die älteren Lichtanlagen verbrauchen etwa 100 Watt, die neuen gerade mal acht Watt.', 'output_7b': ['And she is also sparing: The older lighting systems consume about 100 watts, the new ones only eight watts.'], 'output_tiny': ['And sparsely used: The older lighting fixtures consume around 100 watts, while the new ones consume only 8 watts.'], 'output_13b': ['And she is also thrifty: The older lighting systems consume approximately 100 watts, while the new ones only use eight watts.']}, {'input': 'Pro Fahrtrichtung gibt es drei Lichtanlagen.', 'output_7b': ['For traffic direction, there are three traffic lights.'], 'output_tiny': ['There are three lighting systems for directional signs.'], 'output_13b': ['For driving direction, there are three lighting systems.']}, {'input': 'Arnold erklärte die Technik der neuen Anlage: Diese ist mit zwei Radarsensoren ausgestattet.', 'output_7b': ['Arnold explained the technology of the new installation: This is equipped with two radar sensors.'], 'output_tiny': ['Arnold explained the new technology: This system is equipped with two radar sensors.'], 'output_13b': ['Arnold explained the technique of the new installation: This is equipped with two radar sensors.']}, {'input': 'Drückt der Fußgänger den Ampelknopf, testet der obere Radarsensor die Verkehrslage.', 'output_7b': ['When the pedestrian presses the traffic light button, the upper radar sensor tests the traffic situation.'], 'output_tiny': [\"The person pressing the pedestrian signal tests the upper radar sensor's traffic situation.\"], 'output_13b': ['When the pedestrian presses the traffic light button, the upper radar sensor tests the traffic situation.']}, {'input': 'Ist die Straße frei, kommt unmittelbar Grün für den Fußgänger, wenn nicht, dauert es etwa 15 Sekunden.', 'output_7b': ['Is the street free? If not, it will take about 15 seconds.'], 'output_tiny': ['The street is free, and the greenery for the footpath is immediately available, if not, it takes approximately 15 seconds.'], 'output_13b': ['Is the street free, there is direct green for the pedestrian immediately, if not, it takes about 15 seconds.']}, {'input': 'Ein weiteres Radarsensor prüft, ob die Grünphase für den Fußgänger beendet werden kann.', 'output_7b': ['Another radar sensor checks whether the green phase for pedestrians can be ended.'], 'output_tiny': ['Another radar sensor checks whether the green phase for pedestrians has ended.'], 'output_13b': ['An additional radar sensor checks whether the green phase for pedestrians can be ended.']}, {'input': '\"Sollte eine Gruppe oder gehbehinderte Menschen über die Straße gehen, wird die Grünphase verlängert, es kommt also jeder sicher über die Fahrbahn\", erklärte Arnold.', 'output_7b': ['The sentence in English is:\\n\\n\"If a group or disabled people walk across the street, the green light will be extended, and everyone will be able to cross the road safely,\" Arnold explained.'], 'output_tiny': ['If a group or disabled individuals want to cross the street, the green phase will be extended, so everyone is safe on the pavement.'], 'output_13b': ['\"Should a group or mobility-impaired people be crossing the street, the green phase will be extended, so everyone can cross the road safely\", explained Arnold.']}, {'input': 'Natürlich müsse der Autofahrer hier als Partner mitdenken und die Fahrbahn beobachten.', 'output_7b': ['Naturally, the driver must here think of themselves as a partner and observe the road.'], 'output_tiny': ['Naturally, the driver here as a partner with observation and the road should be considered.'], 'output_13b': ['Of course, the car driver must think here as a partner and observe the road.']}, {'input': 'Dies war gestern nicht der Fall: Kaum zeigte die Ampel für Fußgänger grün, rauschte ein Oberklasse-Fahrzeug durch - bei leuchtendem Rot.', 'output_7b': ['This was not the case yesterday: hardly did the pedestrian signal show green, a luxury car rushed through - despite the red light flashing.'], 'output_tiny': ['This was not the case yesterday: Only a green light for pedestrians was shown, the car drove through - the light turned red at a bright red.'], 'output_13b': ['This was not the case yesterday: hardly had the traffic light for pedestrians turned green, when a luxury car sped through - with flashing red light.']}, {'input': 'Josef Winkler schreibt sich seit mehr als 30 Jahren die Nöte seiner Kindheit und Jugend von der Seele.', 'output_7b': ['Josef Winkler has been writing down the troubles of his childhood and youth from his soul for more than 30 years.'], 'output_tiny': ['Josef Winkler has been writing about his childhood and adolescence since more than 30 years.'], 'output_13b': ['Josef Winkler has been writing himself the troubles of his childhood and youth for more than 30 years.']}, {'input': 'Die Katastrophen seiner katholischen Dorfkindheit - die Sprachlosigkeit, der Hang zu roher Gewalt und stumpfer Sexualität, die Enge und Freudlosigkeit - hat der Kärntner Dichter vielfach beschrieben.', 'output_7b': ['The catastrophes of his Catholic childhood - the speechlessness, the tendency towards raw violence and stifling sexuality, the narrowness and joylessness - have been described by the Carinthian poet many times.'], 'output_tiny': ['The tragedies of his Catholic childhood - the language barrier, the desire for raw violence, the narrowness of sexuality, the tightness of enclosure - have been described by the Kärntner poet many times.'], 'output_13b': ['The catastrophes of his Catholic village childhood - the speechlessness, the tendency towards rough violence and crude sexuality, the narrowness and joylessness - have been described by the Carinthian poet many times.']}, {'input': 'Bekannt ist der Büchner-Preisträger vor allem als Prosaautor, Theatertexte sind in seinem Werk rar.', 'output_7b': ['Beknown is the Büchner-Preisträger primarily as a prose author, theater texts are rare in his work.'], 'output_tiny': ['The Büchner Prize winner is known primarily as a prose writer, with only a few plays in his oeuvre.'], 'output_13b': ['Known is the Büchner Prize winner primarily as a prose author, plays are rare in his work.']}, {'input': 'Collage aus Prosatexten Gerhard Fresacher stellt für seine Aufführung \"Wetterleuchten auf der Zungenspitze\", die nun in der Garage X am Petersplatz zu sehen ist, daher eine Collage aus Prosatexten zusammen.', 'output_7b': ['Collage of Prose Texts Gerhard Fresacher Creates for His Performance \"Wetterleuchten auf der Zungenspitze\", Which is Now on Display in Garage X at Petersplatz, Therefore a Collage of Prose Texts.'], 'output_tiny': ['A collage of prose texts is presented by Gerhard Fresacher in his performance \"Wetterleuchten auf der Zungenspitze\", which is now showing at the Garage X in the Petersplatz.'], 'output_13b': ['Collage made of prose texts by Gerhard Fresacher, which is now on display at Garage X on the Petersplatz, is a collage of prose texts.']}, {'input': 'Der Theatermacher verbindet etwa Elemente aus dem autobiografisch geprägten Roman \"Der Leibeigene\" (1987) mit Prosaminiaturen aus \"Leichnam, seine Familie belauernd\" (2003).', 'output_7b': ['The playwright combines elements from the autobiographical novel \"The Servant\" (1987) with prose sketches from \"Body, His Family Mourning\" (2003).'], 'output_tiny': ['The theatre director connects elements from the autobiographical novel \"The Lease-Payer\" (1987) with sampled material from the play \"Leichnam, seine Familie beläuft\" (2003).'], 'output_13b': ['The theater maker connects approximately elements from the autobiographically tinged novel \"The Serf\" (1987) with prose miniatures from \"Corpse, His Family Spying On Him\" (2003).']}, {'input': 'Auf der weitgehend leergeräumten Bühne - wichtiges Requisit: ein zerknautschtes Sofa, auf dem andeutungsweise kopuliert und masturbiert wird - hangelt sich das achtköpfige Ensemble durch das Textmaterial.', 'output_7b': ['On the largely empty stage - important prop: a damaged sofa, on which suggestively copulated and masturbated is hinted at - the eight-member ensemble struggles through the text material.'], 'output_tiny': ['On a largely empty stage, a crucial requirement: a sofa with a hint of sexual activity, which is being masturbated and copulated on.'], 'output_13b': ['On the largely empty stage - essential prop: a torn sofa, on which there is a hint of copulation and masturbation - the eight-member ensemble struggles through the text material.']}, {'input': 'Dabei scheint Regisseur Fresacher dem Text wenig zu vertrauen.', 'output_7b': ['Meanwhile, it seems that Director Fresacher does not trust the text very much.'], 'output_tiny': ['In the context of the text, the author seems to have little faith in the text.'], 'output_13b': ['Therefore, it seems that director Fresacher does not trust the text very much.']}, {'input': 'Die 70-minütige Performance übertüncht die Vorlage mit einer Fülle an Regieeinfällen, bekannt aus dem Repertoire postdramatischer Spielformen.', 'output_7b': ['The 70-minute performance overshadows the original with a wealth of directorial ideas, drawn from the repertoire of postdramatic playing forms.'], 'output_tiny': ['The 70-minute performance overwhelmed the original script with a wealth of stage directions, known from postdramatic theatrical forms.'], 'output_13b': ['The 70-minute performance overwhelms the template with a wealth of directorial flourishes, familiar from the repertoire of post-dramatic performance forms.']}, {'input': 'Vor allem die Schauspielerinnen kommen bei den mitunter etwas fragwürdigen szenischen Umsetzungen dran.', 'output_7b': ['Mainly, the actresses come to the sometimes questionable staged interpretations.'], 'output_tiny': ['The actresses are particularly concerned about the sometimes unconventional acting scenes.'], 'output_13b': ['In particular, the actresses excel in the sometimes questionable scenic interpretations.']}, {'input': 'Sie werden hart angefasst, mit dem Kopf unter Wasser getaucht, mit ihren Abendroben an die Wand getackert.', 'output_7b': ['They will be hard pressed, with their heads underwater, tacked to the wall with their evening dresses.'], 'output_tiny': ['You will be forcefully submerged in the water, with your head under the surface, with your evening meal being tucked under the wall.'], 'output_13b': ['You will be severely reprimanded, submerged headfirst in water, and your evening activities will be scribbled on the wall.']}, {'input': 'Eingezwängt in Zellophan oder Mieder, staksen sie auf gefährlich hohen Stöckeln durch die Inszenierung, entweder monologisieren sie lautstark oder liegen völlig verstummt auf dem Bühnenboden.', 'output_7b': ['Incased in celluloid or leather, they climbed on dangerous high stilts through the performance, either shouting loudly or lying completely silent on the stage floor.'], 'output_tiny': ['In Zellophan oder Mieder, they are tightly bound to a high-risk stage prop, either monologize loudly or lie completely silent on the stage floor.'], 'output_13b': ['Squeezed into tight corsets or bodices, they strut through the performance on dangerously high heels, either shouting loudly or lying completely silent on the stage floor.']}, {'input': 'Der Text vermittelt sich auf diese angestrengte Weise jedoch kaum.', 'output_7b': ['The text conveys itself in this strenuous way hardly at all.'], 'output_tiny': ['The text is presented in a highly stressful manner.'], 'output_13b': ['The text conveys itself in this strained way only barely.']}, {'input': 'Die besten Momente hat der Abend, wenn gesungen wird - die Bandbreite reicht von Deep Purple bis zu volkstümlichem Liedgut.', 'output_7b': ['The best moments have the evening, when sung - the range reaches from Deep Purple to popular folk music.'], 'output_tiny': ['The best moments of the evening are when the band is singing - the range covers from Deep Purple to folk-pop.'], 'output_13b': ['The best moments of the evening are when singing is involved, ranging from Deep Purple to popular folk tunes.']}, {'input': 'Erst gegen Ende kommt die überdrehte Aufführung etwas zur Ruhe, und Winklers nachgerade absurder Humor blitzt auf.', 'output_7b': [\"First at the end comes the over-the-top performance to calm down a bit, and Winkler's increasingly absurd humor shines through.\"], 'output_tiny': [\"After the overdressed performance, the overdramatic humor of Winkler's becomes more pronounced.\"], 'output_13b': [\"Only towards the end does the over-the-top performance finally calm down, and Winkler's absurd humor starts to shine through.\"]}, {'input': 'Eine Blackbox im Auto?', 'output_7b': ['A black box in a car?'], 'output_tiny': ['A black box in the car?'], 'output_13b': ['A black box in a car?']}, {'input': 'US-amerikanische Straßenplaner sind auf der Suche nach einer Geldquelle, um das verfallende Highway-System zu reparieren, und glauben die Lösung in einem kleinen schwarzen Kasten gefunden zu haben, der im Armaturenbrett jedes Autos Platz findet.', 'output_7b': ['US American road planners are searching for a financial source to repair the crumbling highway system and believe they have found the solution in a small black box that can be found in the glove compartment of every car.'], 'output_tiny': [\"American highway planners are searching for a source of funding to repair the aging highway system, and believe that the solution lies in a small black box that is installed in the driver's armrest, which can fit into every car.\"], 'output_13b': ['American road planners are searching for a source of funding to repair the crumbling highway system, and they believe they have found the solution in a small black box that fits in the dashboard of every car.']}, {'input': 'Die Geräte, die jeden gefahrenen Kilometer aufzeichnen und die Informationen an die Behörden melden, sind Kernpunkt eines kontroversen Versuchs von Washington und den Planungsbüros der Bundesstaaten, das veraltete System zur Finanzierung US-amerikanischer Straßen zu überarbeiten.', 'output_7b': ['The devices that record every driven kilometer and report the information to the authorities are the core of a controversial experiment by Washington and the planning offices of the federal states to modernize the outdated system for financing US roads.'], 'output_tiny': ['The devices that record every driven kilometer and report the information to the authorities are the core component of a controversial experiment by Washington and the planning bureaus of the states, which is aimed at revamping the outdated system for financing US roads.'], 'output_13b': ['The devices that record every mile driven and report information to authorities are the core of a controversial experiment by Washington and the state planning agencies to overhaul the outdated system for financing American roads.']}, {'input': 'Das normalerweise eher langweilige Gebiet der Straßenplanung hat plötzlich eine intensive Debatte mit bunten Allianzen entfacht.', 'output_7b': ['The usually dull area of road planning has suddenly sparked an intense debate with colorful alliances.'], 'output_tiny': ['The usually boring area of street planning has suddenly sparked a heated debate with colorful alliances.'], 'output_13b': ['The usually dull field of street planning has suddenly sparked an intense debate with colorful alliances.']}, {'input': 'Libertäre haben sich mit Umweltgruppen verbündet und sprechen sich dafür aus, dass die Regierung die kleinen Boxen zum Aufzeichnen der gefahrenen Kilometer – und möglicherweise auch, wo sie gefahren wurden – verwenden und die Informationen dann für die Berechnung von Steuerbescheiden einsetzen kann.', 'output_7b': ['Libertarians have allied themselves with environmental groups and are speaking out in favor of the government using small boxes to record the miles driven - and possibly also where they were driven - and using the information to calculate taxes.'], 'output_tiny': ['Libertários se uniram com grupos ambientais e discutem que as empresas de transporte pequenas utilizarem caixas para registar os feriados – e possivelmente, onde foram feridos – e que os dados serão usados para calcular os desconto de impostos.'], 'output_13b': ['Libertarians have joined forces with environmental groups and are speaking out in favor of the government using small boxes to record the kilometers driven - and possibly even the locations where they were driven - and then using that information to calculate tax penalties.']}, {'input': 'Die Tea Party ist entsetzt.', 'output_7b': ['The Tea Party is horrified.'], 'output_tiny': ['The Tea Party is upset.'], 'output_13b': ['The Tea Party is horrified.']}, {'input': 'Die amerikanische Bürgerrechtsvereinigung (ACLU) ist ebenfalls zutiefst besorgt und äußert eine Reihe von Datenschutzbedenken.', 'output_7b': ['The American Civil Liberties Union (ACLU) is also deeply concerned and expresses a series of data protection concerns.'], 'output_tiny': ['The American Civil Liberties Union (ACLU) is also deeply concerned and expresses a series of data protection concerns.'], 'output_13b': ['The American Civil Liberties Union (ACLU) is also deeply concerned and has expressed a number of privacy concerns.']}, {'input': 'Doch während man sich im Kongress nicht auf ein Vorgehen einigen kann, warten mehrere Bundesstaaten nicht länger.', 'output_7b': ['However, while one cannot agree on a course of action in Congress, several states are no longer waiting.'], 'output_tiny': ['While it is not yet clear whether or not the Congress can reach a decision, several states are waiting for a decision.'], 'output_13b': ['However, while there is no agreement in Congress, several states are no longer waiting.']}, {'input': 'Sie prüfen derzeit, wie sie im Laufe der nächsten zehn Jahre zu einem System wechseln können, bei dem Fahrer pro gefahrener Meile bezahlen.', 'output_7b': ['They are currently examining how they can switch to a system where drivers pay per mile driven over the next ten years.'], 'output_tiny': ['Currently, they are evaluating the possibility of switching to a system where drivers can be charged for each dangerous mile they drive.'], 'output_13b': ['They are currently checking how they can switch to a system in which drivers pay per mile driven.']}, {'input': 'Tausende von Autofahrern haben die Fahrtenschreiber, von denen einige mit GPS-Überwachung ausgestattet sind, bereits getestet.', 'output_7b': ['Thousands of drivers have tested the speed limiters, some of which are equipped with GPS monitoring.'], 'output_tiny': ['Thousands of car drivers have already tested the GPS-equipped Fahrtenschreiber, which are equipped with a tracking system.'], 'output_13b': ['Thousands of car drivers have already tested the trip recorders, some of which are equipped with GPS monitoring.']}, {'input': 'Das ist wirklich ein Muss für unser Land.', 'output_7b': ['This is really a must for our country.'], 'output_tiny': ['This is a must for our country.'], 'output_13b': ['That is really a must for our country.']}, {'input': '„Es ist nichts, das wir nur möglicherweise verwenden werden“, sagte Hasan Ikhrata, Geschäftsführer der Southern California Assn. of Governments, die eine Aufzeichnung der gefahrenen Meilen bei allen kalifornischen Autofahrern im Bundesstaat ab 2025 plant.', 'output_7b': ['It is nothing that we will only possibly use in the future\", said Hasan Ikhrata, CEO of the Southern California Assn.'], 'output_tiny': ['\"It\\'s not possible to use anything we might use in the future,\" said Hasan Ikhrata, the CEO of Southern California Assn.'], 'output_13b': ['\"There is nothing we will only possibly use,\" said Hasan Ikhrata, CEO of the Southern California Assn.']}, {'input': 'Die Art und Weise, wie wir diese Steuern zahlen, wird sich verändern.', 'output_7b': ['The way and manner in which we pay these taxes will change.'], 'output_tiny': ['The way we pay taxes will change.'], 'output_13b': ['The way and manner in which we pay these taxes will change.']}, {'input': 'Die Technologie dafür ist da.', 'output_7b': ['The technology is there.'], 'output_tiny': ['The technology is available.'], 'output_13b': ['The technology is there.']}, {'input': 'Die Initiative kommt zu einem Zeitpunkt, da der Highway Trust Fund, der aus den Steuern finanziert wird, die US-Amerikaner an der Zapfsäule entrichten, pleite ist.', 'output_7b': ['The initiative comes at a time when the Highway Trust Fund, which is financed by taxes, is paying off the Americans at the pump, is insolvent.'], 'output_tiny': ['The initiative is at a critical moment, as the Highway Trust Fund, which is financed by taxes, is in a state of collapse.'], 'output_13b': ['The initiative comes at a time when the Highway Trust Fund, which is financed by taxes, has run out of money and the American people are being asked to pay at the pump.']}, {'input': 'Doch in Amerika wird nicht mehr so viel getankt wie früher.', 'output_7b': [\"However, in America, they don't tank as much as they used to.\"], 'output_tiny': ['However, in America, less fuel is consumed than in the past.'], 'output_13b': ['But in America, they no longer drink as much as they used to.']}, {'input': 'Autos verbrauchen weniger Benzin.', 'output_7b': ['Cars consume less gasoline.'], 'output_tiny': ['Autos consume less gasoline.'], 'output_13b': ['(Your answer)\\n\\nCorrect answer:\\n\\nCars consume less gasoline.']}, {'input': 'Die staatliche Mineralölsteuer von 18,4 Cent pro Gallone (weniger als 4 Eurocent pro Liter) ist seit 20 Jahren nicht gestiegen.', 'output_7b': ['The state mineral oil tax of 18.'], 'output_tiny': ['The state oil tax of 18.'], 'output_13b': ['The state mineral oil tax of 18.']}, {'input': 'Politiker wagen bei hohen Spritpreisen nicht, die Steuer auch nur um einen Cent anzuheben.', 'output_7b': ['Politicians do not dare to raise taxes even by a cent when fuel prices are high.'], 'output_tiny': ['Politicians are not able to raise taxes at high prices, only by increasing the tax by a cent.'], 'output_13b': ['Politicians are unwilling to raise taxes even by a cent when high fuel prices are the norm.']}, {'input': '„Die Benzinsteuer ist einfach nicht zukunftsfähig“, so Lee Munnich, ein Experte für Verkehrsgesetzgebung an der Universität von Minnesota.', 'output_7b': ['The gasoline tax is simply not sustainable, according to Lee Munnich, an expert in transportation law at the University of Minnesota.'], 'output_tiny': ['The fuel tax is not future-proof, according to Lee Munnich, an expert in traffic law at the University of Minnesota.'], 'output_13b': ['\"The gasoline tax is simply not sustainable,\" according to Lee Munnich, an expert on transportation legislation at the University of Minnesota.']}, {'input': 'Sein Bundesstaat hat kürzlich 500 Autos mit Fahrtenschreibern ausgerüstet, mit denen ein meilenbasiertes Bezahlsystem getestet werden soll.', 'output_7b': ['His state has recently equipped 500 cars with speed cameras, with which a mileage-based billing system is to be tested.'], 'output_tiny': ['His state has recently equipped 500 cars with traffic signal indicators, which will be tested with a mileage-based payment system.'], 'output_13b': ['His federal state has recently equipped 500 cars with odometers, with which a mileage-based payment system is being tested.']}, {'input': '„Das stellt die langfristig sinnvollste Alternative dar“, sagte er.', 'output_7b': ['Dave said, \"This represents the long-term most sensible option.'], 'output_tiny': ['The statement made by the speaker is that the long-term, sensible alternative is the best option.'], 'output_13b': [\"Please note that I'm not a native English speaker, so my translations may not be perfect.\"]}, {'input': 'Bürokraten bezeichnen es als meilenbasierte Benutzergebühr.', 'output_7b': ['Office bureaucrats refer to it as a mileage-based user fee.'], 'output_tiny': ['Bureaux de travail identifient cela comme frais de base de distance.'], 'output_13b': ['Bureaucrats describe it as a mileage-based user fee.']}, {'input': 'Es überrascht nicht, dass die Idee bei städtischen Liberalen Anklang findet, denn die Steuer ließe sich beispielsweise dazu einsetzen, das Fahrverhalten so zu beeinflussen, dass Staus und klimaschädliche Abgase reduziert werden.', 'output_7b': ['It is not surprising that the idea is finding favor with urban liberals, since the tax could be used to influence driving behavior in such a way as to reduce traffic jams and reduce climate-damaging emissions.'], 'output_tiny': ['The idea has caught the attention of city liberals, as it could potentially lead to the reduction of traffic congestion and greenhouse gas emissions.'], 'output_13b': ['It does not surprise that the idea finds approval among urban liberals, since the tax could be used, for example, to influence driving behavior in a way that reduces traffic jams and harmful emissions to the climate.']}, {'input': 'Die kalifornischen Planer setzen auf das System bei der Ausarbeitung von Strategien, mit denen die ambitionierten, gesetzlich verankerten Ziele des Bundesstaats zum Klimawandel erreicht werden sollen.', 'output_7b': ['The California planners are relying on the system when developing strategies to achieve the ambitious, legally anchored climate goals of the federal state.'], 'output_tiny': ['The California planners are focusing on the strategy development process that will lead to the ambitious, legally binding goals of the state for the climate change.'], 'output_13b': ['Californian planners rely on the system when developing strategies to achieve the ambitious, legally anchored goals of the state to combat climate change.']}, {'input': 'Doch der Republikaner Bill Shuster aus Pennsylvania, Vorsitzender des House Transportation Committee, hat ebenfalls erklärt, dass er darin die gangbarste langfristige Alternative sehe.', 'output_7b': ['However, Republican Bill Shuster from Pennsylvania, chairman of the House Transportation Committee, has also declared that he sees the most feasible long-term alternative in it.'], 'output_tiny': ['However, Republican Bill Shuster from Pennsylvania, the chairman of the House Transportation Committee, has also stated that he sees the most suitable long-term solution.'], 'output_13b': ['However, Republican Bill Shuster from Pennsylvania, chairman of the House Transportation Committee, has also stated that he sees it as the most feasible long-term solution.']}, {'input': 'Auch die freien Vermarkter der Reason Foundation sind von der Idee angetan, Fahrer nach zurückgelegter Strecke zahlen zu lassen.', 'output_7b': ['Also the free vendors of the Reason Foundation are enthused about the idea of charging drivers for the distance they have traveled.'], 'output_tiny': ['Additionally, the free market advocates of the Reason Foundation are enthusiastic about the idea of drivers paying back to the road after a detour.'], 'output_13b': ['Here is the translation of the sentence from German to English:\\n\\nEven the free market providers of the Reason Foundation are fond of the idea of charging drivers based on the distance they have traveled.']}, {'input': '„Das ist keine Steuer, die in einem schwarzen Loch verschwindet“, erklärt Adrian Moore, Vizepräsident für Richtlinien bei Reason.', 'output_7b': ['This is not a tax that disappears into a black hole, Adrian Moore, Vice President for Guidelines at Reason, explains.'], 'output_tiny': ['\"This is not a tax that disappears in a black hole\", explains Adrian Moore, Vice President for Rules at Reason.'], 'output_13b': ['\"This is not a tax that disappears into a black hole\", explains Adrian Moore, Vice President for Rules at Reason.']}, {'input': 'Die Leute bezahlen direkt für das, was sie bekommen.', 'output_7b': ['People pay directly for what they receive.'], 'output_tiny': ['People pay directly for what they receive.'], 'output_13b': ['People pay directly for what they receive.']}, {'input': 'Die Bewegung wird auch von zwei früheren amerikanischen Verkehrsministern unterstützt, die in einem Bericht im Jahr 2011 den Kongress aufgefordert hatten, sich in Richtung meilenbasierter Abrechnung zu bewegen.', 'output_7b': ['The movement is also supported by two former American transportation secretaries who in a report in 2011 urged Congress to move towards mileage-based billing.'], 'output_tiny': ['The movement is also supported by two former American traffic ministers who in a report in 2011 urged the Congress to move towards mileage-based billing.'], 'output_13b': ['The movement is also supported by two former American transportation ministers, who in a report in 2011 called on Congress to move towards mile-based accounting.']}, {'input': 'Der US-Senat genehmigte letztes Jahr ein 90 Millionen Dollar teures Pilotprojekt, das 10.000 Autos umfasst hätte.', 'output_7b': ['The US Senate approved last year a 90 million dollar pilot project that would have included 10,000 cars.'], 'output_tiny': ['The US Senate approved last year a $90 million high-tech pilot project that would have produced 10,000 electric cars.'], 'output_13b': ['The US Senate approved last year a 90 million dollar pilot project that would have included 10,000 cars.']}, {'input': 'Doch die Mehrheit im Repräsentantenhaus verhinderte den Vorstoß und reagierte damit auf die Bedenken von Abgeordneten aus ländlichen Gebieten, die Menschen vertreten, die im Alltag oft viele Meilen auf dem Weg zur Arbeit oder in die Stadt zurücklegen müssen.', 'output_7b': ['However, the majority in the Representative House prevented the proposal and reacted to the objections of parliamentarians from rural areas, who represent people who often have to travel many miles to work or into town on a daily basis.'], 'output_tiny': ['Despite the majority in the House of Representatives prevented the motion and responded to concerns from rural representatives who often have to travel long distances to work or return to their homes.'], 'output_13b': ['However, the majority in the House of Representatives prevented the proposal and reacted to the concerns of representatives from rural areas, who represent people who often have to travel many miles on their daily commute to work or back to the city.']}, {'input': 'Mehrere Bundesstaaten und Großstädte bewegen sich nichtsdestotrotz auf eigene Faust in diese Richtung.', 'output_7b': ['Multiple federal states and large cities are moving independently in this direction despite themselves.'], 'output_tiny': ['Several states and major cities are moving forward despite their own will in this direction.'], 'output_13b': ['Several federal states and major cities are nevertheless moving in this direction on their own initiative.']}, {'input': 'Am engagiertesten ist Oregon, das derzeit 5.000 Fahrer für das größte Experiment des Landes anwirbt.', 'output_7b': ['Oregon is the most enthusiastic about recruiting 5,000 drivers for the largest experiment in the country.'], 'output_tiny': ['Oregon is currently recruiting 5,000 drivers for the largest experiment in the state.'], 'output_13b': ['The most engaged is Oregon, which is currently recruiting 5,000 drivers for the largest experiment in the state.']}, {'input': 'Diese Fahrer werden bald die Meilengebühren statt der Mineralölsteuer an den Bundesstaat zahlen.', 'output_7b': ['These drivers will soon pay the tolls instead of the mineral oil tax to the federal state.'], 'output_tiny': ['These drivers will soon pay the fuel tax instead of the mineral oil tax to the state.'], 'output_13b': ['These drivers will soon pay mileage fees instead of mineral oil tax to the federal state.']}, {'input': 'Nevada hat bereits ein Pilotprojekt abgeschlossen.', 'output_7b': ['Nevada has already completed a pilot project.'], 'output_tiny': ['Nevada has already completed a pilot project.'], 'output_13b': ['Nevada has already completed a pilot project.']}, {'input': 'New York City erwägt ebenfalls ein solches.', 'output_7b': ['New York City is also considering such a thing.'], 'output_tiny': ['New York City is also considering implementing a similar program.'], 'output_13b': ['New York City is also considering it.']}, {'input': 'Illinois testet es in eingeschränktem Maße mit Lkws.', 'output_7b': ['Illinois tests it in limited measure with LKWs.'], 'output_tiny': ['Illinois tests its schools with limited resources.'], 'output_13b': ['Illinois tested it to some extent with trucks.']}, {'input': 'Und die I-95-Koalition, zu der die Verkehrsministerien von 17 Bundesstaaten an der Ostküste gehören (einschließlich Maryland, Pennsylvania, Virginia und Florida), untersucht derzeit, wie man die Änderung einführen könnte.', 'output_7b': ['And the I-95 coalition, which includes the transportation ministries of 17 federal states on the East Coast (including Maryland, Pennsylvania, Virginia, and Florida), is currently investigating how to introduce the changes.'], 'output_tiny': ['The I-95 Coalition, which includes the traffic authorities of 17 states (including Maryland, Pennsylvania, Virginia, and Florida) is currently studying how to implement the change.'], 'output_13b': ['And the I-95 Coalition, which includes the transportation ministries of 17 eastern states (including Maryland, Pennsylvania, Virginia, and Florida), is currently investigating how the change could be introduced.']}, {'input': 'Das Konzept ist kein universeller Hit.', 'output_7b': ['The concept is not a universal hit.'], 'output_tiny': ['The concept is not a universal hit.'], 'output_13b': ['The concept is not a universal hit.']}, {'input': 'In Nevada, wo vor kurzem 50 Freiwillige mit den Geräten ausgestattet wurden, waren Autofahrer skeptisch beim Gedanken, die Regierung könnte jede ihrer Bewegungen verfolgen.', 'output_7b': [\"In Nevada, where just recently 50 volunteers were equipped with devices, drivers were skeptical about the government's ability to track their every move.\"], 'output_tiny': [\"In Nevada, where 50 volunteers were equipped with the devices, car drivers were skeptical about the government's ability to track their movements.\"], 'output_13b': ['In Nevada, where only recently 50 volunteers were equipped with devices, drivers were skeptical of the idea that the government could track their every move.']}, {'input': '„Bedenken gegen Big Brother und derartige Dinge waren ein großes Problem“, erklärt Alauddin Khan, Leiter des Strategie- und Ergebnismanagements im Verkehrsministerium von Nevada.', 'output_7b': ['Bedenken against Big Brother and such things were a big problem, explains Alauddin Khan, head of strategy and result management in the Nevada Department of Transportation.'], 'output_tiny': ['“Observing Big Brother and suchlike things were a major problem”, explains Alauddin Khan, the Head of Strategy and Results Management in the Transport Ministry of Nevada.'], 'output_13b': ['\"Concerns about Big Brother and such things were a big problem\", explains Alauddin Khan, head of strategy and results management at the Nevada Transportation Ministry.']}, {'input': 'Die Leute wollten es nicht.', 'output_7b': ['The people did not want it.'], 'output_tiny': ['The people did not want it.'], 'output_13b': ['The people did not want it.']}, {'input': 'Als der Test anlief, warnte die ACLU von Nevada auf ihrer Website: „Es wäre relativ einfach, die Fahrtenschreiber in ausgewachsene Überwachungsgeräte zu verwandeln.“', 'output_7b': ['As the test approached, the ACLU of Nevada warned on their website: \"It would be relatively easy to turn speed cameras into full-fledged surveillance devices.'], 'output_tiny': ['As the test was conducted, the ACLU warned of Nevada on their website: “It would be relatively easy to convert the driving school instructors into adult monitoring devices.'], 'output_13b': ['As the test began, the ACLU of Nevada warned on their website: \"It would be relatively easy to convert the speed recorders into sophisticated surveillance devices.']}, {'input': 'Es bestehe keine Notwendigkeit, eine gigantische, sperrige technologische Infrastruktur aufzubauen, die unweigerlich dazu verwendet werden würde, Daten über die täglichen Bewegungen von Einzelpersonen zu erfassen.', 'output_7b': ['There is no need to build a gigantic, cumbersome technological infrastructure that would inevitably be used to track the daily movements of individual people.'], 'output_tiny': ['There is no need for a gigantic, impenetrable technological infrastructure that inevitably leads to data collection on daily movements of individuals.'], 'output_13b': [\"There is no need to build a massive, clunky technological infrastructure that would inevitably be used to collect data on individuals' daily movements.\"]}, {'input': 'Nevada gehört zu einer Reihe von Bundesstaaten, die nun nach erschwinglicher Technologie Ausschau halten, mit der der Staat die gefahrenen Kilometer erfassen kann, aber nicht genau wann und wo.', 'output_7b': ['Nevada belongs to a series of states that are now looking for affordable technology to determine how many dangerous miles the state has traveled, but not exactly when and where.'], 'output_tiny': ['Nevada is part of a series of states that are now looking for ways to reduce fuel consumption, with the state looking to capture the kilometers that have been damaged, but not when and where.'], 'output_13b': ['Nevada is one of several states that are now looking for affordable technology to track the number of miles driven by the state, but not exactly when and where.']}, {'input': 'Damit, so Khan, wäre auch die Öffentlichkeit beruhigter.', 'output_7b': ['With that, so Khan, the public would also be reassured.'], 'output_tiny': ['In order to calm the public, Khan would be more effective.'], 'output_13b': ['So, if Khan were to be calmer, the public would also be calmer.']}, {'input': 'Die Jagd nach dieser Technologie hat einige Behörden zu einem kleinen Startup-Unternehmen namens True Mileage in Kalifornien geführt.', 'output_7b': ['The hunt for this technology has led some authorities to a small startup company called True Mileage in California.'], 'output_tiny': ['The hunt for this technology has led some government agencies to a small startup named True Mileage in California.'], 'output_13b': ['The hunt for this technology has led some authorities to a small startup company named True Mileage in California.']}, {'input': 'Die Firma ist ursprünglich nicht angetreten, um Bundesstaaten bei der Besteuerung von Autofahrern zu helfen.', 'output_7b': ['The company was not originally intended to help states with the taxation of car drivers.'], 'output_tiny': ['The company was not originally invited to participate in the federal taxation of drivers in states.'], 'output_13b': ['The company was not initially established to help federal states with the taxation of car drivers.']}, {'input': 'Vielmehr war es ihr Ziel, in einem aufstrebenden Markt für Kfz-Versicherungen Fuß zu fassen, bei denen Fahrer auf Grundlage der gefahrenen Meilen zahlen sollen.', 'output_7b': ['Rather, her goal was to establish a foothold in a rapidly growing market for car insurance, where drivers will be charged based on the number of miles driven.'], 'output_tiny': ['In contrast, the goal of the company was to enter a growing market for car insurance, where drivers are charged based on the number of miles driven.'], 'output_13b': ['Rather, her goal was to establish a foothold in the rapidly growing market for car insurance, where drivers pay based on the number of miles driven.']}, {'input': 'Doch die von ihr getesteten Geräte sind auch für die Straßenplaner interessant, denn sie arbeiten nicht mit GPS und liefern nur begrenzte Informationen, die regelmäßig per Modem hochgeladen werden.', 'output_7b': ['However, the devices tested by her are also interesting for traffic planners, as they do not use GPS and provide only limited information that is regularly uploaded via modem.'], 'output_tiny': ['However, the devices tested by her are also interesting for street planners, as they do not use GPS and provide limited information, which is updated via modem every day.'], 'output_13b': ['However, the devices tested by her are also interesting for street planners, because they do not use GPS and only provide limited information that is regularly uploaded via modem.']}, {'input': '„Die Leute sind eher bereit, sich daran zu beteiligen, wenn ihre Geschwindigkeit und Standorte nicht aufgezeichnet werden“, erklärte Ryan Morrison, Geschäftsführer von True Mileage.', 'output_7b': ['People are generally more willing to participate if their speed and locations are not recorded, explained Ryan Morrison, CEO of True Mileage.'], 'output_tiny': ['The people are more willing to participate when their speed and locations are not recorded, according to Ryan Morrison, CEO of True Mileage.'], 'output_13b': ['(The people are more willing to participate if their speed and locations are not recorded), explained Ryan Morrison, CEO of True Mileage.']}, {'input': 'In einigen dieser öffentlichen Pilotprogramme wurden große Fehler gemacht.', 'output_7b': ['In some of these public pilot programs, significant mistakes were made.'], 'output_tiny': ['In some public pilot programs, errors were made.'], 'output_13b': ['In some of these public pilot programs, big mistakes were made.']}, {'input': 'Es gibt wesentlich billigere und weniger intrusive Möglichkeiten, dies umzusetzen.', 'output_7b': ['There are significantly cheaper and less intrusive ways to do this.'], 'output_tiny': ['There are significantly cheaper and less intrusive ways to transform.'], 'output_13b': ['There are much cheaper and less intrusive ways to implement this.']}, {'input': 'In Oregon experimentieren die Planer damit, Autofahrern eine Reihe von Auswahlmöglichkeiten zu geben.', 'output_7b': ['In Oregon, the planners are experimenting with giving drivers a series of options.'], 'output_tiny': ['In Oregon, the designers are experimenting with the possibility of offering a series of selection options to car drivers.'], 'output_13b': ['In Oregon, planners are experimenting with giving drivers a range of selection options.']}, {'input': 'Sie können sich für ein Gerät mit oder ohne GPS entscheiden.', 'output_7b': ['You can decide whether to choose a device with or without GPS.'], 'output_tiny': ['You can decide whether to purchase a device with or without GPS.'], 'output_13b': ['You can decide on a device with or without GPS.']}, {'input': 'Oder sie wählen überhaupt kein Gerät und zahlen stattdessen eine Pauschalgebühr auf Grundlage der durchschnittlich von allen Einwohnern des Bundesstaates gefahrenen Meilen.', 'output_7b': ['Or they choose not to use any device at all and pay a flat fee based on the average distance driven by all residents of the state.'], 'output_tiny': ['Or they choose not to use any device and pay a pauschal fee based on the average distance traveled by all residents of the state.'], 'output_13b': ['Or they choose no device at all and pay a flat fee based on the average miles driven by all residents of the state.']}, {'input': 'Andere Stellen hoffen, das Konzept einer misstrauischen Öffentlichkeit verkaufen zu können, indem sie die Geräte mit mehr Funktionen ausstatten als mit wenigeren.', 'output_7b': ['Others hope to sell the concept of a skeptical public by equipping the devices with more functions than with fewer.'], 'output_tiny': ['Other places hope that the concept of a misguided public can be sold by selling more devices with more features than with fewer.'], 'output_13b': ['Others hope to sell the concept of a suspicious public by adding more features to the devices than they have with fewer.']}, {'input': 'In New York City wollen Verkehrsbeamte ein Gerät zur Besteuerung entwickeln, mit dem sich auch Parkgebühren bezahlen lassen, eine Versicherung nur für gefahrene Kilometer bezahlt werden muss und Geschwindigkeitsdaten von anderen Fahrzeugen in Echtzeit erhoben werden, dank derer Autofahrer Staus ausweichen können.', 'output_7b': ['In New York City traffic officials want to develop a device to tax drivers, with which they can also pay parking fees, only have to pay insurance for driven kilometers, and receive real-time data on the speed of other vehicles, allowing drivers to avoid traffic jams.'], 'output_tiny': ['In New York City, traffic officials plan to develop a device for billing, which only charges for driving kilometers, insurance only for accidents must be paid, and speed data from other vehicles in real-time can be obtained, allowing drivers to avoid congestion and accidents.'], 'output_13b': ['In New York City, traffic officials are developing a device to monitor and pay for parking fees, insurance only for the actual distance driven, and real-time speed data from other vehicles, allowing drivers to avoid traffic jams.']}, {'input': '„Autofahrer würden durch den Mehrwert der Vorteile, die das System bietet, zur Teilnahme motiviert“, heißt es in einem Planungsdokument der Stadt.', 'output_7b': ['Autofahrer would be motivated to participate through the added value of the advantages offered by the system, according to a planning document from the city.'], 'output_tiny': ['Autofahrer would benefit from the added value of the system, which offers them participation, according to a planning document by the city.'], 'output_13b': ['\"Drivers would be motivated to participate by the added value of the benefits that the system offers\", the document states.']}, {'input': 'Einige Verkehrsplaner fragen sich allerdings, ob das ganze Gerede über das Bezahlen pro Meile nicht nur ein riesiges Ablenkungsmanöver sei.', 'output_7b': ['Some traffic planners are wondering however, whether all this talk about paying per mile is not just a massive distraction.'], 'output_tiny': ['Some traffic planners wonder whether the whole thing about paying per mile is not just a massive traffic manipulation.'], 'output_13b': ['Some transportation planners are questioning whether the entire discussion about paying per mile is not just a huge distraction tactic.']}]\n"
     ]
    }
   ],
   "source": [
    "print(input_output_pairs[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfec28a0-b890-43b1-95f3-d4ee59d07d91",
   "metadata": {},
   "source": [
    "### **CNN_Dailymail (Summarization)**\n",
    "is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a960eee2-cbf1-40b4-aa5f-3e3a3a3a641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cnn_dailymail_dataset = load_dataset('abisee/cnn_dailymail', '2.0.0', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "af892f96-b5de-4184-af58-91629346c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = cnn_dailymail_dataset[100]['article'] \n",
    "input_prompt = \"Summarize the following text in under 50 words: \\n\\n\" + input_text + \"\\n\\n Write the summary here: \"\n",
    "\n",
    "inputs = tinyllama_tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "de4ed3f2-2296-4027-8a2e-b21a08a8e0e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Summarize the following text in under 50 words: \n",
      "\n",
      "(CNN)Anthony Ray Hinton is thankful to be free after nearly 30 years on Alabama's death row for murders he says he didn't commit. And incredulous that it took so long. Hinton, 58, looked up, took in the sunshine and thanked God and his lawyers Friday morning outside the county jail in Birmingham, minutes after taking his first steps as a free man since 1985. He spoke of unjustly losing three decades of his life, under fear of execution, for something he didn't do. \"All they had to do was to test the gun, but when you think you're high and mighty and you're above the law, you don't have to answer to nobody,\" Hinton told reporters. \"But I've got news for you -- everybody that played a part in sending me to death row, you will answer to God.\" Jefferson County Circuit Court Judge Laura Petro had ordered Hinton released after granting the state's motion to dismiss charges against him. Hinton was convicted of murder in the 1985 deaths of two Birmingham-area, fast-food restaurant managers, John Davidson and Thomas Wayne Vason. But a new trial was ordered in 2014 after firearms experts testified 12 years earlier that the revolver Hinton was said to have used in the crimes could not be matched to evidence in either case, and the two killings couldn't be linked to each other. \"Death Row Stories\": Hard questions about the U.S. capital punishment system . The state then declined to re-prosecute the case. Hinton was 29 at the time of the killings and had always maintained his innocence, said the Equal Justice Initiative, a group that helped win his release. \"Race, poverty, inadequate legal assistance, and prosecutorial indifference to innocence conspired to create a textbook example of injustice,\" Bryan Stevenson, the group's executive director and Hinton's lead attorney, said of his African-American client. \"I can't think of a case that more urgently dramatizes the need for reform than what has happened to Anthony Ray Hinton.\" Stevenson said the \"refusal of state prosecutors to re-examine this case despite persuasive and reliable evidence of innocence is disappointing and troubling.\" Amnesty report: Executions down but death sentences on the rise . Dressed in a dark suit and blue shirt, Hinton praised God for his release, saying he was sent \"not just a lawyer, but the best lawyers.\" He said he will continue to pray for the families of the murder victims. Both he and those families have suffered a miscarriage of justice, he said. \"For all of us that say that we believe in justice, this is the case to start showing, because I shouldn't have (sat) on death row for 30 years,\" he said. Woman who spent 22 years on death row has case tossed . Hinton was accompanied Friday by two of his sisters, one of whom still lives in the Birmingham area. Other siblings will fly to the area to see him soon, Stevenson said. His mother, with whom he lived at the time of his arrest, is no longer living, according to the lawyer. Hinton planned to spend at least this weekend at the home of a close friend. He will meet with his attorneys Monday to start planning for his immediate needs, such as obtaining identification and getting a health checkup, Stevenson said. The plan now is to spend a few weeks to get oriented with freedom and \"sort out what he wants to do,\" Stevenson said.\n",
      "\n",
      " Write the summary here: \n",
      "Output: Summarize the following text in under 50 words: \n",
      "\n",
      "(CNN)Anthony Ray Hinton is thankful to be free after nearly 30 years on Alabama's death row for murders he says he didn't commit. And incredulous that it took so long. Hinton, 58, looked up, took in the sunshine and thanked God and his lawyers Friday morning outside the county jail in Birmingham, minutes after taking his first steps as a free man since 1985. He spoke of unjustly losing three decades of his life, under fear of execution, for something he didn't do. \"All they had to do was to test the gun, but when you think you're high and mighty and you're above the law, you don't have to answer to nobody,\" Hinton told reporters. \"But I've got news for you -- everybody that played a part in sending me to death row, you will answer to God.\" Jefferson County Circuit Court Judge Laura Petro had ordered Hinton released after granting the state's motion to dismiss charges against him. Hinton was convicted of murder in the 1985 deaths of two Birmingham-area, fast-food restaurant managers, John Davidson and Thomas Wayne Vason. But a new trial was ordered in 2014 after firearms experts testified 12 years earlier that the revolver Hinton was said to have used in the crimes could not be matched to evidence in either case, and the two killings couldn't be linked to each other. \"Death Row Stories\": Hard questions about the U.S. capital punishment system . The state then declined to re-prosecute the case. Hinton was 29 at the time of the killings and had always maintained his innocence, said the Equal Justice Initiative, a group that helped win his release. \"Race, poverty, inadequate legal assistance, and prosecutorial indifference to innocence conspired to create a textbook example of injustice,\" Bryan Stevenson, the group's executive director and Hinton's lead attorney, said of his African-American client. \"I can't think of a case that more urgently dramatizes the need for reform than what has happened to Anthony Ray Hinton.\" Stevenson said the \"refusal of state prosecutors to re-examine this case despite persuasive and reliable evidence of innocence is disappointing and troubling.\" Amnesty report: Executions down but death sentences on the rise . Dressed in a dark suit and blue shirt, Hinton praised God for his release, saying he was sent \"not just a lawyer, but the best lawyers.\" He said he will continue to pray for the families of the murder victims. Both he and those families have suffered a miscarriage of justice, he said. \"For all of us that say that we believe in justice, this is the case to start showing, because I shouldn't have (sat) on death row for 30 years,\" he said. Woman who spent 22 years on death row has case tossed . Hinton was accompanied Friday by two of his sisters, one of whom still lives in the Birmingham area. Other siblings will fly to the area to see him soon, Stevenson said. His mother, with whom he lived at the time of his arrest, is no longer living, according to the lawyer. Hinton planned to spend at least this weekend at the home of a close friend. He will meet with his attorneys Monday to start planning for his immediate needs, such as obtaining identification and getting a health checkup, Stevenson said. The plan now is to spend a few weeks to get oriented with freedom and \"sort out what he wants to do,\" Stevenson said.\n",
      "\n",
      " Write the summary here: \n",
      "\n",
      "Anthony Ray Hinton, a man who spent nearly 30 years on Alabama's death row for murders he says he didn't commit, is thankful to be free after nearly 30 years on the list. He thanked God and his lawyers Friday morning outside the county jail in Birmingham, minutes after taking his first steps as a free man since 1985. Hinton, 58, looked up, took in\n"
     ]
    }
   ],
   "source": [
    "output = tinyllama.generate(inputs['input_ids'], max_new_tokens=100)\n",
    "\n",
    "output_text = tinyllama_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input: {input_prompt}\")\n",
    "print(f\"Output: {output_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5da15724-547e-4de3-9328-8c0b1058f199",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthony Ray Hinton, a man who spent nearly 30 years on Alabama's death row for murders he says he didn't commit, is thankful to be free after nearly 30 years on the list. He thanked God and his lawyers Friday morning outside the county jail in Birmingham, minutes after taking his first steps as a free man since 1985. Hinton, 58, looked up, took in\n"
     ]
    }
   ],
   "source": [
    "summary_prefix = \"Write the summary here: \"\n",
    "if summary_prefix in output_text:\n",
    "    cleaned_output = output_text.split(summary_prefix)[-1].strip()\n",
    "else:\n",
    "    cleaned_output = output_text.strip()\n",
    "\n",
    "print(cleaned_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "97856407-e2b7-41e8-ad4f-bf0f8aecbe29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points in different token ranges:\n",
      "0-100: 1\n",
      "101-200: 41\n",
      "201-300: 261\n",
      "301-400: 575\n",
      "401-500: 903\n",
      "501-600: 1011\n",
      "601-700: 1030\n",
      "701-800: 1039\n",
      "801-900: 963\n",
      "901-1000: 898\n",
      "1001+: 4768\n",
      "\n",
      "Data point with the most tokens is at index: 603\n",
      "Number of tokens: 3534\n",
      "Input text: (CNN)The nominations for the 69th Annual Tony Awards were announced Tuesday morning. Past Tony winner and three-time nominee Mary-Louise Parker unveiled the nominees with Bruce Willis, who is set to make his Broadway debut in the upcoming play \"Misery.\" The awards are set to be handed out June 7 in a ceremony airing live at 8 p.m. on CBS (tape-delayed on the West Coast) from Radio City Music Hall. Kristin Chenoweth, a nominee for \"On the 20th Century,\" and recent \"Cabaret\" star Alan Cumming are set to host the ceremony. The full list of nominees is below. Best Play . \"The Curious Incident of the Dog in the Night-Time\" Author: Simon Stephens . Producers: Stuart Thompson, Tim Levy for NT America, Warner Bros. Theatre Ventures, Nick Starr & Chris Harper for NT Productions, Bob Boyett, Roger Berlind, Scott M. Delman, Roy Furman, Glass Half Full Productions, Ruth Hendel, Jon B. Platt, Prime Number Group, Scott Rudin, Triple Play Broadway, The Shubert Organization, The National Theatre . \"Disgraced\" Author: Ayad Akhtar . Producers: The Araca Group, Lincoln Center Theater, Jenifer Evans, Amanda Watkins, Richard Winkler, Rodger Hess, Stephanie P. McClelland, Tulchin/Bartner Productions, Jessica Genick, Jonathan Reinis, Carl Levin/Ashley De Simone/TNTDynaMite Productions, Alden Bergson/Rachel Weinstein, Greenleaf Productions, Darren DeVerna/Jere Harris, The Shubert Organization, The David Merrick Arts Foundation . \"Hand to God\" Author: Robert Askins . Producers: Kevin McCollum, Broadway Global Ventures, CMC, Morris Berchard, Mariano V. Tolentino, Jr., Stephanie Kramer, LAMS Productions, DeSimone/Winkler, Joan Raffe & Jhett Tolentino, Timothy Laczynski, Lily Fan, Ayal Miodovnik, Jam Theatricals, Ensemble Studio Theatre, MCC Theater . \"Wolf Hall Parts One & Two\" Co-Authors: Hilary Mantel and Mike Poulton . Producers: Jeffrey Richards, Jerry Frankel, Matthew Byam Shaw, Nia Janis & Nick Salmon for Playfull Productions UK, Carole Shorenstein Hays, Jam Theatricals, Ron Kastner, Kyodo Tokyo, Inc., Tulchin Bartner Productions, WLE MSG, Jane Bergère, Scott M. Delman, Rebecca Gold, Just for Laughs Theatricals, Kit Seidel, Triple Play Productions, Gabrielle Palitz, Georgia Gatti, Jessica Genick, Will Trice, The Shubert Organization, The Royal Shakespeare Company . Best Musical . \"An American in Paris\" Producers: Stuart Oken, Van Kaplan, Roy Furman, Stephanie McClelland, Darren Bagert, Carole L. Haber, James Nederlander, Five Cent Productions, Michael Leavitt, Apples and Oranges Studios/Dominion Pictures, Roger Berlind/Arch Road, Simone Genatt Haft/Marc Routh, Triptyk Studios/Spencer Ross, Ed Walson/Peter May, Adam Zotovich/Celia Atkin, Eugene Beard/Julie Boardman/Kalish-Weinstein, Stuart Ditsky/Jim Herbert/Sandy Robertson, Suzanne Friedman/Independent Presenters Network/Wonderful Productions, The Leonore S. Gershwin 1987 Trust/Jenkins-Taylor/Proctors, Harriet Newman Leve/Jane Dubin/Sarabeth Grossman, Caiola Productions/Jennifer Isaacson/Raise the Curtain, Elephant Eye Theatrical & Pittsburgh CLO, Théâtre du Châtelet . \"Fun Home\" Producers: Fox Theatricals, Barbara Whitman, Carole Shorenstein Hays, Tom Casserly, Paula Marie Black, Latitude Link, Terry Schnuck/Jack Lane, The Forstalls, Nathan Vernon, Mint Theatrical, Elizabeth Armstrong, Jam Theatricals, Delman Whitney, Kristin Caskey & Mike Isaacson, The Public Theater, Oskar Eustis, Patrick Willingham . \"Something Rotten!\" Producers: Kevin McCollum, Broadway Global Ventures, CMC, Mastro/Goodman, Jerry & Ronald Frankel, Morris Berchard, Kyodo Tokyo Inc., Wendy Federman, Barbara Freitag, LAMS Productions, Winkler/DeSimone, Timothy Laczynski, Dan Markley, Harris/Karmazin, Jam Theatricals, Robert Greenblatt, Jujamcyn Theaters . \"The Visit\" Producers: Tom Kirdahy, Edgar Bronfman, Jr., Tom Smedes, Hugh Hayes, Peter Stern, Judith Ann Abrams, Rich Affannato, Hunter Arnold, Carl Daikeler, Ken Davenport, Bharat Mitra & Bhavani Lev, Peter May, Ted Snowdon, Bruno Wang Productions, Taylor Cleghorn, Sandi Moran, Mark Lee & Ed Filipowski, Blodgett Calvin Family, Gabrielle Palitz/Weatherby & Fishman LLC, Marguerite Hoffman/Jeremy Youett, Carlos Arana, Veenerick & Katherine Vos Van Liempt, 42nd.Club/Silva Theatrical, Kate Cannova/Terry Loftis, The Shubert Organization, Williamstown Theatre Festival . Best Revival of a Play . \"The Elephant Man\" Producers: James L. Nederlander, Terry Allen Kramer, Catherine Adler, Roger Berlind, Caiola Productions, Patrick Catullo, Roy Furman, Larry Hirschhorn, Jeffrey Finn Productions, Van Kaplan, Edward M. Kaufmann, Hal Luftig, Arielle Tepper Madover, Peter May, Stephanie P. McClelland, The Shubert Organization, Douglas Smith, Jonathan M. Tisch, WLE MSG, LLC., Scott & Brian Zeilinger, Williamstown Theatre Festival . \"Skylight\" Producers: Robert Fox, Scott Rudin, Eli Bush, Roger Berlind, William Berlind, Roy Furman, Jon B. Platt, The Shubert Organization, Stephanie P. McClelland, Catherine Adler, Jay Alix & Una Jackman, Scott M. Delman, Heni Koenigsberg, Spring Sirkin, Stuart Thompson, True Love Productions, The Araca Group, Carlos Arana, David Mirvish, Joey Parnes, Sue Wagner, John Johnson . \"This Is Our Youth\" Producers: Scott Rudin, Eli Bush, Roger Berlind, William Berlind, Jon B. Platt, Roy Furman, The Shubert Organization, Ruth Hendel, Scott M. Delman, Stephanie P. McClelland, Sonia Friedman, Tulchin Bartner, The Araca Group, Heni Koenigsberg, Daryl Roth, Joan Raffe & Jhett Tolentino, Catherine & Fred Adler, Joey Parnes, Sue Wagner, John Johnson, Steppenwolf Theatre Company . \"You Can't Take It with You\" Producers: Jeffrey Richards, Jerry Frankel, Jam Theatricals, Dominion Pictures, Gutterman & Winkler, Daryl Roth, Terry Schnuck, Jane Bergère, Caiola Productions, Rebecca Gold, Laruffa & Hinderliter, Larry Magid, Gabrielle Palitz, Spisto & Kierstead, SunnySpot Productions, Venuworks Theatricals, Jessica Genick, Will Trice, Roundabout Theatre Company, Todd Haimes, Harold Wolpert, Julia C. Levy, Sydney Beers . Best Revival of a Musical . \"The King and I\" Producers: Lincoln Center Theater, André Bishop, Adam Siegel, Hattie K. Jutagir, Ambassador Theatre Group . \"On the Town\" Producers: Howard and Janet Kagan, Severn Partners Entertainment, Bruce Robert Harris and Jack W. Batman, Paula Marie Black, Nigel Lythgoe, Michael J. Moritz,, Jr., Mahoney/Alden/Badway, Ambassador Theatre Group, Margie and Bryan Weingarten, Kim Schall, Michael Rubenstein, Terry/Louise/Chris Lingner, Brunish & Trinchero, Stephanie Rosenberg, Laruffa & Hinderliter, Rubinstein/Handleman, Lizbeth Bintz, Riki Kane Larimer, 24 Hour Adventure Production, A&A Gordon, Matt Ross/Ben Feldman/Pamela Cooper, Barrington Stage Company . \"On the Twentieth Century\" Producers: Roundabout Theatre Company, Todd Haimes, Harold Wolpert, Julia C. Levy, Sydney Beers . Best Book of a Musical . \"An American in Paris,\" Craig Lucas . \"Fun Home,\" Lisa Kron . \"Something Rotten!\" Karey Kirkpatrick and John O'Farrell . \"The Visit,\" Terrence McNally . Best Original Score (Music and/or Lyrics) Written for the Theatre . \"Fun Home\" Music: Jeanine Tesori . Lyrics: Lisa Kron . \"The Last Ship\" Music & Lyrics: Sting . \"Something Rotten!\" Music & Lyrics: Wayne Kirkpatrick and Karey Kirkpatrick . \"The Visit\" Music: John Kander . Lyrics: Fred Ebb . Best Performance by an Actor in a Leading Role in a Play . Steven Boyer, \"Hand to God\" Bradley Cooper, \"The Elephant Man\" Ben Miles, \"Wolf Hall Parts One & Two\" Bill Nighy, \"Skylight\" Alex Sharp, \"The Curious Incident of the Dog in the Night-Time\" Best Performance by an Actress in a Leading Role in a Play . Geneva Carr, \"Hand to God\" Helen Mirren, \"The Audience\" Elisabeth Moss, \"The Heidi Chronicles\" Carey Mulligan, \"Skylight\" Ruth Wilson, \"Constellations\" Best Performance by an Actor in a Leading Role in a Musical . Michael Cerveris, \"Fun Home\" Robert Fairchild, \"An American in Paris\" Brian d'Arcy James, \"Something Rotten!\" Ken Watanabe, \"The King and I\" Tony Yazbeck, \"On the Town\" Best Performance by an Actress in a Leading Role in a Musical . Kristin Chenoweth, \"On the Twentieth Century\" Leanne Cope, \"An American in Paris\" Beth Malone, \"Fun Home\" Kelli O'Hara, \"The King and I\" Chita Rivera, \"The Visit\" Best Performance by an Actor in a Featured Role in a Play . Matthew Beard, \"Skylight\" K. Todd Freeman, \"Airline Highway\" Richard McCabe, \"The Audience\" Alessandro Nivola, \"The Elephant Man\" Nathaniel Parker, \"Wolf Hall Parts One & Two\" Micah Stock, \"It's Only a Play\" Best Performance by an Actress in a Featured Role in a Play . Annaleigh Ashford, \"You Can't Take It with You\" Patricia Clarkson, \"The Elephant Man\" Lydia Leonard, \"Wolf Hall Parts One & Two\" Sarah Stiles, \"Hand to God\" Julie White, \"Airline Highway\" Best Performance by an Actor in a Featured Role in a Musical . Christian Borle, \"Something Rotten!\" Andy Karl, \"On the Twentieth Century\" Brad Oscar, \"Something Rotten!\" Brandon Uranowitz, \"An American in Paris\" Max von Essen, \"An American in Paris\" Best Performance by an Actress in a Featured Role in a Musical . Victoria Clark, \"Gigi\" Judy Kuhn, \"Fun Home\" Sydney Lucas, \"Fun Home\" Ruthie Ann Miles, \"The King and I\" Emily Skeggs, \"Fun Home\" Best Scenic Design of a Play . Bunny Christie and Finn Ross, \"The Curious Incident of the Dog in the Night-Time\" Bob Crowley, \"Skylight\" Christopher Oram, \"Wolf Hall Parts One & Two\" David Rockwell, \"You Can't Take It with You\" Best Scenic Design of a Musical . Bob Crowley and 59 Productions, \"An American in Paris\" David Rockwell, \"On the Twentieth Century\" Michael Yeargan, \"The King and I\" David Zinn, \"Fun Home\" Best Costume Design of a Play . Bob Crowley, \"The Audience\" Jane Greenwood, \"You Can't Take It with You\" Christopher Oram, \"Wolf Hall Parts One & Two\" David Zinn, \"Airline Highway\" Best Costume Design of a Musical . Gregg Barnes, \"Something Rotten!\" Bob Crowley, \"An American in Paris\" William Ivey Long, \"On the Twentieth Century\" Catherine Zuber, \"The King and I\" Best Lighting Design of a Play . Paule Constable, \"The Curious Incident of the Dog in the Night-Time\" Paule Constable and David Plater, \"Wolf Hall Parts One & Two\" Natasha Katz, \"Skylight\" Japhy Weideman, \"Airline Highway\" Best Lighting Design of a Musical . Donald Holder, \"The King and I\" Natasha Katz, \"An American in Paris\" Ben Stanton, \"Fun Home\" Japhy Weideman, \"The Visit\" Best Direction of a Play . Stephen Daldry, \"Skylight\" Marianne Elliott, \"The Curious Incident of the Dog in the Night-Time\" Scott Ellis, \"You Can't Take It with You\" Jeremy Herrin, \"Wolf Hall Parts One & Two\" Moritz von Stuelpnagel, \"Hand to God\" Best Direction of a Musical . Sam Gold, \"Fun Home\" Casey Nicholaw, \"Something Rotten!\" John Rando, \"On the Town\" Bartlett Sher, \"The King and I\" Christopher Wheeldon, \"An American in Paris\" Best Choreography . Joshua Bergasse, \"On the Town\" Christopher Gattelli, \"The King and I\" Scott Graham & Steven Hoggett for Frantic Assembly, \"The Curious Incident of the Dog . in the Night-Time\" Casey Nicholaw, \"Something Rotten!\" Christopher Wheeldon, \"An American in Paris\" Best Orchestrations . Christopher Austin, Don Sebesky, Bill Elliott, \"An American in Paris\" John Clancy, \"Fun Home\" Larry Hochman, \"Something Rotten!\" Rob Mathes, \"The Last Ship\" Recipients of Awards and Honors in Non-competitive Categories . Special Tony Award for Lifetime Achievement in the Theatre . Tommy Tune . Special Tony Award . John Cameron Mitchell . Regional Theatre Tony Award . Cleveland Play House, Cleveland, Ohio . Isabelle Stevenson Tony Award . Stephen Schwartz . Tony Honors for Excellence in the Theatre . Arnold Abramson . Adrian Bryan-Brown . Gene O'Donovan . ©2015 The Hollywood Reporter. All rights reserved.\n"
     ]
    }
   ],
   "source": [
    "token_ranges = {\n",
    "    '0-100': 0,\n",
    "    '101-200': 0,\n",
    "    '201-300': 0,\n",
    "    '301-400': 0,\n",
    "    '401-500': 0,\n",
    "    '501-600': 0,\n",
    "    '601-700': 0,\n",
    "    '701-800': 0,\n",
    "    '801-900': 0,\n",
    "    '901-1000': 0,\n",
    "    '1001+': 0\n",
    "}\n",
    "\n",
    "max_tokens = -1\n",
    "max_tokens_idx = -1\n",
    "\n",
    "for idx, data in enumerate(cnn_dailymail_dataset):\n",
    "    input_text = data['article']\n",
    "    tokens = llama7b_tokenizer(input_text, return_tensors=\"pt\")\n",
    "    num_tokens = len(tokens['input_ids'][0])\n",
    "    \n",
    "    if num_tokens > max_tokens:\n",
    "        max_tokens = num_tokens\n",
    "        max_tokens_idx = idx\n",
    "    \n",
    "    if num_tokens <= 100:\n",
    "        token_ranges['0-100'] += 1\n",
    "    elif num_tokens <= 200:\n",
    "        token_ranges['101-200'] += 1\n",
    "    elif num_tokens <= 300:\n",
    "        token_ranges['201-300'] += 1\n",
    "    elif num_tokens <= 400:\n",
    "        token_ranges['301-400'] += 1\n",
    "    elif num_tokens <= 500:\n",
    "        token_ranges['401-500'] += 1\n",
    "    elif num_tokens <= 600:\n",
    "        token_ranges['501-600'] += 1\n",
    "    elif num_tokens <= 700:\n",
    "        token_ranges['601-700'] += 1\n",
    "    elif num_tokens <= 800:\n",
    "        token_ranges['701-800'] += 1\n",
    "    elif num_tokens <= 900:\n",
    "        token_ranges['801-900'] += 1\n",
    "    elif num_tokens <= 1000:\n",
    "        token_ranges['901-1000'] += 1\n",
    "    else:\n",
    "        token_ranges['1001+'] += 1\n",
    "\n",
    "print(\"Number of data points in different token ranges:\")\n",
    "for key, value in token_ranges.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(f\"\\nData point with the most tokens is at index: {max_tokens_idx}\")\n",
    "print(f\"Number of tokens: {max_tokens}\")\n",
    "print(f\"Input text: {cnn_dailymail_dataset[max_tokens_idx]['article']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ad52c0d-185d-4290-8500-7eea3211b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(model, tokenizer, dataset, current_idx):\n",
    "    outputs = []\n",
    "    \n",
    "    input_text = cnn_dailymail_dataset[current_idx]['article'] \n",
    "    input_prompt = \"Summarize the following text in under 50 words: \\n\\n\" + input_text + \"\\n\\n Write the summary here: \"\n",
    "    \n",
    "    #inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(\"cuda\")\n",
    "    #output = model.generate(inputs['input_ids'], max_new_tokens=2048)\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "    output = model.generate(inputs['input_ids'], max_new_tokens=100)\n",
    "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    summary_prefix = \"Write the summary here: \"\n",
    "    if summary_prefix in output_text:\n",
    "        cleaned_output = output_text.split(summary_prefix)[-1].strip()\n",
    "    else:\n",
    "        cleaned_output = output_text.strip()\n",
    "\n",
    "    outputs.append(cleaned_output)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91b1b1fc-7320-4272-8530-41bab7fdd2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "outputs_7b = []\n",
    "outputs_tiny = []\n",
    "outputs_13b = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2b98e1d-f1e5-4da4-bcbb-ab65e1342c5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-7b | CURRENT IDX: 987 | Length: 988\n",
      "Llama-7b | CURRENT IDX: 988 | Length: 989\n",
      "Llama-7b | CURRENT IDX: 989 | Length: 990\n",
      "Llama-7b | CURRENT IDX: 990 | Length: 991\n",
      "Llama-7b | CURRENT IDX: 991 | Length: 992\n",
      "Llama-7b | CURRENT IDX: 992 | Length: 993\n",
      "Llama-7b | CURRENT IDX: 993 | Length: 994\n",
      "Llama-7b | CURRENT IDX: 994 | Length: 995\n",
      "Llama-7b | CURRENT IDX: 995 | Length: 996\n",
      "Llama-7b | CURRENT IDX: 996 | Length: 997\n",
      "Llama-7b | CURRENT IDX: 997 | Length: 998\n",
      "Llama-7b | CURRENT IDX: 998 | Length: 999\n",
      "Llama-7b | CURRENT IDX: 999 | Length: 1000\n"
     ]
    }
   ],
   "source": [
    "# for current_idx in range(0, 1000):\n",
    "#     input_text = cnn_dailymail_dataset[current_idx]['article']\n",
    "#     output_7b = generate_output(llama7b, llama7b_tokenizer, input_text, current_idx)\n",
    "\n",
    "#     outputs_7b.append(output_7b)\n",
    "    \n",
    "#     print(f\"Llama-7b | CURRENT IDX: {current_idx} | Length: {len(outputs_7b)}\")\n",
    "#     with open('input_output_pairs_cnn_dailymail_7b', 'wb') as f:\n",
    "#         pickle.dump(outputs_7b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28e82638-691d-4469-86e9-c21d46de34b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "with open('input_output_pairs_cnn_dailymail_7b', 'rb') as f:\n",
    "    outputs_7b = pickle.load(f)\n",
    "\n",
    "print(len(outputs_7b))\n",
    "# print(outputs_7b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "676778c2-e538-46be-b321-2730abc759bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyLlama | CURRENT IDX: 0 | Length: 1\n",
      "TinyLlama | CURRENT IDX: 1 | Length: 2\n",
      "TinyLlama | CURRENT IDX: 2 | Length: 3\n",
      "TinyLlama | CURRENT IDX: 3 | Length: 4\n",
      "TinyLlama | CURRENT IDX: 4 | Length: 5\n",
      "TinyLlama | CURRENT IDX: 5 | Length: 6\n",
      "TinyLlama | CURRENT IDX: 6 | Length: 7\n",
      "TinyLlama | CURRENT IDX: 7 | Length: 8\n",
      "TinyLlama | CURRENT IDX: 8 | Length: 9\n",
      "TinyLlama | CURRENT IDX: 9 | Length: 10\n",
      "TinyLlama | CURRENT IDX: 10 | Length: 11\n",
      "TinyLlama | CURRENT IDX: 11 | Length: 12\n",
      "TinyLlama | CURRENT IDX: 12 | Length: 13\n",
      "TinyLlama | CURRENT IDX: 13 | Length: 14\n",
      "TinyLlama | CURRENT IDX: 14 | Length: 15\n",
      "TinyLlama | CURRENT IDX: 15 | Length: 16\n",
      "TinyLlama | CURRENT IDX: 16 | Length: 17\n",
      "TinyLlama | CURRENT IDX: 17 | Length: 18\n",
      "TinyLlama | CURRENT IDX: 18 | Length: 19\n",
      "TinyLlama | CURRENT IDX: 19 | Length: 20\n",
      "TinyLlama | CURRENT IDX: 20 | Length: 21\n",
      "TinyLlama | CURRENT IDX: 21 | Length: 22\n",
      "TinyLlama | CURRENT IDX: 22 | Length: 23\n",
      "TinyLlama | CURRENT IDX: 23 | Length: 24\n",
      "TinyLlama | CURRENT IDX: 24 | Length: 25\n",
      "TinyLlama | CURRENT IDX: 25 | Length: 26\n",
      "TinyLlama | CURRENT IDX: 26 | Length: 27\n",
      "TinyLlama | CURRENT IDX: 27 | Length: 28\n",
      "TinyLlama | CURRENT IDX: 28 | Length: 29\n",
      "TinyLlama | CURRENT IDX: 29 | Length: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyLlama | CURRENT IDX: 30 | Length: 31\n",
      "TinyLlama | CURRENT IDX: 31 | Length: 32\n",
      "TinyLlama | CURRENT IDX: 32 | Length: 33\n",
      "TinyLlama | CURRENT IDX: 33 | Length: 34\n",
      "TinyLlama | CURRENT IDX: 34 | Length: 35\n",
      "TinyLlama | CURRENT IDX: 35 | Length: 36\n",
      "TinyLlama | CURRENT IDX: 36 | Length: 37\n",
      "TinyLlama | CURRENT IDX: 37 | Length: 38\n",
      "TinyLlama | CURRENT IDX: 38 | Length: 39\n",
      "TinyLlama | CURRENT IDX: 39 | Length: 40\n",
      "TinyLlama | CURRENT IDX: 40 | Length: 41\n",
      "TinyLlama | CURRENT IDX: 41 | Length: 42\n",
      "TinyLlama | CURRENT IDX: 42 | Length: 43\n",
      "TinyLlama | CURRENT IDX: 43 | Length: 44\n",
      "TinyLlama | CURRENT IDX: 44 | Length: 45\n",
      "TinyLlama | CURRENT IDX: 45 | Length: 46\n",
      "TinyLlama | CURRENT IDX: 46 | Length: 47\n",
      "TinyLlama | CURRENT IDX: 47 | Length: 48\n",
      "TinyLlama | CURRENT IDX: 48 | Length: 49\n",
      "TinyLlama | CURRENT IDX: 49 | Length: 50\n",
      "TinyLlama | CURRENT IDX: 50 | Length: 51\n",
      "TinyLlama | CURRENT IDX: 51 | Length: 52\n",
      "TinyLlama | CURRENT IDX: 52 | Length: 53\n",
      "TinyLlama | CURRENT IDX: 53 | Length: 54\n",
      "TinyLlama | CURRENT IDX: 54 | Length: 55\n",
      "TinyLlama | CURRENT IDX: 55 | Length: 56\n",
      "TinyLlama | CURRENT IDX: 56 | Length: 57\n",
      "TinyLlama | CURRENT IDX: 57 | Length: 58\n",
      "TinyLlama | CURRENT IDX: 58 | Length: 59\n",
      "TinyLlama | CURRENT IDX: 59 | Length: 60\n",
      "TinyLlama | CURRENT IDX: 60 | Length: 61\n",
      "TinyLlama | CURRENT IDX: 61 | Length: 62\n",
      "TinyLlama | CURRENT IDX: 62 | Length: 63\n",
      "TinyLlama | CURRENT IDX: 63 | Length: 64\n",
      "TinyLlama | CURRENT IDX: 64 | Length: 65\n",
      "TinyLlama | CURRENT IDX: 65 | Length: 66\n",
      "TinyLlama | CURRENT IDX: 66 | Length: 67\n",
      "TinyLlama | CURRENT IDX: 67 | Length: 68\n",
      "TinyLlama | CURRENT IDX: 68 | Length: 69\n",
      "TinyLlama | CURRENT IDX: 69 | Length: 70\n",
      "TinyLlama | CURRENT IDX: 70 | Length: 71\n",
      "TinyLlama | CURRENT IDX: 71 | Length: 72\n",
      "TinyLlama | CURRENT IDX: 72 | Length: 73\n",
      "TinyLlama | CURRENT IDX: 73 | Length: 74\n",
      "TinyLlama | CURRENT IDX: 74 | Length: 75\n",
      "TinyLlama | CURRENT IDX: 75 | Length: 76\n",
      "TinyLlama | CURRENT IDX: 76 | Length: 77\n",
      "TinyLlama | CURRENT IDX: 77 | Length: 78\n",
      "TinyLlama | CURRENT IDX: 78 | Length: 79\n",
      "TinyLlama | CURRENT IDX: 79 | Length: 80\n",
      "TinyLlama | CURRENT IDX: 80 | Length: 81\n",
      "TinyLlama | CURRENT IDX: 81 | Length: 82\n",
      "TinyLlama | CURRENT IDX: 82 | Length: 83\n",
      "TinyLlama | CURRENT IDX: 83 | Length: 84\n",
      "TinyLlama | CURRENT IDX: 84 | Length: 85\n",
      "TinyLlama | CURRENT IDX: 85 | Length: 86\n",
      "TinyLlama | CURRENT IDX: 86 | Length: 87\n",
      "TinyLlama | CURRENT IDX: 87 | Length: 88\n",
      "TinyLlama | CURRENT IDX: 88 | Length: 89\n",
      "TinyLlama | CURRENT IDX: 89 | Length: 90\n",
      "TinyLlama | CURRENT IDX: 90 | Length: 91\n",
      "TinyLlama | CURRENT IDX: 91 | Length: 92\n",
      "TinyLlama | CURRENT IDX: 92 | Length: 93\n",
      "TinyLlama | CURRENT IDX: 93 | Length: 94\n",
      "TinyLlama | CURRENT IDX: 94 | Length: 95\n",
      "TinyLlama | CURRENT IDX: 95 | Length: 96\n",
      "TinyLlama | CURRENT IDX: 96 | Length: 97\n",
      "TinyLlama | CURRENT IDX: 97 | Length: 98\n",
      "TinyLlama | CURRENT IDX: 98 | Length: 99\n",
      "TinyLlama | CURRENT IDX: 99 | Length: 100\n",
      "TinyLlama | CURRENT IDX: 100 | Length: 101\n",
      "TinyLlama | CURRENT IDX: 101 | Length: 102\n",
      "TinyLlama | CURRENT IDX: 102 | Length: 103\n",
      "TinyLlama | CURRENT IDX: 103 | Length: 104\n",
      "TinyLlama | CURRENT IDX: 104 | Length: 105\n",
      "TinyLlama | CURRENT IDX: 105 | Length: 106\n",
      "TinyLlama | CURRENT IDX: 106 | Length: 107\n",
      "TinyLlama | CURRENT IDX: 107 | Length: 108\n",
      "TinyLlama | CURRENT IDX: 108 | Length: 109\n",
      "TinyLlama | CURRENT IDX: 109 | Length: 110\n",
      "TinyLlama | CURRENT IDX: 110 | Length: 111\n",
      "TinyLlama | CURRENT IDX: 111 | Length: 112\n",
      "TinyLlama | CURRENT IDX: 112 | Length: 113\n",
      "TinyLlama | CURRENT IDX: 113 | Length: 114\n",
      "TinyLlama | CURRENT IDX: 114 | Length: 115\n",
      "TinyLlama | CURRENT IDX: 115 | Length: 116\n",
      "TinyLlama | CURRENT IDX: 116 | Length: 117\n",
      "TinyLlama | CURRENT IDX: 117 | Length: 118\n",
      "TinyLlama | CURRENT IDX: 118 | Length: 119\n",
      "TinyLlama | CURRENT IDX: 119 | Length: 120\n",
      "TinyLlama | CURRENT IDX: 120 | Length: 121\n",
      "TinyLlama | CURRENT IDX: 121 | Length: 122\n",
      "TinyLlama | CURRENT IDX: 122 | Length: 123\n",
      "TinyLlama | CURRENT IDX: 123 | Length: 124\n",
      "TinyLlama | CURRENT IDX: 124 | Length: 125\n",
      "TinyLlama | CURRENT IDX: 125 | Length: 126\n",
      "TinyLlama | CURRENT IDX: 126 | Length: 127\n",
      "TinyLlama | CURRENT IDX: 127 | Length: 128\n",
      "TinyLlama | CURRENT IDX: 128 | Length: 129\n",
      "TinyLlama | CURRENT IDX: 129 | Length: 130\n",
      "TinyLlama | CURRENT IDX: 130 | Length: 131\n",
      "TinyLlama | CURRENT IDX: 131 | Length: 132\n",
      "TinyLlama | CURRENT IDX: 132 | Length: 133\n",
      "TinyLlama | CURRENT IDX: 133 | Length: 134\n",
      "TinyLlama | CURRENT IDX: 134 | Length: 135\n",
      "TinyLlama | CURRENT IDX: 135 | Length: 136\n",
      "TinyLlama | CURRENT IDX: 136 | Length: 137\n",
      "TinyLlama | CURRENT IDX: 137 | Length: 138\n",
      "TinyLlama | CURRENT IDX: 138 | Length: 139\n",
      "TinyLlama | CURRENT IDX: 139 | Length: 140\n",
      "TinyLlama | CURRENT IDX: 140 | Length: 141\n",
      "TinyLlama | CURRENT IDX: 141 | Length: 142\n",
      "TinyLlama | CURRENT IDX: 142 | Length: 143\n",
      "TinyLlama | CURRENT IDX: 143 | Length: 144\n",
      "TinyLlama | CURRENT IDX: 144 | Length: 145\n",
      "TinyLlama | CURRENT IDX: 145 | Length: 146\n",
      "TinyLlama | CURRENT IDX: 146 | Length: 147\n",
      "TinyLlama | CURRENT IDX: 147 | Length: 148\n",
      "TinyLlama | CURRENT IDX: 148 | Length: 149\n",
      "TinyLlama | CURRENT IDX: 149 | Length: 150\n",
      "TinyLlama | CURRENT IDX: 150 | Length: 151\n",
      "TinyLlama | CURRENT IDX: 151 | Length: 152\n",
      "TinyLlama | CURRENT IDX: 152 | Length: 153\n",
      "TinyLlama | CURRENT IDX: 153 | Length: 154\n",
      "TinyLlama | CURRENT IDX: 154 | Length: 155\n",
      "TinyLlama | CURRENT IDX: 155 | Length: 156\n",
      "TinyLlama | CURRENT IDX: 156 | Length: 157\n",
      "TinyLlama | CURRENT IDX: 157 | Length: 158\n",
      "TinyLlama | CURRENT IDX: 158 | Length: 159\n",
      "TinyLlama | CURRENT IDX: 159 | Length: 160\n",
      "TinyLlama | CURRENT IDX: 160 | Length: 161\n",
      "TinyLlama | CURRENT IDX: 161 | Length: 162\n",
      "TinyLlama | CURRENT IDX: 162 | Length: 163\n",
      "TinyLlama | CURRENT IDX: 163 | Length: 164\n",
      "TinyLlama | CURRENT IDX: 164 | Length: 165\n",
      "TinyLlama | CURRENT IDX: 165 | Length: 166\n",
      "TinyLlama | CURRENT IDX: 166 | Length: 167\n",
      "TinyLlama | CURRENT IDX: 167 | Length: 168\n",
      "TinyLlama | CURRENT IDX: 168 | Length: 169\n",
      "TinyLlama | CURRENT IDX: 169 | Length: 170\n",
      "TinyLlama | CURRENT IDX: 170 | Length: 171\n",
      "TinyLlama | CURRENT IDX: 171 | Length: 172\n",
      "TinyLlama | CURRENT IDX: 172 | Length: 173\n",
      "TinyLlama | CURRENT IDX: 173 | Length: 174\n",
      "TinyLlama | CURRENT IDX: 174 | Length: 175\n",
      "TinyLlama | CURRENT IDX: 175 | Length: 176\n",
      "TinyLlama | CURRENT IDX: 176 | Length: 177\n",
      "TinyLlama | CURRENT IDX: 177 | Length: 178\n",
      "TinyLlama | CURRENT IDX: 178 | Length: 179\n",
      "TinyLlama | CURRENT IDX: 179 | Length: 180\n",
      "TinyLlama | CURRENT IDX: 180 | Length: 181\n",
      "TinyLlama | CURRENT IDX: 181 | Length: 182\n",
      "TinyLlama | CURRENT IDX: 182 | Length: 183\n",
      "TinyLlama | CURRENT IDX: 183 | Length: 184\n",
      "TinyLlama | CURRENT IDX: 184 | Length: 185\n",
      "TinyLlama | CURRENT IDX: 185 | Length: 186\n",
      "TinyLlama | CURRENT IDX: 186 | Length: 187\n",
      "TinyLlama | CURRENT IDX: 187 | Length: 188\n",
      "TinyLlama | CURRENT IDX: 188 | Length: 189\n",
      "TinyLlama | CURRENT IDX: 189 | Length: 190\n",
      "TinyLlama | CURRENT IDX: 190 | Length: 191\n",
      "TinyLlama | CURRENT IDX: 191 | Length: 192\n",
      "TinyLlama | CURRENT IDX: 192 | Length: 193\n",
      "TinyLlama | CURRENT IDX: 193 | Length: 194\n",
      "TinyLlama | CURRENT IDX: 194 | Length: 195\n",
      "TinyLlama | CURRENT IDX: 195 | Length: 196\n",
      "TinyLlama | CURRENT IDX: 196 | Length: 197\n",
      "TinyLlama | CURRENT IDX: 197 | Length: 198\n",
      "TinyLlama | CURRENT IDX: 198 | Length: 199\n",
      "TinyLlama | CURRENT IDX: 199 | Length: 200\n",
      "TinyLlama | CURRENT IDX: 200 | Length: 201\n",
      "TinyLlama | CURRENT IDX: 201 | Length: 202\n",
      "TinyLlama | CURRENT IDX: 202 | Length: 203\n",
      "TinyLlama | CURRENT IDX: 203 | Length: 204\n",
      "TinyLlama | CURRENT IDX: 204 | Length: 205\n",
      "TinyLlama | CURRENT IDX: 205 | Length: 206\n",
      "TinyLlama | CURRENT IDX: 206 | Length: 207\n",
      "TinyLlama | CURRENT IDX: 207 | Length: 208\n",
      "TinyLlama | CURRENT IDX: 208 | Length: 209\n",
      "TinyLlama | CURRENT IDX: 209 | Length: 210\n",
      "TinyLlama | CURRENT IDX: 210 | Length: 211\n",
      "TinyLlama | CURRENT IDX: 211 | Length: 212\n",
      "TinyLlama | CURRENT IDX: 212 | Length: 213\n",
      "TinyLlama | CURRENT IDX: 213 | Length: 214\n",
      "TinyLlama | CURRENT IDX: 214 | Length: 215\n",
      "TinyLlama | CURRENT IDX: 215 | Length: 216\n",
      "TinyLlama | CURRENT IDX: 216 | Length: 217\n",
      "TinyLlama | CURRENT IDX: 217 | Length: 218\n",
      "TinyLlama | CURRENT IDX: 218 | Length: 219\n",
      "TinyLlama | CURRENT IDX: 219 | Length: 220\n",
      "TinyLlama | CURRENT IDX: 220 | Length: 221\n",
      "TinyLlama | CURRENT IDX: 221 | Length: 222\n",
      "TinyLlama | CURRENT IDX: 222 | Length: 223\n",
      "TinyLlama | CURRENT IDX: 223 | Length: 224\n",
      "TinyLlama | CURRENT IDX: 224 | Length: 225\n",
      "TinyLlama | CURRENT IDX: 225 | Length: 226\n",
      "TinyLlama | CURRENT IDX: 226 | Length: 227\n",
      "TinyLlama | CURRENT IDX: 227 | Length: 228\n",
      "TinyLlama | CURRENT IDX: 228 | Length: 229\n",
      "TinyLlama | CURRENT IDX: 229 | Length: 230\n",
      "TinyLlama | CURRENT IDX: 230 | Length: 231\n",
      "TinyLlama | CURRENT IDX: 231 | Length: 232\n",
      "TinyLlama | CURRENT IDX: 232 | Length: 233\n",
      "TinyLlama | CURRENT IDX: 233 | Length: 234\n",
      "TinyLlama | CURRENT IDX: 234 | Length: 235\n",
      "TinyLlama | CURRENT IDX: 235 | Length: 236\n",
      "TinyLlama | CURRENT IDX: 236 | Length: 237\n",
      "TinyLlama | CURRENT IDX: 237 | Length: 238\n",
      "TinyLlama | CURRENT IDX: 238 | Length: 239\n",
      "TinyLlama | CURRENT IDX: 239 | Length: 240\n",
      "TinyLlama | CURRENT IDX: 240 | Length: 241\n",
      "TinyLlama | CURRENT IDX: 241 | Length: 242\n",
      "TinyLlama | CURRENT IDX: 242 | Length: 243\n",
      "TinyLlama | CURRENT IDX: 243 | Length: 244\n",
      "TinyLlama | CURRENT IDX: 244 | Length: 245\n",
      "TinyLlama | CURRENT IDX: 245 | Length: 246\n",
      "TinyLlama | CURRENT IDX: 246 | Length: 247\n",
      "TinyLlama | CURRENT IDX: 247 | Length: 248\n",
      "TinyLlama | CURRENT IDX: 248 | Length: 249\n",
      "TinyLlama | CURRENT IDX: 249 | Length: 250\n",
      "TinyLlama | CURRENT IDX: 250 | Length: 251\n",
      "TinyLlama | CURRENT IDX: 251 | Length: 252\n",
      "TinyLlama | CURRENT IDX: 252 | Length: 253\n",
      "TinyLlama | CURRENT IDX: 253 | Length: 254\n",
      "TinyLlama | CURRENT IDX: 254 | Length: 255\n",
      "TinyLlama | CURRENT IDX: 255 | Length: 256\n",
      "TinyLlama | CURRENT IDX: 256 | Length: 257\n",
      "TinyLlama | CURRENT IDX: 257 | Length: 258\n",
      "TinyLlama | CURRENT IDX: 258 | Length: 259\n",
      "TinyLlama | CURRENT IDX: 259 | Length: 260\n",
      "TinyLlama | CURRENT IDX: 260 | Length: 261\n",
      "TinyLlama | CURRENT IDX: 261 | Length: 262\n",
      "TinyLlama | CURRENT IDX: 262 | Length: 263\n",
      "TinyLlama | CURRENT IDX: 263 | Length: 264\n",
      "TinyLlama | CURRENT IDX: 264 | Length: 265\n",
      "TinyLlama | CURRENT IDX: 265 | Length: 266\n",
      "TinyLlama | CURRENT IDX: 266 | Length: 267\n",
      "TinyLlama | CURRENT IDX: 267 | Length: 268\n",
      "TinyLlama | CURRENT IDX: 268 | Length: 269\n",
      "TinyLlama | CURRENT IDX: 269 | Length: 270\n",
      "TinyLlama | CURRENT IDX: 270 | Length: 271\n",
      "TinyLlama | CURRENT IDX: 271 | Length: 272\n",
      "TinyLlama | CURRENT IDX: 272 | Length: 273\n",
      "TinyLlama | CURRENT IDX: 273 | Length: 274\n",
      "TinyLlama | CURRENT IDX: 274 | Length: 275\n",
      "TinyLlama | CURRENT IDX: 275 | Length: 276\n",
      "TinyLlama | CURRENT IDX: 276 | Length: 277\n",
      "TinyLlama | CURRENT IDX: 277 | Length: 278\n",
      "TinyLlama | CURRENT IDX: 278 | Length: 279\n",
      "TinyLlama | CURRENT IDX: 279 | Length: 280\n",
      "TinyLlama | CURRENT IDX: 280 | Length: 281\n",
      "TinyLlama | CURRENT IDX: 281 | Length: 282\n",
      "TinyLlama | CURRENT IDX: 282 | Length: 283\n",
      "TinyLlama | CURRENT IDX: 283 | Length: 284\n",
      "TinyLlama | CURRENT IDX: 284 | Length: 285\n",
      "TinyLlama | CURRENT IDX: 285 | Length: 286\n",
      "TinyLlama | CURRENT IDX: 286 | Length: 287\n",
      "TinyLlama | CURRENT IDX: 287 | Length: 288\n",
      "TinyLlama | CURRENT IDX: 288 | Length: 289\n",
      "TinyLlama | CURRENT IDX: 289 | Length: 290\n",
      "TinyLlama | CURRENT IDX: 290 | Length: 291\n",
      "TinyLlama | CURRENT IDX: 291 | Length: 292\n",
      "TinyLlama | CURRENT IDX: 292 | Length: 293\n",
      "TinyLlama | CURRENT IDX: 293 | Length: 294\n",
      "TinyLlama | CURRENT IDX: 294 | Length: 295\n",
      "TinyLlama | CURRENT IDX: 295 | Length: 296\n",
      "TinyLlama | CURRENT IDX: 296 | Length: 297\n",
      "TinyLlama | CURRENT IDX: 297 | Length: 298\n",
      "TinyLlama | CURRENT IDX: 298 | Length: 299\n",
      "TinyLlama | CURRENT IDX: 299 | Length: 300\n",
      "TinyLlama | CURRENT IDX: 300 | Length: 301\n",
      "TinyLlama | CURRENT IDX: 301 | Length: 302\n",
      "TinyLlama | CURRENT IDX: 302 | Length: 303\n",
      "TinyLlama | CURRENT IDX: 303 | Length: 304\n",
      "TinyLlama | CURRENT IDX: 304 | Length: 305\n",
      "TinyLlama | CURRENT IDX: 305 | Length: 306\n",
      "TinyLlama | CURRENT IDX: 306 | Length: 307\n",
      "TinyLlama | CURRENT IDX: 307 | Length: 308\n",
      "TinyLlama | CURRENT IDX: 308 | Length: 309\n",
      "TinyLlama | CURRENT IDX: 309 | Length: 310\n",
      "TinyLlama | CURRENT IDX: 310 | Length: 311\n",
      "TinyLlama | CURRENT IDX: 311 | Length: 312\n",
      "TinyLlama | CURRENT IDX: 312 | Length: 313\n",
      "TinyLlama | CURRENT IDX: 313 | Length: 314\n",
      "TinyLlama | CURRENT IDX: 314 | Length: 315\n",
      "TinyLlama | CURRENT IDX: 315 | Length: 316\n",
      "TinyLlama | CURRENT IDX: 316 | Length: 317\n",
      "TinyLlama | CURRENT IDX: 317 | Length: 318\n",
      "TinyLlama | CURRENT IDX: 318 | Length: 319\n",
      "TinyLlama | CURRENT IDX: 319 | Length: 320\n",
      "TinyLlama | CURRENT IDX: 320 | Length: 321\n",
      "TinyLlama | CURRENT IDX: 321 | Length: 322\n",
      "TinyLlama | CURRENT IDX: 322 | Length: 323\n",
      "TinyLlama | CURRENT IDX: 323 | Length: 324\n",
      "TinyLlama | CURRENT IDX: 324 | Length: 325\n",
      "TinyLlama | CURRENT IDX: 325 | Length: 326\n",
      "TinyLlama | CURRENT IDX: 326 | Length: 327\n",
      "TinyLlama | CURRENT IDX: 327 | Length: 328\n",
      "TinyLlama | CURRENT IDX: 328 | Length: 329\n",
      "TinyLlama | CURRENT IDX: 329 | Length: 330\n",
      "TinyLlama | CURRENT IDX: 330 | Length: 331\n",
      "TinyLlama | CURRENT IDX: 331 | Length: 332\n",
      "TinyLlama | CURRENT IDX: 332 | Length: 333\n",
      "TinyLlama | CURRENT IDX: 333 | Length: 334\n",
      "TinyLlama | CURRENT IDX: 334 | Length: 335\n",
      "TinyLlama | CURRENT IDX: 335 | Length: 336\n",
      "TinyLlama | CURRENT IDX: 336 | Length: 337\n",
      "TinyLlama | CURRENT IDX: 337 | Length: 338\n",
      "TinyLlama | CURRENT IDX: 338 | Length: 339\n",
      "TinyLlama | CURRENT IDX: 339 | Length: 340\n",
      "TinyLlama | CURRENT IDX: 340 | Length: 341\n",
      "TinyLlama | CURRENT IDX: 341 | Length: 342\n",
      "TinyLlama | CURRENT IDX: 342 | Length: 343\n",
      "TinyLlama | CURRENT IDX: 343 | Length: 344\n",
      "TinyLlama | CURRENT IDX: 344 | Length: 345\n",
      "TinyLlama | CURRENT IDX: 345 | Length: 346\n",
      "TinyLlama | CURRENT IDX: 346 | Length: 347\n",
      "TinyLlama | CURRENT IDX: 347 | Length: 348\n",
      "TinyLlama | CURRENT IDX: 348 | Length: 349\n",
      "TinyLlama | CURRENT IDX: 349 | Length: 350\n",
      "TinyLlama | CURRENT IDX: 350 | Length: 351\n",
      "TinyLlama | CURRENT IDX: 351 | Length: 352\n",
      "TinyLlama | CURRENT IDX: 352 | Length: 353\n",
      "TinyLlama | CURRENT IDX: 353 | Length: 354\n",
      "TinyLlama | CURRENT IDX: 354 | Length: 355\n",
      "TinyLlama | CURRENT IDX: 355 | Length: 356\n",
      "TinyLlama | CURRENT IDX: 356 | Length: 357\n",
      "TinyLlama | CURRENT IDX: 357 | Length: 358\n",
      "TinyLlama | CURRENT IDX: 358 | Length: 359\n",
      "TinyLlama | CURRENT IDX: 359 | Length: 360\n",
      "TinyLlama | CURRENT IDX: 360 | Length: 361\n",
      "TinyLlama | CURRENT IDX: 361 | Length: 362\n",
      "TinyLlama | CURRENT IDX: 362 | Length: 363\n",
      "TinyLlama | CURRENT IDX: 363 | Length: 364\n",
      "TinyLlama | CURRENT IDX: 364 | Length: 365\n",
      "TinyLlama | CURRENT IDX: 365 | Length: 366\n",
      "TinyLlama | CURRENT IDX: 366 | Length: 367\n",
      "TinyLlama | CURRENT IDX: 367 | Length: 368\n",
      "TinyLlama | CURRENT IDX: 368 | Length: 369\n",
      "TinyLlama | CURRENT IDX: 369 | Length: 370\n",
      "TinyLlama | CURRENT IDX: 370 | Length: 371\n",
      "TinyLlama | CURRENT IDX: 371 | Length: 372\n",
      "TinyLlama | CURRENT IDX: 372 | Length: 373\n",
      "TinyLlama | CURRENT IDX: 373 | Length: 374\n",
      "TinyLlama | CURRENT IDX: 374 | Length: 375\n",
      "TinyLlama | CURRENT IDX: 375 | Length: 376\n",
      "TinyLlama | CURRENT IDX: 376 | Length: 377\n",
      "TinyLlama | CURRENT IDX: 377 | Length: 378\n",
      "TinyLlama | CURRENT IDX: 378 | Length: 379\n",
      "TinyLlama | CURRENT IDX: 379 | Length: 380\n",
      "TinyLlama | CURRENT IDX: 380 | Length: 381\n",
      "TinyLlama | CURRENT IDX: 381 | Length: 382\n",
      "TinyLlama | CURRENT IDX: 382 | Length: 383\n",
      "TinyLlama | CURRENT IDX: 383 | Length: 384\n",
      "TinyLlama | CURRENT IDX: 384 | Length: 385\n",
      "TinyLlama | CURRENT IDX: 385 | Length: 386\n",
      "TinyLlama | CURRENT IDX: 386 | Length: 387\n",
      "TinyLlama | CURRENT IDX: 387 | Length: 388\n",
      "TinyLlama | CURRENT IDX: 388 | Length: 389\n",
      "TinyLlama | CURRENT IDX: 389 | Length: 390\n",
      "TinyLlama | CURRENT IDX: 390 | Length: 391\n",
      "TinyLlama | CURRENT IDX: 391 | Length: 392\n",
      "TinyLlama | CURRENT IDX: 392 | Length: 393\n",
      "TinyLlama | CURRENT IDX: 393 | Length: 394\n",
      "TinyLlama | CURRENT IDX: 394 | Length: 395\n",
      "TinyLlama | CURRENT IDX: 395 | Length: 396\n",
      "TinyLlama | CURRENT IDX: 396 | Length: 397\n",
      "TinyLlama | CURRENT IDX: 397 | Length: 398\n",
      "TinyLlama | CURRENT IDX: 398 | Length: 399\n",
      "TinyLlama | CURRENT IDX: 399 | Length: 400\n",
      "TinyLlama | CURRENT IDX: 400 | Length: 401\n",
      "TinyLlama | CURRENT IDX: 401 | Length: 402\n",
      "TinyLlama | CURRENT IDX: 402 | Length: 403\n",
      "TinyLlama | CURRENT IDX: 403 | Length: 404\n",
      "TinyLlama | CURRENT IDX: 404 | Length: 405\n",
      "TinyLlama | CURRENT IDX: 405 | Length: 406\n",
      "TinyLlama | CURRENT IDX: 406 | Length: 407\n",
      "TinyLlama | CURRENT IDX: 407 | Length: 408\n",
      "TinyLlama | CURRENT IDX: 408 | Length: 409\n",
      "TinyLlama | CURRENT IDX: 409 | Length: 410\n",
      "TinyLlama | CURRENT IDX: 410 | Length: 411\n",
      "TinyLlama | CURRENT IDX: 411 | Length: 412\n",
      "TinyLlama | CURRENT IDX: 412 | Length: 413\n",
      "TinyLlama | CURRENT IDX: 413 | Length: 414\n",
      "TinyLlama | CURRENT IDX: 414 | Length: 415\n",
      "TinyLlama | CURRENT IDX: 415 | Length: 416\n",
      "TinyLlama | CURRENT IDX: 416 | Length: 417\n",
      "TinyLlama | CURRENT IDX: 417 | Length: 418\n",
      "TinyLlama | CURRENT IDX: 418 | Length: 419\n",
      "TinyLlama | CURRENT IDX: 419 | Length: 420\n",
      "TinyLlama | CURRENT IDX: 420 | Length: 421\n",
      "TinyLlama | CURRENT IDX: 421 | Length: 422\n",
      "TinyLlama | CURRENT IDX: 422 | Length: 423\n",
      "TinyLlama | CURRENT IDX: 423 | Length: 424\n",
      "TinyLlama | CURRENT IDX: 424 | Length: 425\n",
      "TinyLlama | CURRENT IDX: 425 | Length: 426\n",
      "TinyLlama | CURRENT IDX: 426 | Length: 427\n",
      "TinyLlama | CURRENT IDX: 427 | Length: 428\n",
      "TinyLlama | CURRENT IDX: 428 | Length: 429\n",
      "TinyLlama | CURRENT IDX: 429 | Length: 430\n",
      "TinyLlama | CURRENT IDX: 430 | Length: 431\n",
      "TinyLlama | CURRENT IDX: 431 | Length: 432\n",
      "TinyLlama | CURRENT IDX: 432 | Length: 433\n",
      "TinyLlama | CURRENT IDX: 433 | Length: 434\n",
      "TinyLlama | CURRENT IDX: 434 | Length: 435\n",
      "TinyLlama | CURRENT IDX: 435 | Length: 436\n",
      "TinyLlama | CURRENT IDX: 436 | Length: 437\n",
      "TinyLlama | CURRENT IDX: 437 | Length: 438\n",
      "TinyLlama | CURRENT IDX: 438 | Length: 439\n",
      "TinyLlama | CURRENT IDX: 439 | Length: 440\n",
      "TinyLlama | CURRENT IDX: 440 | Length: 441\n",
      "TinyLlama | CURRENT IDX: 441 | Length: 442\n",
      "TinyLlama | CURRENT IDX: 442 | Length: 443\n",
      "TinyLlama | CURRENT IDX: 443 | Length: 444\n",
      "TinyLlama | CURRENT IDX: 444 | Length: 445\n",
      "TinyLlama | CURRENT IDX: 445 | Length: 446\n",
      "TinyLlama | CURRENT IDX: 446 | Length: 447\n",
      "TinyLlama | CURRENT IDX: 447 | Length: 448\n",
      "TinyLlama | CURRENT IDX: 448 | Length: 449\n",
      "TinyLlama | CURRENT IDX: 449 | Length: 450\n",
      "TinyLlama | CURRENT IDX: 450 | Length: 451\n",
      "TinyLlama | CURRENT IDX: 451 | Length: 452\n",
      "TinyLlama | CURRENT IDX: 452 | Length: 453\n",
      "TinyLlama | CURRENT IDX: 453 | Length: 454\n",
      "TinyLlama | CURRENT IDX: 454 | Length: 455\n",
      "TinyLlama | CURRENT IDX: 455 | Length: 456\n",
      "TinyLlama | CURRENT IDX: 456 | Length: 457\n",
      "TinyLlama | CURRENT IDX: 457 | Length: 458\n",
      "TinyLlama | CURRENT IDX: 458 | Length: 459\n",
      "TinyLlama | CURRENT IDX: 459 | Length: 460\n",
      "TinyLlama | CURRENT IDX: 460 | Length: 461\n",
      "TinyLlama | CURRENT IDX: 461 | Length: 462\n",
      "TinyLlama | CURRENT IDX: 462 | Length: 463\n",
      "TinyLlama | CURRENT IDX: 463 | Length: 464\n",
      "TinyLlama | CURRENT IDX: 464 | Length: 465\n",
      "TinyLlama | CURRENT IDX: 465 | Length: 466\n",
      "TinyLlama | CURRENT IDX: 466 | Length: 467\n",
      "TinyLlama | CURRENT IDX: 467 | Length: 468\n",
      "TinyLlama | CURRENT IDX: 468 | Length: 469\n",
      "TinyLlama | CURRENT IDX: 469 | Length: 470\n",
      "TinyLlama | CURRENT IDX: 470 | Length: 471\n",
      "TinyLlama | CURRENT IDX: 471 | Length: 472\n",
      "TinyLlama | CURRENT IDX: 472 | Length: 473\n",
      "TinyLlama | CURRENT IDX: 473 | Length: 474\n",
      "TinyLlama | CURRENT IDX: 474 | Length: 475\n",
      "TinyLlama | CURRENT IDX: 475 | Length: 476\n",
      "TinyLlama | CURRENT IDX: 476 | Length: 477\n",
      "TinyLlama | CURRENT IDX: 477 | Length: 478\n",
      "TinyLlama | CURRENT IDX: 478 | Length: 479\n",
      "TinyLlama | CURRENT IDX: 479 | Length: 480\n",
      "TinyLlama | CURRENT IDX: 480 | Length: 481\n",
      "TinyLlama | CURRENT IDX: 481 | Length: 482\n",
      "TinyLlama | CURRENT IDX: 482 | Length: 483\n",
      "TinyLlama | CURRENT IDX: 483 | Length: 484\n",
      "TinyLlama | CURRENT IDX: 484 | Length: 485\n",
      "TinyLlama | CURRENT IDX: 485 | Length: 486\n",
      "TinyLlama | CURRENT IDX: 486 | Length: 487\n",
      "TinyLlama | CURRENT IDX: 487 | Length: 488\n",
      "TinyLlama | CURRENT IDX: 488 | Length: 489\n",
      "TinyLlama | CURRENT IDX: 489 | Length: 490\n",
      "TinyLlama | CURRENT IDX: 490 | Length: 491\n",
      "TinyLlama | CURRENT IDX: 491 | Length: 492\n",
      "TinyLlama | CURRENT IDX: 492 | Length: 493\n",
      "TinyLlama | CURRENT IDX: 493 | Length: 494\n",
      "TinyLlama | CURRENT IDX: 494 | Length: 495\n",
      "TinyLlama | CURRENT IDX: 495 | Length: 496\n",
      "TinyLlama | CURRENT IDX: 496 | Length: 497\n",
      "TinyLlama | CURRENT IDX: 497 | Length: 498\n",
      "TinyLlama | CURRENT IDX: 498 | Length: 499\n",
      "TinyLlama | CURRENT IDX: 499 | Length: 500\n",
      "TinyLlama | CURRENT IDX: 500 | Length: 501\n",
      "TinyLlama | CURRENT IDX: 501 | Length: 502\n",
      "TinyLlama | CURRENT IDX: 502 | Length: 503\n",
      "TinyLlama | CURRENT IDX: 503 | Length: 504\n",
      "TinyLlama | CURRENT IDX: 504 | Length: 505\n",
      "TinyLlama | CURRENT IDX: 505 | Length: 506\n",
      "TinyLlama | CURRENT IDX: 506 | Length: 507\n",
      "TinyLlama | CURRENT IDX: 507 | Length: 508\n",
      "TinyLlama | CURRENT IDX: 508 | Length: 509\n",
      "TinyLlama | CURRENT IDX: 509 | Length: 510\n",
      "TinyLlama | CURRENT IDX: 510 | Length: 511\n",
      "TinyLlama | CURRENT IDX: 511 | Length: 512\n",
      "TinyLlama | CURRENT IDX: 512 | Length: 513\n",
      "TinyLlama | CURRENT IDX: 513 | Length: 514\n",
      "TinyLlama | CURRENT IDX: 514 | Length: 515\n",
      "TinyLlama | CURRENT IDX: 515 | Length: 516\n",
      "TinyLlama | CURRENT IDX: 516 | Length: 517\n",
      "TinyLlama | CURRENT IDX: 517 | Length: 518\n",
      "TinyLlama | CURRENT IDX: 518 | Length: 519\n",
      "TinyLlama | CURRENT IDX: 519 | Length: 520\n",
      "TinyLlama | CURRENT IDX: 520 | Length: 521\n",
      "TinyLlama | CURRENT IDX: 521 | Length: 522\n",
      "TinyLlama | CURRENT IDX: 522 | Length: 523\n",
      "TinyLlama | CURRENT IDX: 523 | Length: 524\n",
      "TinyLlama | CURRENT IDX: 524 | Length: 525\n",
      "TinyLlama | CURRENT IDX: 525 | Length: 526\n",
      "TinyLlama | CURRENT IDX: 526 | Length: 527\n",
      "TinyLlama | CURRENT IDX: 527 | Length: 528\n",
      "TinyLlama | CURRENT IDX: 528 | Length: 529\n",
      "TinyLlama | CURRENT IDX: 529 | Length: 530\n",
      "TinyLlama | CURRENT IDX: 530 | Length: 531\n",
      "TinyLlama | CURRENT IDX: 531 | Length: 532\n",
      "TinyLlama | CURRENT IDX: 532 | Length: 533\n",
      "TinyLlama | CURRENT IDX: 533 | Length: 534\n",
      "TinyLlama | CURRENT IDX: 534 | Length: 535\n",
      "TinyLlama | CURRENT IDX: 535 | Length: 536\n",
      "TinyLlama | CURRENT IDX: 536 | Length: 537\n",
      "TinyLlama | CURRENT IDX: 537 | Length: 538\n",
      "TinyLlama | CURRENT IDX: 538 | Length: 539\n",
      "TinyLlama | CURRENT IDX: 539 | Length: 540\n",
      "TinyLlama | CURRENT IDX: 540 | Length: 541\n",
      "TinyLlama | CURRENT IDX: 541 | Length: 542\n",
      "TinyLlama | CURRENT IDX: 542 | Length: 543\n",
      "TinyLlama | CURRENT IDX: 543 | Length: 544\n",
      "TinyLlama | CURRENT IDX: 544 | Length: 545\n",
      "TinyLlama | CURRENT IDX: 545 | Length: 546\n",
      "TinyLlama | CURRENT IDX: 546 | Length: 547\n",
      "TinyLlama | CURRENT IDX: 547 | Length: 548\n",
      "TinyLlama | CURRENT IDX: 548 | Length: 549\n",
      "TinyLlama | CURRENT IDX: 549 | Length: 550\n",
      "TinyLlama | CURRENT IDX: 550 | Length: 551\n",
      "TinyLlama | CURRENT IDX: 551 | Length: 552\n",
      "TinyLlama | CURRENT IDX: 552 | Length: 553\n",
      "TinyLlama | CURRENT IDX: 553 | Length: 554\n",
      "TinyLlama | CURRENT IDX: 554 | Length: 555\n",
      "TinyLlama | CURRENT IDX: 555 | Length: 556\n",
      "TinyLlama | CURRENT IDX: 556 | Length: 557\n",
      "TinyLlama | CURRENT IDX: 557 | Length: 558\n",
      "TinyLlama | CURRENT IDX: 558 | Length: 559\n",
      "TinyLlama | CURRENT IDX: 559 | Length: 560\n",
      "TinyLlama | CURRENT IDX: 560 | Length: 561\n",
      "TinyLlama | CURRENT IDX: 561 | Length: 562\n",
      "TinyLlama | CURRENT IDX: 562 | Length: 563\n",
      "TinyLlama | CURRENT IDX: 563 | Length: 564\n",
      "TinyLlama | CURRENT IDX: 564 | Length: 565\n",
      "TinyLlama | CURRENT IDX: 565 | Length: 566\n",
      "TinyLlama | CURRENT IDX: 566 | Length: 567\n",
      "TinyLlama | CURRENT IDX: 567 | Length: 568\n",
      "TinyLlama | CURRENT IDX: 568 | Length: 569\n",
      "TinyLlama | CURRENT IDX: 569 | Length: 570\n",
      "TinyLlama | CURRENT IDX: 570 | Length: 571\n",
      "TinyLlama | CURRENT IDX: 571 | Length: 572\n",
      "TinyLlama | CURRENT IDX: 572 | Length: 573\n",
      "TinyLlama | CURRENT IDX: 573 | Length: 574\n",
      "TinyLlama | CURRENT IDX: 574 | Length: 575\n",
      "TinyLlama | CURRENT IDX: 575 | Length: 576\n",
      "TinyLlama | CURRENT IDX: 576 | Length: 577\n",
      "TinyLlama | CURRENT IDX: 577 | Length: 578\n",
      "TinyLlama | CURRENT IDX: 578 | Length: 579\n",
      "TinyLlama | CURRENT IDX: 579 | Length: 580\n",
      "TinyLlama | CURRENT IDX: 580 | Length: 581\n",
      "TinyLlama | CURRENT IDX: 581 | Length: 582\n",
      "TinyLlama | CURRENT IDX: 582 | Length: 583\n",
      "TinyLlama | CURRENT IDX: 583 | Length: 584\n",
      "TinyLlama | CURRENT IDX: 584 | Length: 585\n",
      "TinyLlama | CURRENT IDX: 585 | Length: 586\n",
      "TinyLlama | CURRENT IDX: 586 | Length: 587\n",
      "TinyLlama | CURRENT IDX: 587 | Length: 588\n",
      "TinyLlama | CURRENT IDX: 588 | Length: 589\n",
      "TinyLlama | CURRENT IDX: 589 | Length: 590\n",
      "TinyLlama | CURRENT IDX: 590 | Length: 591\n",
      "TinyLlama | CURRENT IDX: 591 | Length: 592\n",
      "TinyLlama | CURRENT IDX: 592 | Length: 593\n",
      "TinyLlama | CURRENT IDX: 593 | Length: 594\n",
      "TinyLlama | CURRENT IDX: 594 | Length: 595\n",
      "TinyLlama | CURRENT IDX: 595 | Length: 596\n",
      "TinyLlama | CURRENT IDX: 596 | Length: 597\n",
      "TinyLlama | CURRENT IDX: 597 | Length: 598\n",
      "TinyLlama | CURRENT IDX: 598 | Length: 599\n",
      "TinyLlama | CURRENT IDX: 599 | Length: 600\n",
      "TinyLlama | CURRENT IDX: 600 | Length: 601\n",
      "TinyLlama | CURRENT IDX: 601 | Length: 602\n",
      "TinyLlama | CURRENT IDX: 602 | Length: 603\n",
      "TinyLlama | CURRENT IDX: 603 | Length: 604\n",
      "TinyLlama | CURRENT IDX: 604 | Length: 605\n",
      "TinyLlama | CURRENT IDX: 605 | Length: 606\n",
      "TinyLlama | CURRENT IDX: 606 | Length: 607\n",
      "TinyLlama | CURRENT IDX: 607 | Length: 608\n",
      "TinyLlama | CURRENT IDX: 608 | Length: 609\n",
      "TinyLlama | CURRENT IDX: 609 | Length: 610\n",
      "TinyLlama | CURRENT IDX: 610 | Length: 611\n",
      "TinyLlama | CURRENT IDX: 611 | Length: 612\n",
      "TinyLlama | CURRENT IDX: 612 | Length: 613\n",
      "TinyLlama | CURRENT IDX: 613 | Length: 614\n",
      "TinyLlama | CURRENT IDX: 614 | Length: 615\n",
      "TinyLlama | CURRENT IDX: 615 | Length: 616\n",
      "TinyLlama | CURRENT IDX: 616 | Length: 617\n",
      "TinyLlama | CURRENT IDX: 617 | Length: 618\n",
      "TinyLlama | CURRENT IDX: 618 | Length: 619\n",
      "TinyLlama | CURRENT IDX: 619 | Length: 620\n",
      "TinyLlama | CURRENT IDX: 620 | Length: 621\n",
      "TinyLlama | CURRENT IDX: 621 | Length: 622\n",
      "TinyLlama | CURRENT IDX: 622 | Length: 623\n",
      "TinyLlama | CURRENT IDX: 623 | Length: 624\n",
      "TinyLlama | CURRENT IDX: 624 | Length: 625\n",
      "TinyLlama | CURRENT IDX: 625 | Length: 626\n",
      "TinyLlama | CURRENT IDX: 626 | Length: 627\n",
      "TinyLlama | CURRENT IDX: 627 | Length: 628\n",
      "TinyLlama | CURRENT IDX: 628 | Length: 629\n",
      "TinyLlama | CURRENT IDX: 629 | Length: 630\n",
      "TinyLlama | CURRENT IDX: 630 | Length: 631\n",
      "TinyLlama | CURRENT IDX: 631 | Length: 632\n",
      "TinyLlama | CURRENT IDX: 632 | Length: 633\n",
      "TinyLlama | CURRENT IDX: 633 | Length: 634\n",
      "TinyLlama | CURRENT IDX: 634 | Length: 635\n",
      "TinyLlama | CURRENT IDX: 635 | Length: 636\n",
      "TinyLlama | CURRENT IDX: 636 | Length: 637\n",
      "TinyLlama | CURRENT IDX: 637 | Length: 638\n",
      "TinyLlama | CURRENT IDX: 638 | Length: 639\n",
      "TinyLlama | CURRENT IDX: 639 | Length: 640\n",
      "TinyLlama | CURRENT IDX: 640 | Length: 641\n",
      "TinyLlama | CURRENT IDX: 641 | Length: 642\n",
      "TinyLlama | CURRENT IDX: 642 | Length: 643\n",
      "TinyLlama | CURRENT IDX: 643 | Length: 644\n",
      "TinyLlama | CURRENT IDX: 644 | Length: 645\n",
      "TinyLlama | CURRENT IDX: 645 | Length: 646\n",
      "TinyLlama | CURRENT IDX: 646 | Length: 647\n",
      "TinyLlama | CURRENT IDX: 647 | Length: 648\n",
      "TinyLlama | CURRENT IDX: 648 | Length: 649\n",
      "TinyLlama | CURRENT IDX: 649 | Length: 650\n",
      "TinyLlama | CURRENT IDX: 650 | Length: 651\n",
      "TinyLlama | CURRENT IDX: 651 | Length: 652\n",
      "TinyLlama | CURRENT IDX: 652 | Length: 653\n",
      "TinyLlama | CURRENT IDX: 653 | Length: 654\n",
      "TinyLlama | CURRENT IDX: 654 | Length: 655\n",
      "TinyLlama | CURRENT IDX: 655 | Length: 656\n",
      "TinyLlama | CURRENT IDX: 656 | Length: 657\n",
      "TinyLlama | CURRENT IDX: 657 | Length: 658\n",
      "TinyLlama | CURRENT IDX: 658 | Length: 659\n",
      "TinyLlama | CURRENT IDX: 659 | Length: 660\n",
      "TinyLlama | CURRENT IDX: 660 | Length: 661\n",
      "TinyLlama | CURRENT IDX: 661 | Length: 662\n",
      "TinyLlama | CURRENT IDX: 662 | Length: 663\n",
      "TinyLlama | CURRENT IDX: 663 | Length: 664\n",
      "TinyLlama | CURRENT IDX: 664 | Length: 665\n",
      "TinyLlama | CURRENT IDX: 665 | Length: 666\n",
      "TinyLlama | CURRENT IDX: 666 | Length: 667\n",
      "TinyLlama | CURRENT IDX: 667 | Length: 668\n",
      "TinyLlama | CURRENT IDX: 668 | Length: 669\n",
      "TinyLlama | CURRENT IDX: 669 | Length: 670\n",
      "TinyLlama | CURRENT IDX: 670 | Length: 671\n",
      "TinyLlama | CURRENT IDX: 671 | Length: 672\n",
      "TinyLlama | CURRENT IDX: 672 | Length: 673\n",
      "TinyLlama | CURRENT IDX: 673 | Length: 674\n",
      "TinyLlama | CURRENT IDX: 674 | Length: 675\n",
      "TinyLlama | CURRENT IDX: 675 | Length: 676\n",
      "TinyLlama | CURRENT IDX: 676 | Length: 677\n",
      "TinyLlama | CURRENT IDX: 677 | Length: 678\n",
      "TinyLlama | CURRENT IDX: 678 | Length: 679\n",
      "TinyLlama | CURRENT IDX: 679 | Length: 680\n",
      "TinyLlama | CURRENT IDX: 680 | Length: 681\n",
      "TinyLlama | CURRENT IDX: 681 | Length: 682\n",
      "TinyLlama | CURRENT IDX: 682 | Length: 683\n",
      "TinyLlama | CURRENT IDX: 683 | Length: 684\n",
      "TinyLlama | CURRENT IDX: 684 | Length: 685\n",
      "TinyLlama | CURRENT IDX: 685 | Length: 686\n",
      "TinyLlama | CURRENT IDX: 686 | Length: 687\n",
      "TinyLlama | CURRENT IDX: 687 | Length: 688\n",
      "TinyLlama | CURRENT IDX: 688 | Length: 689\n",
      "TinyLlama | CURRENT IDX: 689 | Length: 690\n",
      "TinyLlama | CURRENT IDX: 690 | Length: 691\n",
      "TinyLlama | CURRENT IDX: 691 | Length: 692\n",
      "TinyLlama | CURRENT IDX: 692 | Length: 693\n",
      "TinyLlama | CURRENT IDX: 693 | Length: 694\n",
      "TinyLlama | CURRENT IDX: 694 | Length: 695\n",
      "TinyLlama | CURRENT IDX: 695 | Length: 696\n",
      "TinyLlama | CURRENT IDX: 696 | Length: 697\n",
      "TinyLlama | CURRENT IDX: 697 | Length: 698\n",
      "TinyLlama | CURRENT IDX: 698 | Length: 699\n",
      "TinyLlama | CURRENT IDX: 699 | Length: 700\n",
      "TinyLlama | CURRENT IDX: 700 | Length: 701\n",
      "TinyLlama | CURRENT IDX: 701 | Length: 702\n",
      "TinyLlama | CURRENT IDX: 702 | Length: 703\n",
      "TinyLlama | CURRENT IDX: 703 | Length: 704\n",
      "TinyLlama | CURRENT IDX: 704 | Length: 705\n",
      "TinyLlama | CURRENT IDX: 705 | Length: 706\n",
      "TinyLlama | CURRENT IDX: 706 | Length: 707\n",
      "TinyLlama | CURRENT IDX: 707 | Length: 708\n",
      "TinyLlama | CURRENT IDX: 708 | Length: 709\n",
      "TinyLlama | CURRENT IDX: 709 | Length: 710\n",
      "TinyLlama | CURRENT IDX: 710 | Length: 711\n",
      "TinyLlama | CURRENT IDX: 711 | Length: 712\n",
      "TinyLlama | CURRENT IDX: 712 | Length: 713\n",
      "TinyLlama | CURRENT IDX: 713 | Length: 714\n",
      "TinyLlama | CURRENT IDX: 714 | Length: 715\n",
      "TinyLlama | CURRENT IDX: 715 | Length: 716\n",
      "TinyLlama | CURRENT IDX: 716 | Length: 717\n",
      "TinyLlama | CURRENT IDX: 717 | Length: 718\n",
      "TinyLlama | CURRENT IDX: 718 | Length: 719\n",
      "TinyLlama | CURRENT IDX: 719 | Length: 720\n",
      "TinyLlama | CURRENT IDX: 720 | Length: 721\n",
      "TinyLlama | CURRENT IDX: 721 | Length: 722\n",
      "TinyLlama | CURRENT IDX: 722 | Length: 723\n",
      "TinyLlama | CURRENT IDX: 723 | Length: 724\n",
      "TinyLlama | CURRENT IDX: 724 | Length: 725\n",
      "TinyLlama | CURRENT IDX: 725 | Length: 726\n",
      "TinyLlama | CURRENT IDX: 726 | Length: 727\n",
      "TinyLlama | CURRENT IDX: 727 | Length: 728\n",
      "TinyLlama | CURRENT IDX: 728 | Length: 729\n",
      "TinyLlama | CURRENT IDX: 729 | Length: 730\n",
      "TinyLlama | CURRENT IDX: 730 | Length: 731\n",
      "TinyLlama | CURRENT IDX: 731 | Length: 732\n",
      "TinyLlama | CURRENT IDX: 732 | Length: 733\n",
      "TinyLlama | CURRENT IDX: 733 | Length: 734\n",
      "TinyLlama | CURRENT IDX: 734 | Length: 735\n",
      "TinyLlama | CURRENT IDX: 735 | Length: 736\n",
      "TinyLlama | CURRENT IDX: 736 | Length: 737\n",
      "TinyLlama | CURRENT IDX: 737 | Length: 738\n",
      "TinyLlama | CURRENT IDX: 738 | Length: 739\n",
      "TinyLlama | CURRENT IDX: 739 | Length: 740\n",
      "TinyLlama | CURRENT IDX: 740 | Length: 741\n",
      "TinyLlama | CURRENT IDX: 741 | Length: 742\n",
      "TinyLlama | CURRENT IDX: 742 | Length: 743\n",
      "TinyLlama | CURRENT IDX: 743 | Length: 744\n",
      "TinyLlama | CURRENT IDX: 744 | Length: 745\n",
      "TinyLlama | CURRENT IDX: 745 | Length: 746\n",
      "TinyLlama | CURRENT IDX: 746 | Length: 747\n",
      "TinyLlama | CURRENT IDX: 747 | Length: 748\n",
      "TinyLlama | CURRENT IDX: 748 | Length: 749\n",
      "TinyLlama | CURRENT IDX: 749 | Length: 750\n",
      "TinyLlama | CURRENT IDX: 750 | Length: 751\n",
      "TinyLlama | CURRENT IDX: 751 | Length: 752\n",
      "TinyLlama | CURRENT IDX: 752 | Length: 753\n",
      "TinyLlama | CURRENT IDX: 753 | Length: 754\n",
      "TinyLlama | CURRENT IDX: 754 | Length: 755\n",
      "TinyLlama | CURRENT IDX: 755 | Length: 756\n",
      "TinyLlama | CURRENT IDX: 756 | Length: 757\n",
      "TinyLlama | CURRENT IDX: 757 | Length: 758\n",
      "TinyLlama | CURRENT IDX: 758 | Length: 759\n",
      "TinyLlama | CURRENT IDX: 759 | Length: 760\n",
      "TinyLlama | CURRENT IDX: 760 | Length: 761\n",
      "TinyLlama | CURRENT IDX: 761 | Length: 762\n",
      "TinyLlama | CURRENT IDX: 762 | Length: 763\n",
      "TinyLlama | CURRENT IDX: 763 | Length: 764\n",
      "TinyLlama | CURRENT IDX: 764 | Length: 765\n",
      "TinyLlama | CURRENT IDX: 765 | Length: 766\n",
      "TinyLlama | CURRENT IDX: 766 | Length: 767\n",
      "TinyLlama | CURRENT IDX: 767 | Length: 768\n",
      "TinyLlama | CURRENT IDX: 768 | Length: 769\n",
      "TinyLlama | CURRENT IDX: 769 | Length: 770\n",
      "TinyLlama | CURRENT IDX: 770 | Length: 771\n",
      "TinyLlama | CURRENT IDX: 771 | Length: 772\n",
      "TinyLlama | CURRENT IDX: 772 | Length: 773\n",
      "TinyLlama | CURRENT IDX: 773 | Length: 774\n",
      "TinyLlama | CURRENT IDX: 774 | Length: 775\n",
      "TinyLlama | CURRENT IDX: 775 | Length: 776\n",
      "TinyLlama | CURRENT IDX: 776 | Length: 777\n",
      "TinyLlama | CURRENT IDX: 777 | Length: 778\n",
      "TinyLlama | CURRENT IDX: 778 | Length: 779\n",
      "TinyLlama | CURRENT IDX: 779 | Length: 780\n",
      "TinyLlama | CURRENT IDX: 780 | Length: 781\n",
      "TinyLlama | CURRENT IDX: 781 | Length: 782\n",
      "TinyLlama | CURRENT IDX: 782 | Length: 783\n",
      "TinyLlama | CURRENT IDX: 783 | Length: 784\n",
      "TinyLlama | CURRENT IDX: 784 | Length: 785\n",
      "TinyLlama | CURRENT IDX: 785 | Length: 786\n",
      "TinyLlama | CURRENT IDX: 786 | Length: 787\n",
      "TinyLlama | CURRENT IDX: 787 | Length: 788\n",
      "TinyLlama | CURRENT IDX: 788 | Length: 789\n",
      "TinyLlama | CURRENT IDX: 789 | Length: 790\n",
      "TinyLlama | CURRENT IDX: 790 | Length: 791\n",
      "TinyLlama | CURRENT IDX: 791 | Length: 792\n",
      "TinyLlama | CURRENT IDX: 792 | Length: 793\n",
      "TinyLlama | CURRENT IDX: 793 | Length: 794\n",
      "TinyLlama | CURRENT IDX: 794 | Length: 795\n",
      "TinyLlama | CURRENT IDX: 795 | Length: 796\n",
      "TinyLlama | CURRENT IDX: 796 | Length: 797\n",
      "TinyLlama | CURRENT IDX: 797 | Length: 798\n",
      "TinyLlama | CURRENT IDX: 798 | Length: 799\n",
      "TinyLlama | CURRENT IDX: 799 | Length: 800\n",
      "TinyLlama | CURRENT IDX: 800 | Length: 801\n",
      "TinyLlama | CURRENT IDX: 801 | Length: 802\n",
      "TinyLlama | CURRENT IDX: 802 | Length: 803\n",
      "TinyLlama | CURRENT IDX: 803 | Length: 804\n",
      "TinyLlama | CURRENT IDX: 804 | Length: 805\n",
      "TinyLlama | CURRENT IDX: 805 | Length: 806\n",
      "TinyLlama | CURRENT IDX: 806 | Length: 807\n",
      "TinyLlama | CURRENT IDX: 807 | Length: 808\n",
      "TinyLlama | CURRENT IDX: 808 | Length: 809\n",
      "TinyLlama | CURRENT IDX: 809 | Length: 810\n",
      "TinyLlama | CURRENT IDX: 810 | Length: 811\n",
      "TinyLlama | CURRENT IDX: 811 | Length: 812\n",
      "TinyLlama | CURRENT IDX: 812 | Length: 813\n",
      "TinyLlama | CURRENT IDX: 813 | Length: 814\n",
      "TinyLlama | CURRENT IDX: 814 | Length: 815\n",
      "TinyLlama | CURRENT IDX: 815 | Length: 816\n",
      "TinyLlama | CURRENT IDX: 816 | Length: 817\n",
      "TinyLlama | CURRENT IDX: 817 | Length: 818\n",
      "TinyLlama | CURRENT IDX: 818 | Length: 819\n",
      "TinyLlama | CURRENT IDX: 819 | Length: 820\n",
      "TinyLlama | CURRENT IDX: 820 | Length: 821\n",
      "TinyLlama | CURRENT IDX: 821 | Length: 822\n",
      "TinyLlama | CURRENT IDX: 822 | Length: 823\n",
      "TinyLlama | CURRENT IDX: 823 | Length: 824\n",
      "TinyLlama | CURRENT IDX: 824 | Length: 825\n",
      "TinyLlama | CURRENT IDX: 825 | Length: 826\n",
      "TinyLlama | CURRENT IDX: 826 | Length: 827\n",
      "TinyLlama | CURRENT IDX: 827 | Length: 828\n",
      "TinyLlama | CURRENT IDX: 828 | Length: 829\n",
      "TinyLlama | CURRENT IDX: 829 | Length: 830\n",
      "TinyLlama | CURRENT IDX: 830 | Length: 831\n",
      "TinyLlama | CURRENT IDX: 831 | Length: 832\n",
      "TinyLlama | CURRENT IDX: 832 | Length: 833\n",
      "TinyLlama | CURRENT IDX: 833 | Length: 834\n",
      "TinyLlama | CURRENT IDX: 834 | Length: 835\n",
      "TinyLlama | CURRENT IDX: 835 | Length: 836\n",
      "TinyLlama | CURRENT IDX: 836 | Length: 837\n",
      "TinyLlama | CURRENT IDX: 837 | Length: 838\n",
      "TinyLlama | CURRENT IDX: 838 | Length: 839\n",
      "TinyLlama | CURRENT IDX: 839 | Length: 840\n",
      "TinyLlama | CURRENT IDX: 840 | Length: 841\n",
      "TinyLlama | CURRENT IDX: 841 | Length: 842\n",
      "TinyLlama | CURRENT IDX: 842 | Length: 843\n",
      "TinyLlama | CURRENT IDX: 843 | Length: 844\n",
      "TinyLlama | CURRENT IDX: 844 | Length: 845\n",
      "TinyLlama | CURRENT IDX: 845 | Length: 846\n",
      "TinyLlama | CURRENT IDX: 846 | Length: 847\n",
      "TinyLlama | CURRENT IDX: 847 | Length: 848\n",
      "TinyLlama | CURRENT IDX: 848 | Length: 849\n",
      "TinyLlama | CURRENT IDX: 849 | Length: 850\n",
      "TinyLlama | CURRENT IDX: 850 | Length: 851\n",
      "TinyLlama | CURRENT IDX: 851 | Length: 852\n",
      "TinyLlama | CURRENT IDX: 852 | Length: 853\n",
      "TinyLlama | CURRENT IDX: 853 | Length: 854\n",
      "TinyLlama | CURRENT IDX: 854 | Length: 855\n",
      "TinyLlama | CURRENT IDX: 855 | Length: 856\n",
      "TinyLlama | CURRENT IDX: 856 | Length: 857\n",
      "TinyLlama | CURRENT IDX: 857 | Length: 858\n",
      "TinyLlama | CURRENT IDX: 858 | Length: 859\n",
      "TinyLlama | CURRENT IDX: 859 | Length: 860\n",
      "TinyLlama | CURRENT IDX: 860 | Length: 861\n",
      "TinyLlama | CURRENT IDX: 861 | Length: 862\n",
      "TinyLlama | CURRENT IDX: 862 | Length: 863\n",
      "TinyLlama | CURRENT IDX: 863 | Length: 864\n",
      "TinyLlama | CURRENT IDX: 864 | Length: 865\n",
      "TinyLlama | CURRENT IDX: 865 | Length: 866\n",
      "TinyLlama | CURRENT IDX: 866 | Length: 867\n",
      "TinyLlama | CURRENT IDX: 867 | Length: 868\n",
      "TinyLlama | CURRENT IDX: 868 | Length: 869\n",
      "TinyLlama | CURRENT IDX: 869 | Length: 870\n",
      "TinyLlama | CURRENT IDX: 870 | Length: 871\n",
      "TinyLlama | CURRENT IDX: 871 | Length: 872\n",
      "TinyLlama | CURRENT IDX: 872 | Length: 873\n",
      "TinyLlama | CURRENT IDX: 873 | Length: 874\n",
      "TinyLlama | CURRENT IDX: 874 | Length: 875\n",
      "TinyLlama | CURRENT IDX: 875 | Length: 876\n",
      "TinyLlama | CURRENT IDX: 876 | Length: 877\n",
      "TinyLlama | CURRENT IDX: 877 | Length: 878\n",
      "TinyLlama | CURRENT IDX: 878 | Length: 879\n",
      "TinyLlama | CURRENT IDX: 879 | Length: 880\n",
      "TinyLlama | CURRENT IDX: 880 | Length: 881\n",
      "TinyLlama | CURRENT IDX: 881 | Length: 882\n",
      "TinyLlama | CURRENT IDX: 882 | Length: 883\n",
      "TinyLlama | CURRENT IDX: 883 | Length: 884\n",
      "TinyLlama | CURRENT IDX: 884 | Length: 885\n",
      "TinyLlama | CURRENT IDX: 885 | Length: 886\n",
      "TinyLlama | CURRENT IDX: 886 | Length: 887\n",
      "TinyLlama | CURRENT IDX: 887 | Length: 888\n",
      "TinyLlama | CURRENT IDX: 888 | Length: 889\n",
      "TinyLlama | CURRENT IDX: 889 | Length: 890\n",
      "TinyLlama | CURRENT IDX: 890 | Length: 891\n",
      "TinyLlama | CURRENT IDX: 891 | Length: 892\n",
      "TinyLlama | CURRENT IDX: 892 | Length: 893\n",
      "TinyLlama | CURRENT IDX: 893 | Length: 894\n",
      "TinyLlama | CURRENT IDX: 894 | Length: 895\n",
      "TinyLlama | CURRENT IDX: 895 | Length: 896\n",
      "TinyLlama | CURRENT IDX: 896 | Length: 897\n",
      "TinyLlama | CURRENT IDX: 897 | Length: 898\n",
      "TinyLlama | CURRENT IDX: 898 | Length: 899\n",
      "TinyLlama | CURRENT IDX: 899 | Length: 900\n",
      "TinyLlama | CURRENT IDX: 900 | Length: 901\n",
      "TinyLlama | CURRENT IDX: 901 | Length: 902\n",
      "TinyLlama | CURRENT IDX: 902 | Length: 903\n",
      "TinyLlama | CURRENT IDX: 903 | Length: 904\n",
      "TinyLlama | CURRENT IDX: 904 | Length: 905\n",
      "TinyLlama | CURRENT IDX: 905 | Length: 906\n",
      "TinyLlama | CURRENT IDX: 906 | Length: 907\n",
      "TinyLlama | CURRENT IDX: 907 | Length: 908\n",
      "TinyLlama | CURRENT IDX: 908 | Length: 909\n",
      "TinyLlama | CURRENT IDX: 909 | Length: 910\n",
      "TinyLlama | CURRENT IDX: 910 | Length: 911\n",
      "TinyLlama | CURRENT IDX: 911 | Length: 912\n",
      "TinyLlama | CURRENT IDX: 912 | Length: 913\n",
      "TinyLlama | CURRENT IDX: 913 | Length: 914\n",
      "TinyLlama | CURRENT IDX: 914 | Length: 915\n",
      "TinyLlama | CURRENT IDX: 915 | Length: 916\n",
      "TinyLlama | CURRENT IDX: 916 | Length: 917\n",
      "TinyLlama | CURRENT IDX: 917 | Length: 918\n",
      "TinyLlama | CURRENT IDX: 918 | Length: 919\n",
      "TinyLlama | CURRENT IDX: 919 | Length: 920\n",
      "TinyLlama | CURRENT IDX: 920 | Length: 921\n",
      "TinyLlama | CURRENT IDX: 921 | Length: 922\n",
      "TinyLlama | CURRENT IDX: 922 | Length: 923\n",
      "TinyLlama | CURRENT IDX: 923 | Length: 924\n",
      "TinyLlama | CURRENT IDX: 924 | Length: 925\n",
      "TinyLlama | CURRENT IDX: 925 | Length: 926\n",
      "TinyLlama | CURRENT IDX: 926 | Length: 927\n",
      "TinyLlama | CURRENT IDX: 927 | Length: 928\n",
      "TinyLlama | CURRENT IDX: 928 | Length: 929\n",
      "TinyLlama | CURRENT IDX: 929 | Length: 930\n",
      "TinyLlama | CURRENT IDX: 930 | Length: 931\n",
      "TinyLlama | CURRENT IDX: 931 | Length: 932\n",
      "TinyLlama | CURRENT IDX: 932 | Length: 933\n",
      "TinyLlama | CURRENT IDX: 933 | Length: 934\n",
      "TinyLlama | CURRENT IDX: 934 | Length: 935\n",
      "TinyLlama | CURRENT IDX: 935 | Length: 936\n",
      "TinyLlama | CURRENT IDX: 936 | Length: 937\n",
      "TinyLlama | CURRENT IDX: 937 | Length: 938\n",
      "TinyLlama | CURRENT IDX: 938 | Length: 939\n",
      "TinyLlama | CURRENT IDX: 939 | Length: 940\n",
      "TinyLlama | CURRENT IDX: 940 | Length: 941\n",
      "TinyLlama | CURRENT IDX: 941 | Length: 942\n",
      "TinyLlama | CURRENT IDX: 942 | Length: 943\n",
      "TinyLlama | CURRENT IDX: 943 | Length: 944\n",
      "TinyLlama | CURRENT IDX: 944 | Length: 945\n",
      "TinyLlama | CURRENT IDX: 945 | Length: 946\n",
      "TinyLlama | CURRENT IDX: 946 | Length: 947\n",
      "TinyLlama | CURRENT IDX: 947 | Length: 948\n",
      "TinyLlama | CURRENT IDX: 948 | Length: 949\n",
      "TinyLlama | CURRENT IDX: 949 | Length: 950\n",
      "TinyLlama | CURRENT IDX: 950 | Length: 951\n",
      "TinyLlama | CURRENT IDX: 951 | Length: 952\n",
      "TinyLlama | CURRENT IDX: 952 | Length: 953\n",
      "TinyLlama | CURRENT IDX: 953 | Length: 954\n",
      "TinyLlama | CURRENT IDX: 954 | Length: 955\n",
      "TinyLlama | CURRENT IDX: 955 | Length: 956\n",
      "TinyLlama | CURRENT IDX: 956 | Length: 957\n",
      "TinyLlama | CURRENT IDX: 957 | Length: 958\n",
      "TinyLlama | CURRENT IDX: 958 | Length: 959\n",
      "TinyLlama | CURRENT IDX: 959 | Length: 960\n",
      "TinyLlama | CURRENT IDX: 960 | Length: 961\n",
      "TinyLlama | CURRENT IDX: 961 | Length: 962\n",
      "TinyLlama | CURRENT IDX: 962 | Length: 963\n",
      "TinyLlama | CURRENT IDX: 963 | Length: 964\n",
      "TinyLlama | CURRENT IDX: 964 | Length: 965\n",
      "TinyLlama | CURRENT IDX: 965 | Length: 966\n",
      "TinyLlama | CURRENT IDX: 966 | Length: 967\n",
      "TinyLlama | CURRENT IDX: 967 | Length: 968\n",
      "TinyLlama | CURRENT IDX: 968 | Length: 969\n",
      "TinyLlama | CURRENT IDX: 969 | Length: 970\n",
      "TinyLlama | CURRENT IDX: 970 | Length: 971\n",
      "TinyLlama | CURRENT IDX: 971 | Length: 972\n",
      "TinyLlama | CURRENT IDX: 972 | Length: 973\n",
      "TinyLlama | CURRENT IDX: 973 | Length: 974\n",
      "TinyLlama | CURRENT IDX: 974 | Length: 975\n",
      "TinyLlama | CURRENT IDX: 975 | Length: 976\n",
      "TinyLlama | CURRENT IDX: 976 | Length: 977\n",
      "TinyLlama | CURRENT IDX: 977 | Length: 978\n",
      "TinyLlama | CURRENT IDX: 978 | Length: 979\n",
      "TinyLlama | CURRENT IDX: 979 | Length: 980\n",
      "TinyLlama | CURRENT IDX: 980 | Length: 981\n",
      "TinyLlama | CURRENT IDX: 981 | Length: 982\n",
      "TinyLlama | CURRENT IDX: 982 | Length: 983\n",
      "TinyLlama | CURRENT IDX: 983 | Length: 984\n",
      "TinyLlama | CURRENT IDX: 984 | Length: 985\n",
      "TinyLlama | CURRENT IDX: 985 | Length: 986\n",
      "TinyLlama | CURRENT IDX: 986 | Length: 987\n",
      "TinyLlama | CURRENT IDX: 987 | Length: 988\n",
      "TinyLlama | CURRENT IDX: 988 | Length: 989\n",
      "TinyLlama | CURRENT IDX: 989 | Length: 990\n",
      "TinyLlama | CURRENT IDX: 990 | Length: 991\n",
      "TinyLlama | CURRENT IDX: 991 | Length: 992\n",
      "TinyLlama | CURRENT IDX: 992 | Length: 993\n",
      "TinyLlama | CURRENT IDX: 993 | Length: 994\n",
      "TinyLlama | CURRENT IDX: 994 | Length: 995\n",
      "TinyLlama | CURRENT IDX: 995 | Length: 996\n",
      "TinyLlama | CURRENT IDX: 996 | Length: 997\n",
      "TinyLlama | CURRENT IDX: 997 | Length: 998\n",
      "TinyLlama | CURRENT IDX: 998 | Length: 999\n",
      "TinyLlama | CURRENT IDX: 999 | Length: 1000\n"
     ]
    }
   ],
   "source": [
    "# for current_idx in range(0, 1000):\n",
    "#     input_text = cnn_dailymail_dataset[current_idx]['article']\n",
    "#     output_tiny = generate_output(tinyllama, tinyllama_tokenizer, input_text, current_idx)\n",
    "    \n",
    "#     outputs_tiny.append(output_tiny)\n",
    "    \n",
    "#     print(f\"TinyLlama | CURRENT IDX: {current_idx} | Length: {len(outputs_tiny)}\")\n",
    "#     with open('input_output_pairs_cnn_dailymail_tinyllama', 'wb') as f:\n",
    "#         pickle.dump(outputs_tiny, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb8800bc-7725-4106-8f72-c28bda260403",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "with open('input_output_pairs_cnn_dailymail_tinyllama', 'rb') as f:\n",
    "    outputs_tiny = pickle.load(f)\n",
    "\n",
    "print(len(outputs_tiny))\n",
    "# print(outputs_tiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d378239-0de1-4d87-ba94-659dffc7f572",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-13b | CURRENT IDX: 604 | Length: 605\n",
      "Llama-13b | CURRENT IDX: 605 | Length: 606\n",
      "Llama-13b | CURRENT IDX: 606 | Length: 607\n",
      "Llama-13b | CURRENT IDX: 607 | Length: 608\n",
      "Llama-13b | CURRENT IDX: 608 | Length: 609\n",
      "Llama-13b | CURRENT IDX: 609 | Length: 610\n",
      "Llama-13b | CURRENT IDX: 610 | Length: 611\n",
      "Llama-13b | CURRENT IDX: 611 | Length: 612\n",
      "Llama-13b | CURRENT IDX: 612 | Length: 613\n",
      "Llama-13b | CURRENT IDX: 613 | Length: 614\n",
      "Llama-13b | CURRENT IDX: 614 | Length: 615\n",
      "Llama-13b | CURRENT IDX: 615 | Length: 616\n",
      "Llama-13b | CURRENT IDX: 616 | Length: 617\n",
      "Llama-13b | CURRENT IDX: 617 | Length: 618\n",
      "Llama-13b | CURRENT IDX: 618 | Length: 619\n",
      "Llama-13b | CURRENT IDX: 619 | Length: 620\n",
      "Llama-13b | CURRENT IDX: 620 | Length: 621\n",
      "Llama-13b | CURRENT IDX: 621 | Length: 622\n",
      "Llama-13b | CURRENT IDX: 622 | Length: 623\n",
      "Llama-13b | CURRENT IDX: 623 | Length: 624\n",
      "Llama-13b | CURRENT IDX: 624 | Length: 625\n",
      "Llama-13b | CURRENT IDX: 625 | Length: 626\n",
      "Llama-13b | CURRENT IDX: 626 | Length: 627\n",
      "Llama-13b | CURRENT IDX: 627 | Length: 628\n",
      "Llama-13b | CURRENT IDX: 628 | Length: 629\n",
      "Llama-13b | CURRENT IDX: 629 | Length: 630\n",
      "Llama-13b | CURRENT IDX: 630 | Length: 631\n",
      "Llama-13b | CURRENT IDX: 631 | Length: 632\n",
      "Llama-13b | CURRENT IDX: 632 | Length: 633\n",
      "Llama-13b | CURRENT IDX: 633 | Length: 634\n",
      "Llama-13b | CURRENT IDX: 634 | Length: 635\n",
      "Llama-13b | CURRENT IDX: 635 | Length: 636\n",
      "Llama-13b | CURRENT IDX: 636 | Length: 637\n",
      "Llama-13b | CURRENT IDX: 637 | Length: 638\n",
      "Llama-13b | CURRENT IDX: 638 | Length: 639\n",
      "Llama-13b | CURRENT IDX: 639 | Length: 640\n",
      "Llama-13b | CURRENT IDX: 640 | Length: 641\n",
      "Llama-13b | CURRENT IDX: 641 | Length: 642\n",
      "Llama-13b | CURRENT IDX: 642 | Length: 643\n",
      "Llama-13b | CURRENT IDX: 643 | Length: 644\n",
      "Llama-13b | CURRENT IDX: 644 | Length: 645\n",
      "Llama-13b | CURRENT IDX: 645 | Length: 646\n",
      "Llama-13b | CURRENT IDX: 646 | Length: 647\n",
      "Llama-13b | CURRENT IDX: 647 | Length: 648\n",
      "Llama-13b | CURRENT IDX: 648 | Length: 649\n",
      "Llama-13b | CURRENT IDX: 649 | Length: 650\n",
      "Llama-13b | CURRENT IDX: 650 | Length: 651\n",
      "Llama-13b | CURRENT IDX: 651 | Length: 652\n",
      "Llama-13b | CURRENT IDX: 652 | Length: 653\n",
      "Llama-13b | CURRENT IDX: 653 | Length: 654\n",
      "Llama-13b | CURRENT IDX: 654 | Length: 655\n",
      "Llama-13b | CURRENT IDX: 655 | Length: 656\n",
      "Llama-13b | CURRENT IDX: 656 | Length: 657\n",
      "Llama-13b | CURRENT IDX: 657 | Length: 658\n",
      "Llama-13b | CURRENT IDX: 658 | Length: 659\n",
      "Llama-13b | CURRENT IDX: 659 | Length: 660\n",
      "Llama-13b | CURRENT IDX: 660 | Length: 661\n",
      "Llama-13b | CURRENT IDX: 661 | Length: 662\n",
      "Llama-13b | CURRENT IDX: 662 | Length: 663\n",
      "Llama-13b | CURRENT IDX: 663 | Length: 664\n",
      "Llama-13b | CURRENT IDX: 664 | Length: 665\n",
      "Llama-13b | CURRENT IDX: 665 | Length: 666\n",
      "Llama-13b | CURRENT IDX: 666 | Length: 667\n",
      "Llama-13b | CURRENT IDX: 667 | Length: 668\n",
      "Llama-13b | CURRENT IDX: 668 | Length: 669\n",
      "Llama-13b | CURRENT IDX: 669 | Length: 670\n",
      "Llama-13b | CURRENT IDX: 670 | Length: 671\n",
      "Llama-13b | CURRENT IDX: 671 | Length: 672\n",
      "Llama-13b | CURRENT IDX: 672 | Length: 673\n",
      "Llama-13b | CURRENT IDX: 673 | Length: 674\n",
      "Llama-13b | CURRENT IDX: 674 | Length: 675\n",
      "Llama-13b | CURRENT IDX: 675 | Length: 676\n",
      "Llama-13b | CURRENT IDX: 676 | Length: 677\n",
      "Llama-13b | CURRENT IDX: 677 | Length: 678\n",
      "Llama-13b | CURRENT IDX: 678 | Length: 679\n",
      "Llama-13b | CURRENT IDX: 679 | Length: 680\n",
      "Llama-13b | CURRENT IDX: 680 | Length: 681\n",
      "Llama-13b | CURRENT IDX: 681 | Length: 682\n",
      "Llama-13b | CURRENT IDX: 682 | Length: 683\n",
      "Llama-13b | CURRENT IDX: 683 | Length: 684\n",
      "Llama-13b | CURRENT IDX: 684 | Length: 685\n",
      "Llama-13b | CURRENT IDX: 685 | Length: 686\n",
      "Llama-13b | CURRENT IDX: 686 | Length: 687\n",
      "Llama-13b | CURRENT IDX: 687 | Length: 688\n",
      "Llama-13b | CURRENT IDX: 688 | Length: 689\n",
      "Llama-13b | CURRENT IDX: 689 | Length: 690\n",
      "Llama-13b | CURRENT IDX: 690 | Length: 691\n",
      "Llama-13b | CURRENT IDX: 691 | Length: 692\n",
      "Llama-13b | CURRENT IDX: 692 | Length: 693\n",
      "Llama-13b | CURRENT IDX: 693 | Length: 694\n",
      "Llama-13b | CURRENT IDX: 694 | Length: 695\n",
      "Llama-13b | CURRENT IDX: 695 | Length: 696\n",
      "Llama-13b | CURRENT IDX: 696 | Length: 697\n",
      "Llama-13b | CURRENT IDX: 697 | Length: 698\n",
      "Llama-13b | CURRENT IDX: 698 | Length: 699\n",
      "Llama-13b | CURRENT IDX: 699 | Length: 700\n",
      "Llama-13b | CURRENT IDX: 700 | Length: 701\n",
      "Llama-13b | CURRENT IDX: 701 | Length: 702\n",
      "Llama-13b | CURRENT IDX: 702 | Length: 703\n",
      "Llama-13b | CURRENT IDX: 703 | Length: 704\n",
      "Llama-13b | CURRENT IDX: 704 | Length: 705\n",
      "Llama-13b | CURRENT IDX: 705 | Length: 706\n",
      "Llama-13b | CURRENT IDX: 706 | Length: 707\n",
      "Llama-13b | CURRENT IDX: 707 | Length: 708\n",
      "Llama-13b | CURRENT IDX: 708 | Length: 709\n",
      "Llama-13b | CURRENT IDX: 709 | Length: 710\n",
      "Llama-13b | CURRENT IDX: 710 | Length: 711\n",
      "Llama-13b | CURRENT IDX: 711 | Length: 712\n",
      "Llama-13b | CURRENT IDX: 712 | Length: 713\n",
      "Llama-13b | CURRENT IDX: 713 | Length: 714\n",
      "Llama-13b | CURRENT IDX: 714 | Length: 715\n",
      "Llama-13b | CURRENT IDX: 715 | Length: 716\n",
      "Llama-13b | CURRENT IDX: 716 | Length: 717\n",
      "Llama-13b | CURRENT IDX: 717 | Length: 718\n",
      "Llama-13b | CURRENT IDX: 718 | Length: 719\n",
      "Llama-13b | CURRENT IDX: 719 | Length: 720\n",
      "Llama-13b | CURRENT IDX: 720 | Length: 721\n",
      "Llama-13b | CURRENT IDX: 721 | Length: 722\n",
      "Llama-13b | CURRENT IDX: 722 | Length: 723\n",
      "Llama-13b | CURRENT IDX: 723 | Length: 724\n",
      "Llama-13b | CURRENT IDX: 724 | Length: 725\n",
      "Llama-13b | CURRENT IDX: 725 | Length: 726\n",
      "Llama-13b | CURRENT IDX: 726 | Length: 727\n",
      "Llama-13b | CURRENT IDX: 727 | Length: 728\n",
      "Llama-13b | CURRENT IDX: 728 | Length: 729\n",
      "Llama-13b | CURRENT IDX: 729 | Length: 730\n",
      "Llama-13b | CURRENT IDX: 730 | Length: 731\n",
      "Llama-13b | CURRENT IDX: 731 | Length: 732\n",
      "Llama-13b | CURRENT IDX: 732 | Length: 733\n",
      "Llama-13b | CURRENT IDX: 733 | Length: 734\n",
      "Llama-13b | CURRENT IDX: 734 | Length: 735\n",
      "Llama-13b | CURRENT IDX: 735 | Length: 736\n",
      "Llama-13b | CURRENT IDX: 736 | Length: 737\n",
      "Llama-13b | CURRENT IDX: 737 | Length: 738\n",
      "Llama-13b | CURRENT IDX: 738 | Length: 739\n",
      "Llama-13b | CURRENT IDX: 739 | Length: 740\n",
      "Llama-13b | CURRENT IDX: 740 | Length: 741\n",
      "Llama-13b | CURRENT IDX: 741 | Length: 742\n",
      "Llama-13b | CURRENT IDX: 742 | Length: 743\n",
      "Llama-13b | CURRENT IDX: 743 | Length: 744\n",
      "Llama-13b | CURRENT IDX: 744 | Length: 745\n",
      "Llama-13b | CURRENT IDX: 745 | Length: 746\n",
      "Llama-13b | CURRENT IDX: 746 | Length: 747\n",
      "Llama-13b | CURRENT IDX: 747 | Length: 748\n",
      "Llama-13b | CURRENT IDX: 748 | Length: 749\n",
      "Llama-13b | CURRENT IDX: 749 | Length: 750\n",
      "Llama-13b | CURRENT IDX: 750 | Length: 751\n",
      "Llama-13b | CURRENT IDX: 751 | Length: 752\n",
      "Llama-13b | CURRENT IDX: 752 | Length: 753\n",
      "Llama-13b | CURRENT IDX: 753 | Length: 754\n",
      "Llama-13b | CURRENT IDX: 754 | Length: 755\n",
      "Llama-13b | CURRENT IDX: 755 | Length: 756\n",
      "Llama-13b | CURRENT IDX: 756 | Length: 757\n",
      "Llama-13b | CURRENT IDX: 757 | Length: 758\n",
      "Llama-13b | CURRENT IDX: 758 | Length: 759\n",
      "Llama-13b | CURRENT IDX: 759 | Length: 760\n",
      "Llama-13b | CURRENT IDX: 760 | Length: 761\n",
      "Llama-13b | CURRENT IDX: 761 | Length: 762\n",
      "Llama-13b | CURRENT IDX: 762 | Length: 763\n",
      "Llama-13b | CURRENT IDX: 763 | Length: 764\n",
      "Llama-13b | CURRENT IDX: 764 | Length: 765\n",
      "Llama-13b | CURRENT IDX: 765 | Length: 766\n",
      "Llama-13b | CURRENT IDX: 766 | Length: 767\n",
      "Llama-13b | CURRENT IDX: 767 | Length: 768\n",
      "Llama-13b | CURRENT IDX: 768 | Length: 769\n",
      "Llama-13b | CURRENT IDX: 769 | Length: 770\n",
      "Llama-13b | CURRENT IDX: 770 | Length: 771\n",
      "Llama-13b | CURRENT IDX: 771 | Length: 772\n",
      "Llama-13b | CURRENT IDX: 772 | Length: 773\n",
      "Llama-13b | CURRENT IDX: 773 | Length: 774\n",
      "Llama-13b | CURRENT IDX: 774 | Length: 775\n",
      "Llama-13b | CURRENT IDX: 775 | Length: 776\n",
      "Llama-13b | CURRENT IDX: 776 | Length: 777\n",
      "Llama-13b | CURRENT IDX: 777 | Length: 778\n",
      "Llama-13b | CURRENT IDX: 778 | Length: 779\n",
      "Llama-13b | CURRENT IDX: 779 | Length: 780\n",
      "Llama-13b | CURRENT IDX: 780 | Length: 781\n",
      "Llama-13b | CURRENT IDX: 781 | Length: 782\n",
      "Llama-13b | CURRENT IDX: 782 | Length: 783\n",
      "Llama-13b | CURRENT IDX: 783 | Length: 784\n",
      "Llama-13b | CURRENT IDX: 784 | Length: 785\n",
      "Llama-13b | CURRENT IDX: 785 | Length: 786\n",
      "Llama-13b | CURRENT IDX: 786 | Length: 787\n",
      "Llama-13b | CURRENT IDX: 787 | Length: 788\n",
      "Llama-13b | CURRENT IDX: 788 | Length: 789\n",
      "Llama-13b | CURRENT IDX: 789 | Length: 790\n",
      "Llama-13b | CURRENT IDX: 790 | Length: 791\n",
      "Llama-13b | CURRENT IDX: 791 | Length: 792\n",
      "Llama-13b | CURRENT IDX: 792 | Length: 793\n",
      "Llama-13b | CURRENT IDX: 793 | Length: 794\n",
      "Llama-13b | CURRENT IDX: 794 | Length: 795\n",
      "Llama-13b | CURRENT IDX: 795 | Length: 796\n",
      "Llama-13b | CURRENT IDX: 796 | Length: 797\n",
      "Llama-13b | CURRENT IDX: 797 | Length: 798\n",
      "Llama-13b | CURRENT IDX: 798 | Length: 799\n",
      "Llama-13b | CURRENT IDX: 799 | Length: 800\n",
      "Llama-13b | CURRENT IDX: 800 | Length: 801\n",
      "Llama-13b | CURRENT IDX: 801 | Length: 802\n",
      "Llama-13b | CURRENT IDX: 802 | Length: 803\n",
      "Llama-13b | CURRENT IDX: 803 | Length: 804\n",
      "Llama-13b | CURRENT IDX: 804 | Length: 805\n",
      "Llama-13b | CURRENT IDX: 805 | Length: 806\n",
      "Llama-13b | CURRENT IDX: 806 | Length: 807\n",
      "Llama-13b | CURRENT IDX: 807 | Length: 808\n",
      "Llama-13b | CURRENT IDX: 808 | Length: 809\n",
      "Llama-13b | CURRENT IDX: 809 | Length: 810\n",
      "Llama-13b | CURRENT IDX: 810 | Length: 811\n",
      "Llama-13b | CURRENT IDX: 811 | Length: 812\n",
      "Llama-13b | CURRENT IDX: 812 | Length: 813\n",
      "Llama-13b | CURRENT IDX: 813 | Length: 814\n",
      "Llama-13b | CURRENT IDX: 814 | Length: 815\n",
      "Llama-13b | CURRENT IDX: 815 | Length: 816\n",
      "Llama-13b | CURRENT IDX: 816 | Length: 817\n",
      "Llama-13b | CURRENT IDX: 817 | Length: 818\n",
      "Llama-13b | CURRENT IDX: 818 | Length: 819\n",
      "Llama-13b | CURRENT IDX: 819 | Length: 820\n",
      "Llama-13b | CURRENT IDX: 820 | Length: 821\n",
      "Llama-13b | CURRENT IDX: 821 | Length: 822\n",
      "Llama-13b | CURRENT IDX: 822 | Length: 823\n",
      "Llama-13b | CURRENT IDX: 823 | Length: 824\n",
      "Llama-13b | CURRENT IDX: 824 | Length: 825\n",
      "Llama-13b | CURRENT IDX: 825 | Length: 826\n",
      "Llama-13b | CURRENT IDX: 826 | Length: 827\n",
      "Llama-13b | CURRENT IDX: 827 | Length: 828\n",
      "Llama-13b | CURRENT IDX: 828 | Length: 829\n",
      "Llama-13b | CURRENT IDX: 829 | Length: 830\n",
      "Llama-13b | CURRENT IDX: 830 | Length: 831\n",
      "Llama-13b | CURRENT IDX: 831 | Length: 832\n",
      "Llama-13b | CURRENT IDX: 832 | Length: 833\n",
      "Llama-13b | CURRENT IDX: 833 | Length: 834\n",
      "Llama-13b | CURRENT IDX: 834 | Length: 835\n",
      "Llama-13b | CURRENT IDX: 835 | Length: 836\n",
      "Llama-13b | CURRENT IDX: 836 | Length: 837\n",
      "Llama-13b | CURRENT IDX: 837 | Length: 838\n",
      "Llama-13b | CURRENT IDX: 838 | Length: 839\n",
      "Llama-13b | CURRENT IDX: 839 | Length: 840\n",
      "Llama-13b | CURRENT IDX: 840 | Length: 841\n",
      "Llama-13b | CURRENT IDX: 841 | Length: 842\n",
      "Llama-13b | CURRENT IDX: 842 | Length: 843\n",
      "Llama-13b | CURRENT IDX: 843 | Length: 844\n",
      "Llama-13b | CURRENT IDX: 844 | Length: 845\n",
      "Llama-13b | CURRENT IDX: 845 | Length: 846\n",
      "Llama-13b | CURRENT IDX: 846 | Length: 847\n",
      "Llama-13b | CURRENT IDX: 847 | Length: 848\n",
      "Llama-13b | CURRENT IDX: 848 | Length: 849\n",
      "Llama-13b | CURRENT IDX: 849 | Length: 850\n",
      "Llama-13b | CURRENT IDX: 850 | Length: 851\n",
      "Llama-13b | CURRENT IDX: 851 | Length: 852\n",
      "Llama-13b | CURRENT IDX: 852 | Length: 853\n",
      "Llama-13b | CURRENT IDX: 853 | Length: 854\n",
      "Llama-13b | CURRENT IDX: 854 | Length: 855\n",
      "Llama-13b | CURRENT IDX: 855 | Length: 856\n",
      "Llama-13b | CURRENT IDX: 856 | Length: 857\n",
      "Llama-13b | CURRENT IDX: 857 | Length: 858\n",
      "Llama-13b | CURRENT IDX: 858 | Length: 859\n",
      "Llama-13b | CURRENT IDX: 859 | Length: 860\n",
      "Llama-13b | CURRENT IDX: 860 | Length: 861\n",
      "Llama-13b | CURRENT IDX: 861 | Length: 862\n",
      "Llama-13b | CURRENT IDX: 862 | Length: 863\n",
      "Llama-13b | CURRENT IDX: 863 | Length: 864\n",
      "Llama-13b | CURRENT IDX: 864 | Length: 865\n",
      "Llama-13b | CURRENT IDX: 865 | Length: 866\n",
      "Llama-13b | CURRENT IDX: 866 | Length: 867\n",
      "Llama-13b | CURRENT IDX: 867 | Length: 868\n",
      "Llama-13b | CURRENT IDX: 868 | Length: 869\n",
      "Llama-13b | CURRENT IDX: 869 | Length: 870\n",
      "Llama-13b | CURRENT IDX: 870 | Length: 871\n",
      "Llama-13b | CURRENT IDX: 871 | Length: 872\n",
      "Llama-13b | CURRENT IDX: 872 | Length: 873\n",
      "Llama-13b | CURRENT IDX: 873 | Length: 874\n",
      "Llama-13b | CURRENT IDX: 874 | Length: 875\n",
      "Llama-13b | CURRENT IDX: 875 | Length: 876\n",
      "Llama-13b | CURRENT IDX: 876 | Length: 877\n",
      "Llama-13b | CURRENT IDX: 877 | Length: 878\n",
      "Llama-13b | CURRENT IDX: 878 | Length: 879\n",
      "Llama-13b | CURRENT IDX: 879 | Length: 880\n",
      "Llama-13b | CURRENT IDX: 880 | Length: 881\n",
      "Llama-13b | CURRENT IDX: 881 | Length: 882\n",
      "Llama-13b | CURRENT IDX: 882 | Length: 883\n",
      "Llama-13b | CURRENT IDX: 883 | Length: 884\n",
      "Llama-13b | CURRENT IDX: 884 | Length: 885\n",
      "Llama-13b | CURRENT IDX: 885 | Length: 886\n",
      "Llama-13b | CURRENT IDX: 886 | Length: 887\n",
      "Llama-13b | CURRENT IDX: 887 | Length: 888\n",
      "Llama-13b | CURRENT IDX: 888 | Length: 889\n",
      "Llama-13b | CURRENT IDX: 889 | Length: 890\n",
      "Llama-13b | CURRENT IDX: 890 | Length: 891\n",
      "Llama-13b | CURRENT IDX: 891 | Length: 892\n",
      "Llama-13b | CURRENT IDX: 892 | Length: 893\n",
      "Llama-13b | CURRENT IDX: 893 | Length: 894\n",
      "Llama-13b | CURRENT IDX: 894 | Length: 895\n",
      "Llama-13b | CURRENT IDX: 895 | Length: 896\n",
      "Llama-13b | CURRENT IDX: 896 | Length: 897\n",
      "Llama-13b | CURRENT IDX: 897 | Length: 898\n",
      "Llama-13b | CURRENT IDX: 898 | Length: 899\n",
      "Llama-13b | CURRENT IDX: 899 | Length: 900\n",
      "Llama-13b | CURRENT IDX: 900 | Length: 901\n",
      "Llama-13b | CURRENT IDX: 901 | Length: 902\n",
      "Llama-13b | CURRENT IDX: 902 | Length: 903\n",
      "Llama-13b | CURRENT IDX: 903 | Length: 904\n",
      "Llama-13b | CURRENT IDX: 904 | Length: 905\n",
      "Llama-13b | CURRENT IDX: 905 | Length: 906\n",
      "Llama-13b | CURRENT IDX: 906 | Length: 907\n",
      "Llama-13b | CURRENT IDX: 907 | Length: 908\n",
      "Llama-13b | CURRENT IDX: 908 | Length: 909\n",
      "Llama-13b | CURRENT IDX: 909 | Length: 910\n",
      "Llama-13b | CURRENT IDX: 910 | Length: 911\n",
      "Llama-13b | CURRENT IDX: 911 | Length: 912\n",
      "Llama-13b | CURRENT IDX: 912 | Length: 913\n",
      "Llama-13b | CURRENT IDX: 913 | Length: 914\n",
      "Llama-13b | CURRENT IDX: 914 | Length: 915\n",
      "Llama-13b | CURRENT IDX: 915 | Length: 916\n",
      "Llama-13b | CURRENT IDX: 916 | Length: 917\n",
      "Llama-13b | CURRENT IDX: 917 | Length: 918\n",
      "Llama-13b | CURRENT IDX: 918 | Length: 919\n",
      "Llama-13b | CURRENT IDX: 919 | Length: 920\n",
      "Llama-13b | CURRENT IDX: 920 | Length: 921\n",
      "Llama-13b | CURRENT IDX: 921 | Length: 922\n",
      "Llama-13b | CURRENT IDX: 922 | Length: 923\n",
      "Llama-13b | CURRENT IDX: 923 | Length: 924\n",
      "Llama-13b | CURRENT IDX: 924 | Length: 925\n",
      "Llama-13b | CURRENT IDX: 925 | Length: 926\n",
      "Llama-13b | CURRENT IDX: 926 | Length: 927\n",
      "Llama-13b | CURRENT IDX: 927 | Length: 928\n",
      "Llama-13b | CURRENT IDX: 928 | Length: 929\n",
      "Llama-13b | CURRENT IDX: 929 | Length: 930\n",
      "Llama-13b | CURRENT IDX: 930 | Length: 931\n",
      "Llama-13b | CURRENT IDX: 931 | Length: 932\n",
      "Llama-13b | CURRENT IDX: 932 | Length: 933\n",
      "Llama-13b | CURRENT IDX: 933 | Length: 934\n",
      "Llama-13b | CURRENT IDX: 934 | Length: 935\n",
      "Llama-13b | CURRENT IDX: 935 | Length: 936\n",
      "Llama-13b | CURRENT IDX: 936 | Length: 937\n",
      "Llama-13b | CURRENT IDX: 937 | Length: 938\n",
      "Llama-13b | CURRENT IDX: 938 | Length: 939\n",
      "Llama-13b | CURRENT IDX: 939 | Length: 940\n",
      "Llama-13b | CURRENT IDX: 940 | Length: 941\n",
      "Llama-13b | CURRENT IDX: 941 | Length: 942\n",
      "Llama-13b | CURRENT IDX: 942 | Length: 943\n",
      "Llama-13b | CURRENT IDX: 943 | Length: 944\n",
      "Llama-13b | CURRENT IDX: 944 | Length: 945\n",
      "Llama-13b | CURRENT IDX: 945 | Length: 946\n",
      "Llama-13b | CURRENT IDX: 946 | Length: 947\n",
      "Llama-13b | CURRENT IDX: 947 | Length: 948\n",
      "Llama-13b | CURRENT IDX: 948 | Length: 949\n",
      "Llama-13b | CURRENT IDX: 949 | Length: 950\n",
      "Llama-13b | CURRENT IDX: 950 | Length: 951\n",
      "Llama-13b | CURRENT IDX: 951 | Length: 952\n",
      "Llama-13b | CURRENT IDX: 952 | Length: 953\n",
      "Llama-13b | CURRENT IDX: 953 | Length: 954\n",
      "Llama-13b | CURRENT IDX: 954 | Length: 955\n",
      "Llama-13b | CURRENT IDX: 955 | Length: 956\n",
      "Llama-13b | CURRENT IDX: 956 | Length: 957\n",
      "Llama-13b | CURRENT IDX: 957 | Length: 958\n",
      "Llama-13b | CURRENT IDX: 958 | Length: 959\n",
      "Llama-13b | CURRENT IDX: 959 | Length: 960\n",
      "Llama-13b | CURRENT IDX: 970 | Length: 971\n",
      "Llama-13b | CURRENT IDX: 971 | Length: 972\n",
      "Llama-13b | CURRENT IDX: 972 | Length: 973\n",
      "Llama-13b | CURRENT IDX: 973 | Length: 974\n",
      "Llama-13b | CURRENT IDX: 974 | Length: 975\n",
      "Llama-13b | CURRENT IDX: 975 | Length: 976\n",
      "Llama-13b | CURRENT IDX: 976 | Length: 977\n",
      "Llama-13b | CURRENT IDX: 977 | Length: 978\n",
      "Llama-13b | CURRENT IDX: 978 | Length: 979\n",
      "Llama-13b | CURRENT IDX: 979 | Length: 980\n",
      "Llama-13b | CURRENT IDX: 980 | Length: 981\n",
      "Llama-13b | CURRENT IDX: 981 | Length: 982\n",
      "Llama-13b | CURRENT IDX: 982 | Length: 983\n",
      "Llama-13b | CURRENT IDX: 983 | Length: 984\n",
      "Llama-13b | CURRENT IDX: 984 | Length: 985\n",
      "Llama-13b | CURRENT IDX: 985 | Length: 986\n",
      "Llama-13b | CURRENT IDX: 986 | Length: 987\n",
      "Llama-13b | CURRENT IDX: 987 | Length: 988\n",
      "Llama-13b | CURRENT IDX: 988 | Length: 989\n",
      "Llama-13b | CURRENT IDX: 989 | Length: 990\n",
      "Llama-13b | CURRENT IDX: 990 | Length: 991\n",
      "Llama-13b | CURRENT IDX: 991 | Length: 992\n",
      "Llama-13b | CURRENT IDX: 992 | Length: 993\n",
      "Llama-13b | CURRENT IDX: 993 | Length: 994\n",
      "Llama-13b | CURRENT IDX: 994 | Length: 995\n",
      "Llama-13b | CURRENT IDX: 995 | Length: 996\n",
      "Llama-13b | CURRENT IDX: 996 | Length: 997\n",
      "Llama-13b | CURRENT IDX: 997 | Length: 998\n",
      "Llama-13b | CURRENT IDX: 998 | Length: 999\n",
      "Llama-13b | CURRENT IDX: 999 | Length: 1000\n"
     ]
    }
   ],
   "source": [
    "# for current_idx in range(604, 1000):\n",
    "#     input_text = cnn_dailymail_dataset[current_idx]['article']\n",
    "#     output_13b = generate_output(llama13b, llama13b_tokenizer, input_text, current_idx)\n",
    "    \n",
    "#     outputs_13b.append(output_13b)\n",
    "    \n",
    "#     print(f\"Llama-13b | CURRENT IDX: {current_idx} | Length: {len(outputs_13b)}\")\n",
    "#     with open('input_output_pairs_cnn_dailymail_13b.pkl', 'wb') as f:\n",
    "#         pickle.dump(outputs_13b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcc09a71-df5a-4bfd-98db-2608440b8400",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "with open('input_output_pairs_cnn_dailymail_13b.pkl', 'rb') as f:\n",
    "    outputs_13b = pickle.load(f)\n",
    "\n",
    "print(len(outputs_13b))\n",
    "# print(outputs_13b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2f73a22-341c-49b9-9dba-db868f5fb04a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun 23 21:44:43 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  | 00000000:17:00.0 Off |                  N/A |\n",
      "| 39%   57C    P8              23W / 370W |  11258MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3070        On  | 00000000:65:00.0 Off |                  N/A |\n",
      "| 57%   61C    P8              26W / 220W |   7537MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A       919      G   /usr/lib/xorg/Xorg                            9MiB |\n",
      "|    0   N/A  N/A      1043      G   /usr/bin/gnome-shell                          6MiB |\n",
      "|    0   N/A  N/A     81037      C   /home/ryan/myenv/bin/python3               3616MiB |\n",
      "|    0   N/A  N/A     84648      C   /home/ryan/myenv/bin/python3               7608MiB |\n",
      "|    1   N/A  N/A       919      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A     84648      C   /home/ryan/myenv/bin/python3               7522MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49015124-8531-4dbc-8349-84068d82cd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_output_pairs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea46f13f-4363-4c98-ac03-b5c048bb2174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(len(cnn_dailymail_dataset)):\n",
    "#     outputs = {\n",
    "#         'input': cnn_dailymail_dataset[idx][\"article\"],\n",
    "#         'output_7b': outputs_7b[idx],\n",
    "#         'output_tiny': outputs_tiny[idx],\n",
    "#         'output_13b': outputs_13b[idx]\n",
    "#     }\n",
    "    \n",
    "#     input_output_pairs.append(outputs)\n",
    "    \n",
    "#     print(\"---------------------------------------------------------------------------\")\n",
    "#     print(f\"CURRENT IDX: {idx}\")\n",
    "#     print(f\"Length: {len(input_output_pairs)}\")\n",
    "#     # print(f\"Current Dataset: {input_output_pairs[-1]}\")\n",
    "#     with open('input_output_pairs_cnn_dailymail.pkl', 'wb') as f:\n",
    "#         pickle.dump(input_output_pairs, f)\n",
    "#     print(\"---------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40832e0c-0b2e-486e-911f-c4a5ab593e5f",
   "metadata": {},
   "source": [
    "### **GSM8K (Math)**\n",
    "is a dataset of 8.5K high quality linguistically diverse grade school math word problems. The dataset was created to support the task of question answering on basic mathematical problems that require multi-step reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "612c579d-6007-4845-a252-6d8271f6d81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "gsm8k_dataset = load_dataset('openai/gsm8k', 'main', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74248079-e029-48b7-8993-0260ddd26ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = gsm8k_dataset[0]['question'] \n",
    "input_prompt = \"Answer the following math question: \\n\\n\" + input_text + \"\\n\\n Lets think step by step: \"\n",
    "\n",
    "inputs = tinyllama_tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d7178d2-9851-42ad-aec1-15f471eabe42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Answer the following math question: \n",
      "\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "\n",
      " Lets think step by step: \n",
      "Output: Answer the following math question: \n",
      "\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "\n",
      " Lets think step by step: \n",
      "\n",
      "1. Natalia sold clips to 48 of her friends in April.\n",
      "2. She sold half as many clips in May.\n",
      "3. She sold 48 clips in April and 24 clips in May.\n",
      "4. She sold 72 clips in total.\n",
      "\n",
      "So, the answer is: 72 clips.\n"
     ]
    }
   ],
   "source": [
    "output = tinyllama.generate(inputs['input_ids'])\n",
    "\n",
    "output_text = tinyllama_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input: {input_prompt}\")\n",
    "print(f\"Output: {output_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca26d07f-768f-4764-bf57-836b516a0be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Natalia sold clips to 48 of her friends in April.\n",
      "2. She sold half as many clips in May.\n",
      "3. She sold 48 clips in April and 24 clips in May.\n",
      "4. She sold 72 clips in total.\n",
      "\n",
      "So, the answer is: 72 clips.\n"
     ]
    }
   ],
   "source": [
    "answer_prefix = \"Lets think step by step: \"\n",
    "if answer_prefix in output_text:\n",
    "    cleaned_output = output_text.split(answer_prefix)[-1].strip()\n",
    "else:\n",
    "    cleaned_output = output_text.strip()\n",
    "\n",
    "print(cleaned_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9b9aafb-e828-4b0c-babc-feb38150621d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points in different token ranges:\n",
      "0-50: 2041\n",
      "51-100: 4718\n",
      "101-150: 667\n",
      "151-200: 44\n",
      "201-250: 3\n",
      "\n",
      "Data point with the most tokens is at index: 3331\n",
      "Number of tokens: 240\n",
      "Input text: Hasan is packing up his apartment because he’s moving across the country for a new job. He needs to ship several boxes to his new home. The movers have asked that Hasan avoid putting more than a certain weight in pounds in any cardboard box. The moving company has helpfully provided Hasan with a digital scale that will alert him if a package is too heavy. Hasan is in the kitchen, and he fills a cardboard box with 38 dinner plates. When he checks the box, the scale reports his box is too heavy. Hasan knows each of his plates weighs 10 ounces. He removes a single plate from the box and checks the movers’ scale again. The scale reports his box is still too heavy. Hasan repeats the process again and again. When he has removed enough plates, the movers’ scale shows the box is now an acceptable weight for shipping. Hasan deduces that each shipping box can hold 20 pounds before the scale says the box is too heavy.  How many plates did Hasan need to remove from the shipping box?\n"
     ]
    }
   ],
   "source": [
    "token_ranges = {\n",
    "    '0-50': 0,\n",
    "    '51-100': 0,\n",
    "    '101-150': 0,\n",
    "    '151-200': 0,\n",
    "    '201-250': 0\n",
    "}\n",
    "\n",
    "max_tokens = -1\n",
    "\n",
    "for idx, data in enumerate(gsm8k_dataset):\n",
    "    input_text = data['question']\n",
    "    tokens = llama7b_tokenizer(input_text, return_tensors=\"pt\")\n",
    "    num_tokens = len(tokens['input_ids'][0])\n",
    "    \n",
    "    if num_tokens > max_tokens:\n",
    "        max_tokens = num_tokens\n",
    "        max_tokens_idx = idx\n",
    "    \n",
    "    if num_tokens <= 50:\n",
    "        token_ranges['0-50'] += 1\n",
    "    elif num_tokens <= 100:\n",
    "        token_ranges['51-100'] += 1\n",
    "    elif num_tokens <= 150:\n",
    "        token_ranges['101-150'] += 1\n",
    "    elif num_tokens <= 200:\n",
    "        token_ranges['151-200'] += 1\n",
    "    elif num_tokens <= 250:\n",
    "        token_ranges['201-250'] += 1\n",
    "\n",
    "print(\"Number of data points in different token ranges:\")\n",
    "for key, value in token_ranges.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(f\"\\nData point with the most tokens is at index: {max_tokens_idx}\")\n",
    "print(f\"Number of tokens: {max_tokens}\")\n",
    "print(f\"Input text: {gsm8k_dataset[max_tokens_idx]['question']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d36c56d2-9596-49e3-8cf4-42de3a2cd295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(model, tokenizer, dataset, current_idx):\n",
    "    outputs = []\n",
    "    \n",
    "    input_text = gsm8k_dataset[current_idx]['question'] \n",
    "    input_prompt = \"Answer the following math question: \\n\\n\" + input_text + \"\\n\\n Lets think step by step: \"\n",
    "\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "    output = model.generate(inputs['input_ids'])\n",
    "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    answer_prefix = \"Lets think step by step: \"\n",
    "    if answer_prefix in output_text:\n",
    "        cleaned_output = output_text.split(answer_prefix)[-1].strip()\n",
    "    else:\n",
    "        cleaned_output = output_text.strip()\n",
    "\n",
    "    outputs.append(cleaned_output)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc18749e-3245-4e06-842d-f08cfc0cfd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "# outputs_7b = []\n",
    "# outputs_tiny = []\n",
    "# outputs_13b = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fed70679-1f84-4bdd-96cc-29d9aa38bb78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-7b | CURRENT IDX: 948 | Length: 949\n",
      "Llama-7b | CURRENT IDX: 949 | Length: 950\n",
      "Llama-7b | CURRENT IDX: 950 | Length: 951\n",
      "Llama-7b | CURRENT IDX: 951 | Length: 952\n",
      "Llama-7b | CURRENT IDX: 952 | Length: 953\n",
      "Llama-7b | CURRENT IDX: 953 | Length: 954\n",
      "Llama-7b | CURRENT IDX: 954 | Length: 955\n",
      "Llama-7b | CURRENT IDX: 955 | Length: 956\n",
      "Llama-7b | CURRENT IDX: 956 | Length: 957\n",
      "Llama-7b | CURRENT IDX: 957 | Length: 958\n",
      "Llama-7b | CURRENT IDX: 958 | Length: 959\n",
      "Llama-7b | CURRENT IDX: 959 | Length: 960\n",
      "Llama-7b | CURRENT IDX: 960 | Length: 961\n",
      "Llama-7b | CURRENT IDX: 961 | Length: 962\n",
      "Llama-7b | CURRENT IDX: 962 | Length: 963\n",
      "Llama-7b | CURRENT IDX: 963 | Length: 964\n",
      "Llama-7b | CURRENT IDX: 964 | Length: 965\n",
      "Llama-7b | CURRENT IDX: 965 | Length: 966\n",
      "Llama-7b | CURRENT IDX: 966 | Length: 967\n",
      "Llama-7b | CURRENT IDX: 967 | Length: 968\n",
      "Llama-7b | CURRENT IDX: 968 | Length: 969\n",
      "Llama-7b | CURRENT IDX: 969 | Length: 970\n",
      "Llama-7b | CURRENT IDX: 970 | Length: 971\n",
      "Llama-7b | CURRENT IDX: 971 | Length: 972\n",
      "Llama-7b | CURRENT IDX: 972 | Length: 973\n",
      "Llama-7b | CURRENT IDX: 973 | Length: 974\n",
      "Llama-7b | CURRENT IDX: 974 | Length: 975\n",
      "Llama-7b | CURRENT IDX: 975 | Length: 976\n",
      "Llama-7b | CURRENT IDX: 976 | Length: 977\n",
      "Llama-7b | CURRENT IDX: 977 | Length: 978\n",
      "Llama-7b | CURRENT IDX: 978 | Length: 979\n",
      "Llama-7b | CURRENT IDX: 979 | Length: 980\n",
      "Llama-7b | CURRENT IDX: 980 | Length: 981\n",
      "Llama-7b | CURRENT IDX: 981 | Length: 982\n",
      "Llama-7b | CURRENT IDX: 982 | Length: 983\n",
      "Llama-7b | CURRENT IDX: 983 | Length: 984\n",
      "Llama-7b | CURRENT IDX: 984 | Length: 985\n",
      "Llama-7b | CURRENT IDX: 985 | Length: 986\n",
      "Llama-7b | CURRENT IDX: 986 | Length: 987\n",
      "Llama-7b | CURRENT IDX: 987 | Length: 988\n",
      "Llama-7b | CURRENT IDX: 988 | Length: 989\n",
      "Llama-7b | CURRENT IDX: 989 | Length: 990\n",
      "Llama-7b | CURRENT IDX: 990 | Length: 991\n",
      "Llama-7b | CURRENT IDX: 991 | Length: 992\n",
      "Llama-7b | CURRENT IDX: 992 | Length: 993\n",
      "Llama-7b | CURRENT IDX: 993 | Length: 994\n",
      "Llama-7b | CURRENT IDX: 994 | Length: 995\n",
      "Llama-7b | CURRENT IDX: 995 | Length: 996\n",
      "Llama-7b | CURRENT IDX: 996 | Length: 997\n",
      "Llama-7b | CURRENT IDX: 997 | Length: 998\n",
      "Llama-7b | CURRENT IDX: 998 | Length: 999\n",
      "Llama-7b | CURRENT IDX: 999 | Length: 1000\n"
     ]
    }
   ],
   "source": [
    "for current_idx in range(948, 1000):\n",
    "    input_text = gsm8k_dataset[current_idx]['question']\n",
    "    output_7b = generate_output(llama7b, llama7b_tokenizer, input_text, current_idx)\n",
    "\n",
    "    outputs_7b.append(output_7b)\n",
    "    \n",
    "    print(f\"Llama-7b | CURRENT IDX: {current_idx} | Length: {len(outputs_7b)}\")\n",
    "    # with open('input_output_pairs_gsm8k_7b', 'wb') as f:\n",
    "    #     pickle.dump(outputs_7b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b19b43f3-e5ef-4308-9893-6add3ab0433e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyLlama | CURRENT IDX: 957 | Length: 958\n",
      "TinyLlama | CURRENT IDX: 958 | Length: 959\n",
      "TinyLlama | CURRENT IDX: 959 | Length: 960\n",
      "TinyLlama | CURRENT IDX: 960 | Length: 961\n",
      "TinyLlama | CURRENT IDX: 961 | Length: 962\n",
      "TinyLlama | CURRENT IDX: 962 | Length: 963\n",
      "TinyLlama | CURRENT IDX: 963 | Length: 964\n",
      "TinyLlama | CURRENT IDX: 964 | Length: 965\n",
      "TinyLlama | CURRENT IDX: 965 | Length: 966\n",
      "TinyLlama | CURRENT IDX: 966 | Length: 967\n",
      "TinyLlama | CURRENT IDX: 967 | Length: 968\n",
      "TinyLlama | CURRENT IDX: 968 | Length: 969\n",
      "TinyLlama | CURRENT IDX: 969 | Length: 970\n",
      "TinyLlama | CURRENT IDX: 970 | Length: 971\n",
      "TinyLlama | CURRENT IDX: 971 | Length: 972\n",
      "TinyLlama | CURRENT IDX: 972 | Length: 973\n",
      "TinyLlama | CURRENT IDX: 973 | Length: 974\n",
      "TinyLlama | CURRENT IDX: 974 | Length: 975\n",
      "TinyLlama | CURRENT IDX: 975 | Length: 976\n",
      "TinyLlama | CURRENT IDX: 976 | Length: 977\n",
      "TinyLlama | CURRENT IDX: 977 | Length: 978\n",
      "TinyLlama | CURRENT IDX: 978 | Length: 979\n",
      "TinyLlama | CURRENT IDX: 979 | Length: 980\n",
      "TinyLlama | CURRENT IDX: 980 | Length: 981\n",
      "TinyLlama | CURRENT IDX: 981 | Length: 982\n",
      "TinyLlama | CURRENT IDX: 982 | Length: 983\n",
      "TinyLlama | CURRENT IDX: 983 | Length: 984\n",
      "TinyLlama | CURRENT IDX: 984 | Length: 985\n",
      "TinyLlama | CURRENT IDX: 985 | Length: 986\n",
      "TinyLlama | CURRENT IDX: 986 | Length: 987\n",
      "TinyLlama | CURRENT IDX: 987 | Length: 988\n",
      "TinyLlama | CURRENT IDX: 988 | Length: 989\n",
      "TinyLlama | CURRENT IDX: 989 | Length: 990\n",
      "TinyLlama | CURRENT IDX: 990 | Length: 991\n",
      "TinyLlama | CURRENT IDX: 991 | Length: 992\n",
      "TinyLlama | CURRENT IDX: 992 | Length: 993\n",
      "TinyLlama | CURRENT IDX: 993 | Length: 994\n",
      "TinyLlama | CURRENT IDX: 994 | Length: 995\n",
      "TinyLlama | CURRENT IDX: 995 | Length: 996\n",
      "TinyLlama | CURRENT IDX: 996 | Length: 997\n",
      "TinyLlama | CURRENT IDX: 997 | Length: 998\n",
      "TinyLlama | CURRENT IDX: 998 | Length: 999\n",
      "TinyLlama | CURRENT IDX: 999 | Length: 1000\n"
     ]
    }
   ],
   "source": [
    "for current_idx in range(957, 1000):\n",
    "    input_text = gsm8k_dataset[current_idx]['question']\n",
    "    output_tiny = generate_output(tinyllama, tinyllama_tokenizer, input_text, current_idx)\n",
    "\n",
    "    outputs_tiny.append(output_tiny)\n",
    "    \n",
    "    print(f\"TinyLlama | CURRENT IDX: {current_idx} | Length: {len(outputs_tiny)}\")\n",
    "    # with open('input_output_pairs_gsm8k_tiny', 'wb') as f:\n",
    "    #     pickle.dump(outputs_tiny, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a92f60e-8220-4075-9026-527a0bbdd63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "[\"1. Miss Grayson's class raised $50 for their field trip.  2. Each student contributed $5 each.  3. The cost of the trip is $7 for each student.  4. After all the field trip costs were paid, how much is left in Miss Grayson's class fund?   The answer is: $43.50.   Remember, the class fund is the amount of money that remains after all the field trip costs are paid.\"]\n"
     ]
    }
   ],
   "source": [
    "with open('input_output_pairs_gsm8k_tiny', 'rb') as f:\n",
    "    outputs_tiny = pickle.load(f)\n",
    "\n",
    "print(len(outputs_tiny))\n",
    "print(outputs_tiny[999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62a8876a-5336-4ec1-99b4-221d566cd8d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama13b | CURRENT IDX: 899 | Length: 900\n",
      "Llama13b | CURRENT IDX: 900 | Length: 901\n",
      "Llama13b | CURRENT IDX: 901 | Length: 902\n",
      "Llama13b | CURRENT IDX: 902 | Length: 903\n",
      "Llama13b | CURRENT IDX: 903 | Length: 904\n",
      "Llama13b | CURRENT IDX: 904 | Length: 905\n",
      "Llama13b | CURRENT IDX: 905 | Length: 906\n",
      "Llama13b | CURRENT IDX: 906 | Length: 907\n",
      "Llama13b | CURRENT IDX: 907 | Length: 908\n",
      "Llama13b | CURRENT IDX: 908 | Length: 909\n",
      "Llama13b | CURRENT IDX: 909 | Length: 910\n",
      "Llama13b | CURRENT IDX: 910 | Length: 911\n",
      "Llama13b | CURRENT IDX: 911 | Length: 912\n",
      "Llama13b | CURRENT IDX: 912 | Length: 913\n",
      "Llama13b | CURRENT IDX: 913 | Length: 914\n",
      "Llama13b | CURRENT IDX: 914 | Length: 915\n",
      "Llama13b | CURRENT IDX: 915 | Length: 916\n",
      "Llama13b | CURRENT IDX: 916 | Length: 917\n",
      "Llama13b | CURRENT IDX: 917 | Length: 918\n",
      "Llama13b | CURRENT IDX: 918 | Length: 919\n",
      "Llama13b | CURRENT IDX: 919 | Length: 920\n",
      "Llama13b | CURRENT IDX: 920 | Length: 921\n",
      "Llama13b | CURRENT IDX: 921 | Length: 922\n",
      "Llama13b | CURRENT IDX: 922 | Length: 923\n",
      "Llama13b | CURRENT IDX: 923 | Length: 924\n",
      "Llama13b | CURRENT IDX: 924 | Length: 925\n",
      "Llama13b | CURRENT IDX: 925 | Length: 926\n",
      "Llama13b | CURRENT IDX: 926 | Length: 927\n",
      "Llama13b | CURRENT IDX: 927 | Length: 928\n",
      "Llama13b | CURRENT IDX: 928 | Length: 929\n",
      "Llama13b | CURRENT IDX: 929 | Length: 930\n",
      "Llama13b | CURRENT IDX: 930 | Length: 931\n",
      "Llama13b | CURRENT IDX: 931 | Length: 932\n",
      "Llama13b | CURRENT IDX: 932 | Length: 933\n",
      "Llama13b | CURRENT IDX: 933 | Length: 934\n",
      "Llama13b | CURRENT IDX: 934 | Length: 935\n",
      "Llama13b | CURRENT IDX: 935 | Length: 936\n",
      "Llama13b | CURRENT IDX: 936 | Length: 937\n",
      "Llama13b | CURRENT IDX: 937 | Length: 938\n",
      "Llama13b | CURRENT IDX: 938 | Length: 939\n",
      "Llama13b | CURRENT IDX: 939 | Length: 940\n",
      "Llama13b | CURRENT IDX: 940 | Length: 941\n",
      "Llama13b | CURRENT IDX: 941 | Length: 942\n",
      "Llama13b | CURRENT IDX: 942 | Length: 943\n",
      "Llama13b | CURRENT IDX: 943 | Length: 944\n",
      "Llama13b | CURRENT IDX: 944 | Length: 945\n",
      "Llama13b | CURRENT IDX: 945 | Length: 946\n",
      "Llama13b | CURRENT IDX: 946 | Length: 947\n",
      "Llama13b | CURRENT IDX: 947 | Length: 948\n",
      "Llama13b | CURRENT IDX: 948 | Length: 949\n",
      "Llama13b | CURRENT IDX: 949 | Length: 950\n",
      "Llama13b | CURRENT IDX: 950 | Length: 951\n",
      "Llama13b | CURRENT IDX: 951 | Length: 952\n",
      "Llama13b | CURRENT IDX: 952 | Length: 953\n",
      "Llama13b | CURRENT IDX: 953 | Length: 954\n",
      "Llama13b | CURRENT IDX: 954 | Length: 955\n",
      "Llama13b | CURRENT IDX: 955 | Length: 956\n",
      "Llama13b | CURRENT IDX: 956 | Length: 957\n",
      "Llama13b | CURRENT IDX: 957 | Length: 958\n",
      "Llama13b | CURRENT IDX: 958 | Length: 959\n",
      "Llama13b | CURRENT IDX: 959 | Length: 960\n",
      "Llama13b | CURRENT IDX: 960 | Length: 961\n",
      "Llama13b | CURRENT IDX: 961 | Length: 962\n",
      "Llama13b | CURRENT IDX: 962 | Length: 963\n",
      "Llama13b | CURRENT IDX: 963 | Length: 964\n",
      "Llama13b | CURRENT IDX: 964 | Length: 965\n",
      "Llama13b | CURRENT IDX: 965 | Length: 966\n",
      "Llama13b | CURRENT IDX: 966 | Length: 967\n",
      "Llama13b | CURRENT IDX: 967 | Length: 968\n",
      "Llama13b | CURRENT IDX: 968 | Length: 969\n",
      "Llama13b | CURRENT IDX: 969 | Length: 970\n",
      "Llama13b | CURRENT IDX: 970 | Length: 971\n",
      "Llama13b | CURRENT IDX: 971 | Length: 972\n",
      "Llama13b | CURRENT IDX: 972 | Length: 973\n",
      "Llama13b | CURRENT IDX: 973 | Length: 974\n",
      "Llama13b | CURRENT IDX: 974 | Length: 975\n",
      "Llama13b | CURRENT IDX: 975 | Length: 976\n",
      "Llama13b | CURRENT IDX: 976 | Length: 977\n",
      "Llama13b | CURRENT IDX: 977 | Length: 978\n",
      "Llama13b | CURRENT IDX: 978 | Length: 979\n",
      "Llama13b | CURRENT IDX: 979 | Length: 980\n",
      "Llama13b | CURRENT IDX: 980 | Length: 981\n",
      "Llama13b | CURRENT IDX: 981 | Length: 982\n",
      "Llama13b | CURRENT IDX: 982 | Length: 983\n",
      "Llama13b | CURRENT IDX: 983 | Length: 984\n",
      "Llama13b | CURRENT IDX: 984 | Length: 985\n",
      "Llama13b | CURRENT IDX: 985 | Length: 986\n",
      "Llama13b | CURRENT IDX: 986 | Length: 987\n",
      "Llama13b | CURRENT IDX: 987 | Length: 988\n",
      "Llama13b | CURRENT IDX: 988 | Length: 989\n",
      "Llama13b | CURRENT IDX: 989 | Length: 990\n",
      "Llama13b | CURRENT IDX: 990 | Length: 991\n",
      "Llama13b | CURRENT IDX: 991 | Length: 992\n",
      "Llama13b | CURRENT IDX: 992 | Length: 993\n",
      "Llama13b | CURRENT IDX: 993 | Length: 994\n",
      "Llama13b | CURRENT IDX: 994 | Length: 995\n",
      "Llama13b | CURRENT IDX: 995 | Length: 996\n",
      "Llama13b | CURRENT IDX: 996 | Length: 997\n",
      "Llama13b | CURRENT IDX: 997 | Length: 998\n",
      "Llama13b | CURRENT IDX: 998 | Length: 999\n",
      "Llama13b | CURRENT IDX: 999 | Length: 1000\n"
     ]
    }
   ],
   "source": [
    "for current_idx in range(899, 1000):\n",
    "    input_text = gsm8k_dataset[current_idx]['question']\n",
    "    output_13b = generate_output(llama13b, llama13b_tokenizer, input_text, current_idx)\n",
    "\n",
    "    outputs_13b.append(output_13b)\n",
    "    \n",
    "    print(f\"Llama13b | CURRENT IDX: {current_idx} | Length: {len(outputs_13b)}\")\n",
    "    # with open('input_output_pairs_gsm8k_13b', 'wb') as f:\n",
    "    #     pickle.dump(outputs_13b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee4d3f79-e33f-48b4-bd9f-6675badf27fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "[\"Step 1: Let's find out how many popsicle sticks Sam has.  Step 2: Let's find out how many popsicle sticks Sid has.  Step 3: Let's find out how many popsicle sticks Steve has.  Step 4: Let's add up all the popsicle sticks to find out the total number of popsicle sticks they have.  Step 5: Let's find out how many popsicle sticks they can use for their Art class activity.  So, let's start!  Step 1: Sam has thrice as many popsicle sticks as Sid. That means Sam has 3x as many popsicle sticks as Sid.  Step 2: Sid has twice as many popsicle sticks as Steve. That means Sid has 2x as many popsicle sticks as Steve.  Step 3: Steve has 12 popsicle sticks.  Now, let's add up all the popsicle sticks:  Sam has 3x as many popsicle sticks as Sid, so Sam has 3x 12 = 36 popsicle sticks.  Sid has 2x as many popsicle sticks as Steve, so Sid has 2x 12 = 24 popsicle sticks.  Total number of popsicle sticks = 36 + 24 = 60 popsicle sticks.  Step 5: They can use 60 popsicle sticks for their Art class activity.  Therefore, the answer to the question is 60 popsicle sticks.\"]\n"
     ]
    }
   ],
   "source": [
    "with open('input_output_pairs_gsm8k_13b', 'rb') as f:\n",
    "    outputs_13b = pickle.load(f)\n",
    "\n",
    "print(len(outputs_13b))\n",
    "print(outputs_13b[831])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dcd8dda6-63ae-47c9-999b-bd977d523e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(outputs_7b))\n",
    "print(len(outputs_tiny))\n",
    "print(len(outputs_13b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2c270a-7afb-4324-bdd8-106cd35706a7",
   "metadata": {},
   "source": [
    "Remove the \\n from the 3 output arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68434cac-3a15-4935-928b-8392082c60a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_7b = [[output.replace('\\n', ' ') for output in output_list] for output_list in outputs_7b]\n",
    "outputs_tiny = [[output.replace('\\n', ' ') for output in output_list] for output_list in outputs_tiny]\n",
    "outputs_13b = [[output.replace('\\n', ' ') for output in output_list] for output_list in outputs_13b]\n",
    "\n",
    "# with open('input_output_pairs_gsm8k_7b', 'wb') as f:\n",
    "#     pickle.dump(outputs_7b, f)\n",
    "# with open('input_output_pairs_gsm8k_tiny', 'wb') as f:\n",
    "#     pickle.dump(outputs_tiny, f)\n",
    "# with open('input_output_pairs_gsm8k_13b', 'wb') as f:\n",
    "#     pickle.dump(outputs_13b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03673b4a-191a-4c3d-ba0d-44d6181c11f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_output_pairs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea121d07-6684-4e95-97c5-b672dbed2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(outputs_7b)):\n",
    "    outputs = {\n",
    "        'input': gsm8k_dataset[idx][\"question\"],\n",
    "        'output_7b': outputs_7b[idx],\n",
    "        'output_tiny': outputs_tiny[idx],\n",
    "        'output_13b': outputs_13b[idx]\n",
    "    }\n",
    "    \n",
    "    input_output_pairs.append(outputs)\n",
    "\n",
    "# with open('input_output_pairs_gsm8k.pkl', 'wb') as f:\n",
    "#     pickle.dump(input_output_pairs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d9262b-1247-4a84-8855-e278da35c48e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "94fc2af8-507f-4d8f-bf94-81d930e9877e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_output_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfef1deb-b2d5-4088-a39b-c069ea755712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
