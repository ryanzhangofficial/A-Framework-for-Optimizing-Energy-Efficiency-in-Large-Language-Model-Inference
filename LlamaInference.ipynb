{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcac6095-37d9-4508-805e-aa190de4a224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch ipywidgets sentence_transformers matplotlib nltk bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8c32c9-dbcd-401d-b9c6-7caecdad8317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995df5ec-914f-44e0-8f47-f4178edd9232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d622bd4-6223-43e9-a191-3f499b0354fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8bcbb0-25b0-4b42-8bc9-3607d7d6bb64",
   "metadata": {},
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0045c0b-2dfd-4044-a56e-591ff3017b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a493bcb-fb76-4e61-996c-f4237d8fa5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad198ed-734f-4f0c-90d4-55dbcaf61c54",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c74653-3188-48ad-a901-1ebd6296aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2a7502-f859-40a5-b3b9-fcad071c07b6",
   "metadata": {},
   "source": [
    "Llama7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bea5f32-1cb1-4893-8a56-06175223f555",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "llama7b_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llama7b = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                               torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11e79aa-7da1-4745-a3b4-9eaa5b48ae0b",
   "metadata": {},
   "source": [
    "TinyLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073e3e3d-3950-4467-a4f5-c46aef74121b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
    "\n",
    "tinyllama_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tinyllama = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                 torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f42151-3491-4838-a518-7a86d0003126",
   "metadata": {},
   "source": [
    "Llama13b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119fe424-aeba-4652-b1e5-911dcafe9bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama13_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                    bnb_4bit_compute_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b69b5a3-1066-4369-9ebc-037674f3e1e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "\n",
    "llama13b_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llama13b = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                device_map='auto',\n",
    "                                                quantization_config=llama13_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556c99dc-6b86-4e0e-b655-d9b02a5a77d2",
   "metadata": {},
   "source": [
    "### Basic Model Loading + Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acb3914-3d72-42cd-a5e0-e396f5429951",
   "metadata": {},
   "source": [
    "#### Llama 7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b92648-b804-4757-9813-94d533fb7f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "input_text = \"Once upon a time in a land far, far away\"\n",
    "inputs = llama7b_tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "output = llama7b.generate(inputs['input_ids'], max_length=100)\n",
    "\n",
    "output_text = llama7b_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output_text)\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "print()\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ada08d8-9b4e-46b8-bee5-280d2217fd17",
   "metadata": {},
   "source": [
    "#### TinyLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada3b66-3cc4-4d71-a014-c55b610ecfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "input_text = \"Once upon a time in a land far, far away\"\n",
    "inputs = tinyllama_tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "output = tinyllama.generate(inputs['input_ids'], max_length=100)\n",
    "\n",
    "output_text = tinyllama_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output_text)\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "print()\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9da26a-875d-42e1-a3e5-b5680e16054d",
   "metadata": {},
   "source": [
    "#### Llama 13b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a310f942-ad56-421c-a3a0-7a0dc803825e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "input_text = \"Once upon a time in a land far, far away\"\n",
    "inputs = llama13b_tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "output = llama13b.generate(inputs['input_ids'], max_length=100)\n",
    "\n",
    "output_text = llama13b_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output_text)\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "print()\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd5e40f-ccab-4a71-98e6-c3aebcd0858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a9f561-ba57-4ba1-8f5f-ac5a5d9475bf",
   "metadata": {},
   "source": [
    "### **WMT 2014 (Machine Translation)** \n",
    "is a collection of datasets used in shared tasks of the Ninth Workshop on Statistical Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ea7983-d95a-4f39-9017-7baae54dc8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wmt14_dataset = load_dataset('wmt14', 'de-en', split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a458a1af-7109-4c22-a3c3-0e2412ab7538",
   "metadata": {},
   "source": [
    "#### Example inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033965fd-4a81-48c5-b12c-d26b030dc6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = wmt14_dataset[0]['translation']['en']  \n",
    "input_prompt = f\"Translate to English: {input_text}\"\n",
    "\n",
    "inputs = llama7b_tokenizer(input_prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df2231-71ad-4eed-9aab-3e3ce34b8d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llama7b.generate(inputs['input_ids'], max_length=50)\n",
    "\n",
    "output_text = llama7b_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b6fbde-bcae-4a8e-a519-06305470d524",
   "metadata": {},
   "source": [
    "#### Llama 7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4f6857-42da-4e3e-8d5d-de0684612d46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_examples = 5 \n",
    "\n",
    "state_time = time.time()\n",
    "for i in range(num_examples):\n",
    "    input_text = wmt14_dataset[i]['translation']['de']\n",
    "    input_prompt = f\"Translate to English: {input_text}\"\n",
    "    \n",
    "    inputs = llama7b_tokenizer(input_prompt, return_tensors=\"pt\", truncation=True, max_length=50)\n",
    "    output = llama7b.generate(inputs['input_ids'], max_new_tokens=50)\n",
    "    output_text = llama7b_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"Input: {input_prompt}\")\n",
    "    print(f\"Output: {output_text}\")\n",
    "    print()\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "print()\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9107d737-f3ff-4004-ba17-6c40523cbcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e70660-33a1-4f3a-83db-a2cffea3b2b5",
   "metadata": {},
   "source": [
    "#### Tiny Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf1fdc4-47b6-4b1a-872c-0ded38b4c2d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_examples = 5 \n",
    "\n",
    "state_time = time.time()\n",
    "for i in range(num_examples):\n",
    "    input_text = wmt14_dataset[i]['translation']['de']\n",
    "    input_prompt = f\"Translate to English: {input_text}\"\n",
    "    \n",
    "    inputs = tinyllama_tokenizer(input_prompt, return_tensors=\"pt\", truncation=True, max_length=50)\n",
    "    output = tinyllama.generate(inputs['input_ids'], max_new_tokens=50)\n",
    "    output_text = tinyllama_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"Input: {input_prompt}\")\n",
    "    print(f\"Output: {output_text}\")\n",
    "    print()\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "print()\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79237f29-9453-463c-98a1-78a1ece9b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887106ac-190d-4f02-a3e0-5e60b2242c8a",
   "metadata": {},
   "source": [
    "#### Llama 13b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dd4c5a-42c8-4f3e-82c9-4058fe252b8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_examples = 5 \n",
    "\n",
    "state_time = time.time()\n",
    "for i in range(num_examples):\n",
    "    input_text = wmt14_dataset[i]['translation']['de']\n",
    "    input_prompt = f\"Translate to English: {input_text}\"\n",
    "    \n",
    "    inputs = llama13b_tokenizer(input_prompt, return_tensors=\"pt\", truncation=True, max_length=50)\n",
    "    output = llama13b.generate(inputs['input_ids'], max_new_tokens=50)\n",
    "    output_text = llama13b_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"Input: {input_prompt}\")\n",
    "    print(f\"Output: {output_text}\")\n",
    "    print()\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "print()\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d4ac2d-b268-4749-879f-a0d49be38ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4746b4-6ec3-44d4-a5c2-08310eeca7a3",
   "metadata": {},
   "source": [
    "#### Functionalize Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e1268b-ff84-4f21-88db-f29db0b3bde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ranges = {\n",
    "    '0-50': 0,\n",
    "    '51-100': 0,\n",
    "    '101-150': 0\n",
    "}\n",
    "\n",
    "max_tokens = -1\n",
    "\n",
    "for idx, data in enumerate(wmt14_dataset):\n",
    "    input_text = data['translation']['de']\n",
    "    tokens = llama13b_tokenizer(input_text, return_tensors=\"pt\")\n",
    "    num_tokens = len(tokens['input_ids'][0])\n",
    "    \n",
    "    if num_tokens > max_tokens:\n",
    "        max_tokens = num_tokens\n",
    "        max_tokens_idx = idx\n",
    "    \n",
    "    if num_tokens <= 50:\n",
    "        token_ranges['0-50'] += 1\n",
    "    elif num_tokens <= 100:\n",
    "        token_ranges['51-100'] += 1\n",
    "    elif num_tokens <= 150:\n",
    "        token_ranges['101-150'] += 1\n",
    "\n",
    "print(\"Number of data points in different token ranges:\")\n",
    "for key, value in token_ranges.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(f\"\\nData point with the most tokens is at index: {max_tokens_idx}\")\n",
    "print(f\"Number of tokens: {max_tokens}\")\n",
    "print(f\"Input text: {wmt14_dataset[max_tokens_idx]['translation']['de']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d739df1-1344-45f5-9b19-f6359f1d8904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(model, tokenizer, dataset, current_idx):\n",
    "    outputs = []\n",
    "    \n",
    "    input_text = wmt14_dataset[current_idx]['translation']['de']\n",
    "    input_prompt = \"Translate the sentence from German to English: \\n\\n\" + input_text + \"\\n\\n Write the translation here: \"\n",
    "\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "    output = model.generate(inputs['input_ids'])\n",
    "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    answer_prefix = \"Write the translation here: \"\n",
    "    if answer_prefix in output_text:\n",
    "        cleaned_output = output_text.split(answer_prefix)[-1].strip()\n",
    "    else:\n",
    "        cleaned_output = output_text.strip()\n",
    "\n",
    "    first_sentence = cleaned_output.split('.')[0] + '.' if '.' in cleaned_output else cleaned_output\n",
    "    outputs.append(first_sentence)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583b2579-af14-4bef-b356-4d42e059f662",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f50e32-583b-4f30-b13e-733b9ce2f949",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "outputs_7b = []\n",
    "outputs_tiny = []\n",
    "outputs_13b = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caea6a20-681a-4f4e-afa4-2b381dce00ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for current_idx in range(396, 1000):\n",
    "    input_text = wmt14_dataset[current_idx]['translation']['de']\n",
    "    output_7b = generate_output(llama7b, llama7b_tokenizer, input_text, current_idx)\n",
    "\n",
    "    outputs_7b.append(output_7b)\n",
    "    \n",
    "    print(f\"Llama-7b | CURRENT IDX: {current_idx} | Length: {len(outputs_7b)}\")\n",
    "    # with open('input_output_pairs_wmt14_7b', 'wb') as f:\n",
    "    #     pickle.dump(outputs_7b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33977506-402a-4b86-a6b7-5750f58dcc89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('input_output_pairs_wmt14_7b', 'rb') as f:\n",
    "    outputs_7b = pickle.load(f)\n",
    "\n",
    "print(len(outputs_7b))\n",
    "print(outputs_7b[998:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caf86b3-83c9-4df6-bed6-45fd16543919",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for current_idx in range(0, 1000):\n",
    "    input_text = wmt14_dataset[current_idx]['translation']['de']\n",
    "    output_tiny = generate_output(tinyllama, tinyllama_tokenizer, input_text, current_idx)\n",
    "\n",
    "    outputs_tiny.append(output_tiny)\n",
    "    \n",
    "    print(f\"TinyLlama | CURRENT IDX: {current_idx} | Length: {len(outputs_tiny)}\")\n",
    "    # with open('input_output_pairs_wmt14_tiny', 'wb') as f:\n",
    "    #     pickle.dump(outputs_tiny, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475d096e-e042-44c6-90c6-3090757ae5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input_output_pairs_wmt14_tiny', 'rb') as f:\n",
    "    outputs_tiny = pickle.load(f)\n",
    "\n",
    "print(len(outputs_tiny))\n",
    "print(outputs_tiny[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b8adfc-0d88-44a1-b91c-4a2fc9809f88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for current_idx in range(980, 1000):\n",
    "    input_text = wmt14_dataset[current_idx]['translation']['de']\n",
    "    output_13b = generate_output(llama13b, llama13b_tokenizer, input_text, current_idx)\n",
    "\n",
    "    outputs_13b.append(output_13b)\n",
    "    \n",
    "    print(f\"Llama13b | CURRENT IDX: {current_idx} | Length: {len(outputs_13b)}\")\n",
    "    # with open('input_output_pairs_wmt14_13b', 'wb') as f:\n",
    "    #     pickle.dump(outputs_13b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bdfc17-2c40-4a57-b5fd-0cc8f6d62a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input_output_pairs_wmt14_13b', 'rb') as f:\n",
    "    outputs_13b = pickle.load(f)\n",
    "\n",
    "print(len(outputs_13b))\n",
    "print(outputs_13b[999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466874b1-0d79-42be-b66a-c7ffcb261dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_output_pairs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6c3260-13ed-400c-bf83-c357fafa4e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmt14_dataset[500]['translation']['de']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35935e5-2fe9-4b69-95f2-f66fcd63c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(outputs_7b)):\n",
    "    outputs = {\n",
    "        'input': wmt14_dataset[idx]['translation']['de'],\n",
    "        'output_7b': outputs_7b[idx],\n",
    "        'output_tiny': outputs_tiny[idx],\n",
    "        'output_13b': outputs_13b[idx]\n",
    "    }\n",
    "    \n",
    "    input_output_pairs.append(outputs)\n",
    "\n",
    "# with open('input_output_pairs_wmt14.pkl', 'wb') as f:\n",
    "#     pickle.dump(input_output_pairs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b2ae33-a0d6-40b0-8e23-c55fb982694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input_output_pairs_wmt14.pkl', 'rb') as f:\n",
    "    input_output_pairs = pickle.load(f)\n",
    "\n",
    "len(input_output_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2dfd5a-6a1e-409c-9749-acd15d6ba547",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(input_output_pairs[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfec28a0-b890-43b1-95f3-d4ee59d07d91",
   "metadata": {},
   "source": [
    "### **CNN_Dailymail (Summarization)**\n",
    "is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a960eee2-cbf1-40b4-aa5f-3e3a3a3a641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cnn_dailymail_dataset = load_dataset('abisee/cnn_dailymail', '2.0.0', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af892f96-b5de-4184-af58-91629346c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = cnn_dailymail_dataset[100]['article'] \n",
    "input_prompt = \"Summarize the following text in under 50 words: \\n\\n\" + input_text + \"\\n\\n Write the summary here: \"\n",
    "\n",
    "inputs = tinyllama_tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ed3f2-2296-4027-8a2e-b21a08a8e0e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = tinyllama.generate(inputs['input_ids'], max_new_tokens=100)\n",
    "\n",
    "output_text = tinyllama_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input: {input_prompt}\")\n",
    "print(f\"Output: {output_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da15724-547e-4de3-9328-8c0b1058f199",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_prefix = \"Write the summary here: \"\n",
    "if summary_prefix in output_text:\n",
    "    cleaned_output = output_text.split(summary_prefix)[-1].strip()\n",
    "else:\n",
    "    cleaned_output = output_text.strip()\n",
    "\n",
    "print(cleaned_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97856407-e2b7-41e8-ad4f-bf0f8aecbe29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_ranges = {\n",
    "    '0-100': 0,\n",
    "    '101-200': 0,\n",
    "    '201-300': 0,\n",
    "    '301-400': 0,\n",
    "    '401-500': 0,\n",
    "    '501-600': 0,\n",
    "    '601-700': 0,\n",
    "    '701-800': 0,\n",
    "    '801-900': 0,\n",
    "    '901-1000': 0,\n",
    "    '1001+': 0\n",
    "}\n",
    "\n",
    "max_tokens = -1\n",
    "max_tokens_idx = -1\n",
    "\n",
    "for idx, data in enumerate(cnn_dailymail_dataset):\n",
    "    input_text = data['article']\n",
    "    tokens = llama7b_tokenizer(input_text, return_tensors=\"pt\")\n",
    "    num_tokens = len(tokens['input_ids'][0])\n",
    "    \n",
    "    if num_tokens > max_tokens:\n",
    "        max_tokens = num_tokens\n",
    "        max_tokens_idx = idx\n",
    "    \n",
    "    if num_tokens <= 100:\n",
    "        token_ranges['0-100'] += 1\n",
    "    elif num_tokens <= 200:\n",
    "        token_ranges['101-200'] += 1\n",
    "    elif num_tokens <= 300:\n",
    "        token_ranges['201-300'] += 1\n",
    "    elif num_tokens <= 400:\n",
    "        token_ranges['301-400'] += 1\n",
    "    elif num_tokens <= 500:\n",
    "        token_ranges['401-500'] += 1\n",
    "    elif num_tokens <= 600:\n",
    "        token_ranges['501-600'] += 1\n",
    "    elif num_tokens <= 700:\n",
    "        token_ranges['601-700'] += 1\n",
    "    elif num_tokens <= 800:\n",
    "        token_ranges['701-800'] += 1\n",
    "    elif num_tokens <= 900:\n",
    "        token_ranges['801-900'] += 1\n",
    "    elif num_tokens <= 1000:\n",
    "        token_ranges['901-1000'] += 1\n",
    "    else:\n",
    "        token_ranges['1001+'] += 1\n",
    "\n",
    "print(\"Number of data points in different token ranges:\")\n",
    "for key, value in token_ranges.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(f\"\\nData point with the most tokens is at index: {max_tokens_idx}\")\n",
    "print(f\"Number of tokens: {max_tokens}\")\n",
    "print(f\"Input text: {cnn_dailymail_dataset[max_tokens_idx]['article']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad52c0d-185d-4290-8500-7eea3211b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(model, tokenizer, dataset, current_idx):\n",
    "    outputs = []\n",
    "    \n",
    "    input_text = cnn_dailymail_dataset[current_idx]['article'] \n",
    "    input_prompt = \"Summarize the following text in under 50 words: \\n\\n\" + input_text + \"\\n\\n Write the summary here: \"\n",
    "    \n",
    "    #inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(\"cuda\")\n",
    "    #output = model.generate(inputs['input_ids'], max_new_tokens=2048)\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "    output = model.generate(inputs['input_ids'], max_new_tokens=100)\n",
    "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    summary_prefix = \"Write the summary here: \"\n",
    "    if summary_prefix in output_text:\n",
    "        cleaned_output = output_text.split(summary_prefix)[-1].strip()\n",
    "    else:\n",
    "        cleaned_output = output_text.strip()\n",
    "\n",
    "    outputs.append(cleaned_output)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b1b1fc-7320-4272-8530-41bab7fdd2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "outputs_7b = []\n",
    "outputs_tiny = []\n",
    "outputs_13b = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b98e1d-f1e5-4da4-bcbb-ab65e1342c5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for current_idx in range(0, 1000):\n",
    "#     input_text = cnn_dailymail_dataset[current_idx]['article']\n",
    "#     output_7b = generate_output(llama7b, llama7b_tokenizer, input_text, current_idx)\n",
    "\n",
    "#     outputs_7b.append(output_7b)\n",
    "    \n",
    "#     print(f\"Llama-7b | CURRENT IDX: {current_idx} | Length: {len(outputs_7b)}\")\n",
    "#     with open('input_output_pairs_cnn_dailymail_7b', 'wb') as f:\n",
    "#         pickle.dump(outputs_7b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e82638-691d-4469-86e9-c21d46de34b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('input_output_pairs_cnn_dailymail_7b', 'rb') as f:\n",
    "    outputs_7b = pickle.load(f)\n",
    "\n",
    "print(len(outputs_7b))\n",
    "# print(outputs_7b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676778c2-e538-46be-b321-2730abc759bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for current_idx in range(0, 1000):\n",
    "#     input_text = cnn_dailymail_dataset[current_idx]['article']\n",
    "#     output_tiny = generate_output(tinyllama, tinyllama_tokenizer, input_text, current_idx)\n",
    "    \n",
    "#     outputs_tiny.append(output_tiny)\n",
    "    \n",
    "#     print(f\"TinyLlama | CURRENT IDX: {current_idx} | Length: {len(outputs_tiny)}\")\n",
    "#     with open('input_output_pairs_cnn_dailymail_tinyllama', 'wb') as f:\n",
    "#         pickle.dump(outputs_tiny, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8800bc-7725-4106-8f72-c28bda260403",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('input_output_pairs_cnn_dailymail_tinyllama', 'rb') as f:\n",
    "    outputs_tiny = pickle.load(f)\n",
    "\n",
    "print(len(outputs_tiny))\n",
    "# print(outputs_tiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d378239-0de1-4d87-ba94-659dffc7f572",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for current_idx in range(604, 1000):\n",
    "#     input_text = cnn_dailymail_dataset[current_idx]['article']\n",
    "#     output_13b = generate_output(llama13b, llama13b_tokenizer, input_text, current_idx)\n",
    "    \n",
    "#     outputs_13b.append(output_13b)\n",
    "    \n",
    "#     print(f\"Llama-13b | CURRENT IDX: {current_idx} | Length: {len(outputs_13b)}\")\n",
    "#     with open('input_output_pairs_cnn_dailymail_13b.pkl', 'wb') as f:\n",
    "#         pickle.dump(outputs_13b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc09a71-df5a-4bfd-98db-2608440b8400",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('input_output_pairs_cnn_dailymail_13b.pkl', 'rb') as f:\n",
    "    outputs_13b = pickle.load(f)\n",
    "\n",
    "print(len(outputs_13b))\n",
    "# print(outputs_13b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f73a22-341c-49b9-9dba-db868f5fb04a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49015124-8531-4dbc-8349-84068d82cd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_output_pairs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea46f13f-4363-4c98-ac03-b5c048bb2174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(len(cnn_dailymail_dataset)):\n",
    "#     outputs = {\n",
    "#         'input': cnn_dailymail_dataset[idx][\"article\"],\n",
    "#         'output_7b': outputs_7b[idx],\n",
    "#         'output_tiny': outputs_tiny[idx],\n",
    "#         'output_13b': outputs_13b[idx]\n",
    "#     }\n",
    "    \n",
    "#     input_output_pairs.append(outputs)\n",
    "    \n",
    "#     print(\"---------------------------------------------------------------------------\")\n",
    "#     print(f\"CURRENT IDX: {idx}\")\n",
    "#     print(f\"Length: {len(input_output_pairs)}\")\n",
    "#     # print(f\"Current Dataset: {input_output_pairs[-1]}\")\n",
    "#     with open('input_output_pairs_cnn_dailymail.pkl', 'wb') as f:\n",
    "#         pickle.dump(input_output_pairs, f)\n",
    "#     print(\"---------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40832e0c-0b2e-486e-911f-c4a5ab593e5f",
   "metadata": {},
   "source": [
    "### **GSM8K (Math)**\n",
    "is a dataset of 8.5K high quality linguistically diverse grade school math word problems. The dataset was created to support the task of question answering on basic mathematical problems that require multi-step reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612c579d-6007-4845-a252-6d8271f6d81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "gsm8k_dataset = load_dataset('openai/gsm8k', 'main', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74248079-e029-48b7-8993-0260ddd26ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = gsm8k_dataset[0]['question'] \n",
    "input_prompt = \"Answer the following math question: \\n\\n\" + input_text + \"\\n\\n Lets think step by step: \"\n",
    "\n",
    "inputs = tinyllama_tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7178d2-9851-42ad-aec1-15f471eabe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tinyllama.generate(inputs['input_ids'])\n",
    "\n",
    "output_text = tinyllama_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input: {input_prompt}\")\n",
    "print(f\"Output: {output_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca26d07f-768f-4764-bf57-836b516a0be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_prefix = \"Lets think step by step: \"\n",
    "if answer_prefix in output_text:\n",
    "    cleaned_output = output_text.split(answer_prefix)[-1].strip()\n",
    "else:\n",
    "    cleaned_output = output_text.strip()\n",
    "\n",
    "print(cleaned_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b9aafb-e828-4b0c-babc-feb38150621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ranges = {\n",
    "    '0-50': 0,\n",
    "    '51-100': 0,\n",
    "    '101-150': 0,\n",
    "    '151-200': 0,\n",
    "    '201-250': 0\n",
    "}\n",
    "\n",
    "max_tokens = -1\n",
    "\n",
    "for idx, data in enumerate(gsm8k_dataset):\n",
    "    input_text = data['question']\n",
    "    tokens = llama7b_tokenizer(input_text, return_tensors=\"pt\")\n",
    "    num_tokens = len(tokens['input_ids'][0])\n",
    "    \n",
    "    if num_tokens > max_tokens:\n",
    "        max_tokens = num_tokens\n",
    "        max_tokens_idx = idx\n",
    "    \n",
    "    if num_tokens <= 50:\n",
    "        token_ranges['0-50'] += 1\n",
    "    elif num_tokens <= 100:\n",
    "        token_ranges['51-100'] += 1\n",
    "    elif num_tokens <= 150:\n",
    "        token_ranges['101-150'] += 1\n",
    "    elif num_tokens <= 200:\n",
    "        token_ranges['151-200'] += 1\n",
    "    elif num_tokens <= 250:\n",
    "        token_ranges['201-250'] += 1\n",
    "\n",
    "print(\"Number of data points in different token ranges:\")\n",
    "for key, value in token_ranges.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(f\"\\nData point with the most tokens is at index: {max_tokens_idx}\")\n",
    "print(f\"Number of tokens: {max_tokens}\")\n",
    "print(f\"Input text: {gsm8k_dataset[max_tokens_idx]['question']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36c56d2-9596-49e3-8cf4-42de3a2cd295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(model, tokenizer, dataset, current_idx):\n",
    "    outputs = []\n",
    "    \n",
    "    input_text = gsm8k_dataset[current_idx]['question'] \n",
    "    input_prompt = \"Answer the following math question: \\n\\n\" + input_text + \"\\n\\n Lets think step by step: \"\n",
    "\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "    output = model.generate(inputs['input_ids'])\n",
    "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    answer_prefix = \"Lets think step by step: \"\n",
    "    if answer_prefix in output_text:\n",
    "        cleaned_output = output_text.split(answer_prefix)[-1].strip()\n",
    "    else:\n",
    "        cleaned_output = output_text.strip()\n",
    "\n",
    "    outputs.append(cleaned_output)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc18749e-3245-4e06-842d-f08cfc0cfd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "# outputs_7b = []\n",
    "# outputs_tiny = []\n",
    "# outputs_13b = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed70679-1f84-4bdd-96cc-29d9aa38bb78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for current_idx in range(948, 1000):\n",
    "    input_text = gsm8k_dataset[current_idx]['question']\n",
    "    output_7b = generate_output(llama7b, llama7b_tokenizer, input_text, current_idx)\n",
    "\n",
    "    outputs_7b.append(output_7b)\n",
    "    \n",
    "    print(f\"Llama-7b | CURRENT IDX: {current_idx} | Length: {len(outputs_7b)}\")\n",
    "    # with open('input_output_pairs_gsm8k_7b', 'wb') as f:\n",
    "    #     pickle.dump(outputs_7b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19b43f3-e5ef-4308-9893-6add3ab0433e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for current_idx in range(957, 1000):\n",
    "    input_text = gsm8k_dataset[current_idx]['question']\n",
    "    output_tiny = generate_output(tinyllama, tinyllama_tokenizer, input_text, current_idx)\n",
    "\n",
    "    outputs_tiny.append(output_tiny)\n",
    "    \n",
    "    print(f\"TinyLlama | CURRENT IDX: {current_idx} | Length: {len(outputs_tiny)}\")\n",
    "    # with open('input_output_pairs_gsm8k_tiny', 'wb') as f:\n",
    "    #     pickle.dump(outputs_tiny, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a92f60e-8220-4075-9026-527a0bbdd63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input_output_pairs_gsm8k_tiny', 'rb') as f:\n",
    "    outputs_tiny = pickle.load(f)\n",
    "\n",
    "print(len(outputs_tiny))\n",
    "print(outputs_tiny[999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a8876a-5336-4ec1-99b4-221d566cd8d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for current_idx in range(899, 1000):\n",
    "    input_text = gsm8k_dataset[current_idx]['question']\n",
    "    output_13b = generate_output(llama13b, llama13b_tokenizer, input_text, current_idx)\n",
    "\n",
    "    outputs_13b.append(output_13b)\n",
    "    \n",
    "    print(f\"Llama13b | CURRENT IDX: {current_idx} | Length: {len(outputs_13b)}\")\n",
    "    # with open('input_output_pairs_gsm8k_13b', 'wb') as f:\n",
    "    #     pickle.dump(outputs_13b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d3f79-e33f-48b4-bd9f-6675badf27fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input_output_pairs_gsm8k_13b', 'rb') as f:\n",
    "    outputs_13b = pickle.load(f)\n",
    "\n",
    "print(len(outputs_13b))\n",
    "print(outputs_13b[831])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd8dda6-63ae-47c9-999b-bd977d523e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(outputs_7b))\n",
    "print(len(outputs_tiny))\n",
    "print(len(outputs_13b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2c270a-7afb-4324-bdd8-106cd35706a7",
   "metadata": {},
   "source": [
    "Remove the \\n from the 3 output arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68434cac-3a15-4935-928b-8392082c60a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_7b = [[output.replace('\\n', ' ') for output in output_list] for output_list in outputs_7b]\n",
    "outputs_tiny = [[output.replace('\\n', ' ') for output in output_list] for output_list in outputs_tiny]\n",
    "outputs_13b = [[output.replace('\\n', ' ') for output in output_list] for output_list in outputs_13b]\n",
    "\n",
    "# with open('input_output_pairs_gsm8k_7b', 'wb') as f:\n",
    "#     pickle.dump(outputs_7b, f)\n",
    "# with open('input_output_pairs_gsm8k_tiny', 'wb') as f:\n",
    "#     pickle.dump(outputs_tiny, f)\n",
    "# with open('input_output_pairs_gsm8k_13b', 'wb') as f:\n",
    "#     pickle.dump(outputs_13b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03673b4a-191a-4c3d-ba0d-44d6181c11f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_output_pairs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea121d07-6684-4e95-97c5-b672dbed2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(outputs_7b)):\n",
    "    outputs = {\n",
    "        'input': gsm8k_dataset[idx][\"question\"],\n",
    "        'output_7b': outputs_7b[idx],\n",
    "        'output_tiny': outputs_tiny[idx],\n",
    "        'output_13b': outputs_13b[idx]\n",
    "    }\n",
    "    \n",
    "    input_output_pairs.append(outputs)\n",
    "\n",
    "# with open('input_output_pairs_gsm8k.pkl', 'wb') as f:\n",
    "#     pickle.dump(input_output_pairs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d9262b-1247-4a84-8855-e278da35c48e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fc2af8-507f-4d8f-bf94-81d930e9877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_output_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfef1deb-b2d5-4088-a39b-c069ea755712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
